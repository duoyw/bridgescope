[
    {
        "question_id": 333,
        "db_id": "toxicology",
        "question": "Delete all carbon atoms from the molecule TR008.",
        "evidence": "carbon refers to element = 'c'",
        "SQL": "SELECT COUNT(T.atom_id) FROM atom AS T WHERE T.molecule_id = 'TR008' AND T.element = 'c'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM atom WHERE atom.molecule_id = 'TR008' AND atom.element = 'c' ",
        "result_size": 1,
        "result": "(10,)",
        "base_pg_sql": "SELECT COUNT(T.atom_id) FROM atom AS T WHERE T.molecule_id = 'TR008' AND T.element = 'c'",
        "base_question": "In the molecule TR008, how many carbons are present?",
        "base_evidence": "carbon refers to element = 'c'",
        "gt": {
            "type": "delete",
            "table": "atom",
            "condition": "SELECT atom.atom_id FROM atom WHERE atom.molecule_id = 'TR008' AND atom.element = 'c' "
        }
    },
    {
        "question_id": 165,
        "db_id": "financial",
        "question": "Delete all transactions made by accounts that belong to district 5.",
        "evidence": "",
        "SQL": "SELECT T3.trans_id FROM district AS T1 INNER JOIN account AS T2 ON T1.district_id = T2.district_id INNER JOIN trans AS T3 ON T2.account_id = T3.account_id WHERE T1.district_id = 5",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM trans USING district AS T1, account AS T2 WHERE T1.district_id = T2.district_id AND T2.account_id = trans.account_id AND T1.district_id = 5 ",
        "result_size": 15140,
        "result": "(837,)####(844,)####(845,)####(846,)####(847,)####(848,)####(849,)####(850,)####(851,)####(852,)####(853,)####(854,)####(855,)####(856,)####(857,)####(858,)####(859,)####(860,)####(861,)####(873,)####(874,)####(875,)####(876,)####(877,)####(878,)####(879,)####(880,)####(881,)####(882,)####(883,)####(884,)####(885,)####(897,)####(898,)####(899,)####(900,)####(901,)####(902,)####(903,)####(904,)####(905,)####(906,)####(907,)####(908,)####(909,)####(921,)####(922,)####(923,)####(924,)####(925,)####(926,)####(927,)####(928,)####(929,)####(930,)####(931,)####(932,)####(933,)####(938,)####(939,)####(946,)####(947,)####(948,)####(949,)####(950,)####(951,)####(952,)####(956,)####(957,)####(969,)####(970,)####(971,)####(972,)####(973,)####(974,)####(975,)####(976,)####(977,)####(978,)####(979,)####(980,)####(981,)####(982,)####(983,)####(984,)####(985,)####(986,)####(987,)####(988,)####(989,)####(990,)####(991,)####(992,)####(993,)####(994,)####(995,)####(996,)####(997,)####(998,)####(9009,)####(9018,)####(9019,)####(9020,)####(9021,)####(9022,)####(9023,)####(9024,)####(9025,)####(9026,)####(9027,)####(9028,)####(9029,)####(9030,)####(9031,)####(9032,)####(9033,)####(9034,)####(9035,)####(9036,)####(9037,)####(9038,)####(9039,)####(9040,)####(9041,)####(9042,)####(9043,)####(9044,)####(9045,)####(9046,)####(9047,)####(9048,)####(9049,)####(9050,)####(9051,)####(9052,)####(9053,)####(9054,)####(9055,)####(9056,)####(9057,)####(9059,)####(9060,)####(9061,)####(9063,)####(9064,)####(9065,)####(9067,)####(9068,)####(9069,)####(9077,)####(9078,)####(9079,)####(9080,)####(9081,)####(9082,)####(9083,)####(9084,)####(9085,)####(9086,)####(9087,)####(9088,)####(9089,)####(9090,)####(9091,)####(9105,)####(9106,)####(9107,)####(9108,)####(9109,)####(9110,)####(9111,)####(9112,)####(9113,)####(9114,)####(9115,)####(9116,)####(9117,)####(9118,)####(9119,)####(9120,)####(9121,)####(9122,)####(9123,)####(9124,)####(9125,)####(9126,)####(9127,)####(9128,)####(9129,)####(9130,)####(9131,)####(9132,)####(9133,)####(9134,)####(9135,)####(9136,)####(9137,)####(9138,)####(9139,)####(9140,)####(9141,)####(9142,)####(9143,)####(9144,)####(9145,)####(9146,)####(9147,)####(9148,)####(9149,)####(9150,)####(9151,)####(9152,)####(9153,)####(9154,)####(9155,)####(9156,)####(9157,)####(9158,)####(9159,)####(9160,)####(9161,)####(9162,)####(9163,)####(9164,)####(9165,)####(9166,)####(9167,)####(9168,)####(9169,)####(9170,)####(9171,)####(9172,)####(9173,)####(9174,)####(9175,)####(9176,)####(9177,)####(9178,)####(9179,)####(38689,)####(38691,)####(38697,)####(38698,)####(38699,)####(38700,)####(38701,)####(38702,)####(38703,)####(38704,)####(38705,)####(38706,)####(38707,)####(38708,)####(38709,)####(38710,)####(38711,)####(38712,)####(38713,)####(38714,)####(38715,)####(38716,)####(38717,)####(38718,)####(38719,)####(38720,)####(38721,)####(38722,)####(38723,)####(38724,)####(38725,)####(38726,)####(38727,)####(38728,)####(38729,)####(38730,)####(38731,)####(38732,)####(38733,)####(38734,)####(38735,)####(38736,)####(38737,)####(38738,)####(38739,)####(38740,)####(38741,)####(38742,)####(38743,)####(38744,)####(38745,)####(38746,)####(38747,)####(38764,)####(38765,)####(38766,)####(38767,)####(38768,)####(38769,)####(38770,)####(38771,)####(38772,)####(38773,)####(38774,)####(38775,)####(38776,)####(38777,)####(38778,)####(38779,)####(38780,)####(38781,)####(38782,)####(38783,)####(38784,)####(38785,)####(38786,)####(38787,)####(38788,)####(38789,)####(38790,)####(38791,)####(38792,)####(38793,)####(38794,)####(38795,)####(38796,)####(38797,)####(38798,)####(38799,)####(38800,)####(38801,)####(38802,)####(38803,)####(38804,)####(38805,)####(38806,)####(38807,)####(38808,)####(38809,)####(38810,)####(38811,)####(38812,)####(38813,)####(38814,)####(38815,)####(38816,)####(38817,)####(38818,)####(38819,)####(38826,)####(38827,)####(38830,)####(38837,)####(38840,)####(38841,)####(38842,)####(38844,)####(38845,)####(38846,)####(38850,)####(38852,)####(38855,)####(38859,)####(38860,)####(38864,)####(38875,)####(38879,)####(38895,)####(38901,)####(38902,)####(38903,)####(38907,)####(38910,)####(38912,)####(38913,)####(38921,)####(38926,)####(38927,)####(38928,)####(38932,)####(38933,)####(38934,)####(38937,)####(38944,)####(38945,)####(38946,)####(38947,)####(38948,)####(38949,)####(38950,)####(38952,)####(38970,)####(38971,)####(38972,)####(38973,)####(38974,)####(38975,)####(38976,)####(38977,)####(38978,)####(38979,)####(38980,)####(38981,)####(38982,)####(38983,)####(38984,)####(38985,)####(38986,)####(38987,)####(38988,)####(38989,)####(38990,)####(38991,)####(38992,)####(38993,)####(38994,)####(38995,)####(38996,)####(38997,)####(38998,)####(38999,)####(39000,)####(39001,)####(39002,)####(39003,)####(39004,)####(39005,)####(39006,)####(39007,)####(39008,)####(39009,)####(39010,)####(39011,)####(39012,)####(39013,)####(39014,)####(39015,)####(39016,)####(39017,)####(39018,)####(39019,)####(39020,)####(39021,)####(39022,)####(39023,)####(39024,)####(39025,)####(39026,)####(39027,)####(39028,)####(39029,)####(39030,)####(39031,)####(39032,)####(39033,)####(39034,)####(39035,)####(39036,)####(39037,)####(39038,)####(39039,)####(39040,)####(39041,)####(39042,)####(39043,)####(39044,)####(39045,)####(39046,)####(39047,)####(39048,)####(39049,)####(39050,)####(39051,)####(39052,)####(39053,)####(39054,)####(39055,)####(39056,)####(39057,)####(39058,)####(39059,)####(39060,)####(39061,)####(39062,)####(39063,)####(39064,)####(39065,)####(39066,)####(39067,)####(39068,)####(39069,)####(39070,)####(39071,)####(39072,)####(39073,)####(39074,)####(39075,)####(39076,)####(39077,)####(39078,)####(39079,)####(39080,)####(39081,)####(39082,)####(39083,)####(39084,)####(39085,)####(39086,)####(39087,)####(39088,)####(39089,)####(39090,)####(39091,)####(55845,)####(55849,)####(55850,)####(55851,)####(55852,)####(55853,)####(55854,)####(55855,)####(55856,)####(55857,)####(55858,)####(55938,)####(55939,)####(55940,)####(55941,)####(55942,)####(55943,)####(55944,)####(55945,)####(55946,)####(55947,)####(55948,)####(55949,)####(55950,)####(55951,)####(55952,)####(55859,)####(55860,)####(55861,)####(55862,)####(55863,)####(55864,)####(55865,)####(55866,)####(55867,)####(55868,)####(55869,)####(55870,)####(55871,)####(55953,)####(55872,)####(55873,)####(55874,)####(55875,)####(55876,)####(55877,)####(55878,)####(55879,)####(55880,)####(55881,)####(55882,)####(55954,)####(55883,)####(55884,)####(55885,)####(55955,)####(55886,)####(55887,)####(55888,)####(55889,)####(55890,)####(55891,)####(55892,)####(55893,)####(55894,)####(56042,)####(56043,)####(56047,)####(56049,)####(56050,)####(56052,)####(56053,)####(56055,)####(56058,)####(55895,)####(55896,)####(55897,)####(55898,)####(55899,)####(55900,)####(55901,)####(55902,)####(55903,)####(55904,)####(55905,)####(55906,)####(55907,)####(55908,)####(55909,)####(55910,)####(55911,)####(55912,)####(55913,)####(55914,)####(55915,)####(55916,)####(55917,)####(55926,)####(55927,)####(55928,)####(55929,)####(55930,)####(55931,)####(55932,)####(55933,)####(55934,)####(55935,)####(55936,)####(55937,)####(56060,)####(56061,)####(56063,)####(56064,)####(56065,)####(56069,)####(56070,)####(56072,)####(56073,)####(56082,)####(56083,)####(56084,)####(56085,)####(56086,)####(56087,)####(56088,)####(56089,)####(56090,)####(56091,)####(56092,)####(56093,)####(56094,)####(56095,)####(56096,)####(56097,)####(56098,)####(56099,)####(56100,)####(56101,)####(55956,)####(56102,)####(56103,)####(56155,)####(56104,)####(56156,)####(56105,)####(55957,)####(56106,)####(55958,)####(56107,)####(56108,)####(55959,)####(56109,)####(55960,)####(56110,)####(56111,)####(55961,)####(56112,)####(56113,)####(56114,)####(55962,)####(56115,)####(55963,)####(56116,)####(56117,)####(56118,)####(55964,)####(56119,)####(56120,)####(56121,)####(55965,)####(56122,)####(56123,)####(55966,)####(56124,)####(56125,)####(55967,)####(56126,)####(56127,)####(55968,)####(56128,)####(56129,)####(55969,)####(56130,)####(56131,)####(55970,)####(56132,)####(55971,)####(56133,)####(56134,)####(56135,)####(55972,)####(56136,)####(55973,)####(55974,)####(55975,)####(55976,)####(55977,)####(55978,)####(55979,)####(55980,)####(56137,)####(56138,)####(56139,)####(56140,)####(56141,)####(56142,)####(56143,)####(56144,)####(56145,)####(56146,)####(56147,)####(56148,)####(56149,)####(56150,)####(56151,)####(56152,)####(56153,)####(56154,)####(55981,)####(55982,)####(55983,)####(55984,)####(55985,)####(55986,)####(55987,)####(55988,)####(55989,)####(55991,)####(55993,)####(55994,)####(55997,)####(55999,)####(56002,)####(56003,)####(56004,)####(56006,)####(56007,)####(56014,)####(56015,)####(56016,)####(56017,)####(56018,)####(56019,)####(56020,)####(56021,)####(56024,)####(56025,)####(56026,)####(56027,)####(56029,)####(56031,)####(56032,)####(56034,)####(56038,)####(56039,)####(56040,)####(56041,)####(68544,)####(68378,)####(68388,)####(68389,)####(68390,)####(68391,)####(68392,)####(68393,)####(68394,)####(68395,)####(68396,)####(68397,)####(68398,)####(68399,)####(68400,)####(68401,)####(68402,)####(68403,)####(68404,)####(68405,)####(68440,)####(68441,)####(68442,)####(68443,)####(68444,)####(68445,)####(68446,)####(68447,)####(68448,)####(68449,)####(68450,)####(68465,)####(68466,)####(68467,)####(68468,)####(68469,)####(68470,)####(68471,)####(68472,)####(68473,)####(68474,)####(68475,)####(68476,)####(68477,)####(68478,)####(68479,)####(68480,)####(68481,)####(68482,)####(68406,)####(68407,)####(68408,)####(68409,)####(68410,)####(68411,)####(68412,)####(68413,)####(68414,)####(68415,)####(68416,)####(68417,)####(68418,)####(68419,)####(68420,)####(68421,)####(68422,)####(68423,)####(68424,)####(68425,)####(68426,)####(68427,)####(68428,)####(68429,)####(68430,)####(68431,)####(68432,)####(68433,)####(68434,)####(68435,)####(68436,)####(68437,)####(68438,)####(68439,)####(68643,)####(68644,)####(68645,)####(68646,)####(68648,)####(68656,)####(68657,)####(68658,)####(68659,)####(68660,)####(68661,)####(68662,)####(68663,)####(68664,)####(68665,)####(68666,)####(68667,)####(68668,)####(68669,)####(68670,)####(68671,)####(68672,)####(68673,)####(68674,)####(68690,)####(68691,)####(68692,)####(68693,)####(68694,)####(68695,)####(68696,)####(68697,)####(68698,)####(68699,)####(68700,)####(68701,)####(68702,)####(68703,)####(68704,)####(68705,)####(68706,)####(68707,)####(68708,)####(68709,)####(68710,)####(68711,)####(68712,)####(68713,)####(68714,)####(68715,)####(68716,)####(68717,)####(68718,)####(68719,)####(68720,)####(68721,)####(68722,)####(68723,)####(68724,)####(68725,)####(68726,)####(68727,)####(68728,)####(68729,)####(68730,)####(68731,)####(68732,)####(68733,)####(68734,)####(68735,)####(68736,)####(68737,)####(68738,)####(68739,)####(68740,)####(68741,)####(68742,)####(68743,)####(68744,)####(68745,)####(68746,)####(68747,)####(68748,)####(68749,)####(68750,)####(68751,)####(68752,)####(68753,)####(68754,)####(68755,)####(68756,)####(68483,)####(68484,)####(68485,)####(68486,)####(68487,)####(68488,)####(68489,)####(68490,)####(68491,)####(68492,)####(68493,)####(68494,)####(68495,)####(68496,)####(68497,)####(68498,)####(68499,)####(68500,)####(68501,)####(68502,)####(68503,)####(68504,)####(68505,)####(68506,)####(68507,)####(68508,)####(68509,)####(68510,)####(68511,)####(68512,)####(68513,)####(68514,)####(68515,)####(68516,)####(68517,)####(68518,)####(68519,)####(68520,)####(68521,)####(68522,)####(68536,)####(68537,)####(68538,)####(68539,)####(68540,)####(68541,)####(68542,)####(68543,)####(68757,)####(68758,)####(68759,)####(68760,)####(68761,)####(68762,)####(68763,)####(68764,)####(68765,)####(68766,)####(68767,)####(68768,)####(68769,)####(68770,)####(68771,)####(68772,)####(68773,)####(68774,)####(68775,)####(68776,)####(68777,)####(68778,)####(68779,)####(68780,)####(68781,)####(68782,)####(68783,)####(68784,)####(68785,)####(68786,)####(68787,)####(68788,)####(68789,)####(68790,)####(68791,)####(68792,)####(68793,)####(68794,)####(68795,)####(68796,)####(68545,)####(68546,)####(68547,)####(68548,)####(68549,)####(68550,)####(68551,)####(68552,)####(68553,)####(68554,)####(68555,)####(68556,)####(68557,)####(68558,)####(68559,)####(68560,)####(68561,)####(68562,)####(68563,)####(68564,)####(68565,)####(68566,)####(68567,)####(68568,)####(68569,)####(68570,)####(68571,)####(68572,)####(68573,)####(68574,)####(68575,)####(68576,)####(68577,)####(68578,)####(68579,)####(68580,)####(68581,)####(68582,)####(68583,)####(68584,)####(68585,)####(68586,)####(68587,)####(68588,)####(68589,)####(68590,)####(68591,)####(68592,)####(68593,)####(68594,)####(68596,)####(68598,)####(68599,)####(68600,)####(68603,)####(68604,)####(68605,)####(68606,)####(68608,)####(68609,)####(68610,)####(68611,)####(68612,)####(68625,)####(68626,)####(68627,)####(68628,)####(68629,)####(68630,)####(68631,)####(68632,)####(68633,)####(68634,)####(68635,)####(68636,)####(68637,)####(68638,)####(68639,)####(68640,)####(68641,)####(68642,)####(101065,)####(100957,)####(100969,)####(100970,)####(100971,)####(100972,)####(100973,)####(100974,)####(100975,)####(100976,)####(100977,)####(100978,)####(100979,)####(100980,)####(100981,)####(100982,)####(100983,)####(100984,)####(100985,)####(100986,)####(100987,)####(100988,)####(100989,)####(100990,)####(100991,)####(100992,)####(100993,)####(100994,)####(101176,)####(101177,)####(101178,)####(101179,)####(101180,)####(101181,)####(101182,)####(101183,)####(101184,)####(101185,)####(101186,)####(101187,)####(101188,)####(101189,)####(101190,)####(101191,)####(101208,)####(101209,)####(100995,)####(100996,)####(100997,)####(100998,)####(100999,)####(101000,)####(101001,)####(101002,)####(101003,)####(101004,)####(101005,)####(101006,)####(101007,)####(101008,)####(101009,)####(101010,)####(101011,)####(101012,)####(101013,)####(101014,)####(101015,)####(101016,)####(101017,)####(101018,)####(101019,)####(101020,)####(101021,)####(101022,)####(101023,)####(101024,)####(101025,)####(101026,)####(101027,)####(101028,)####(101029,)####(101046,)####(101047,)####(101048,)####(101049,)####(101050,)####(101051,)####(101052,)####(101053,)####(101054,)####(101055,)####(101056,)####(101057,)####(101058,)####(101059,)####(101060,)####(101061,)####(101062,)####(101063,)####(101064,)####(101210,)####(101211,)####(101212,)####(101213,)####(101214,)####(101215,)####(101216,)####(101217,)####(101218,)####(101219,)####(101220,)####(101221,)####(101222,)####(101223,)####(101224,)####(101225,)####(101226,)####(101227,)####(101228,)####(101229,)####(101230,)####(101231,)####(101232,)####(101233,)####(101234,)####(101235,)####(101236,)####(101237,)####(101238,)####(101239,)####(101240,)####(101241,)####(101242,)####(101243,)####(101244,)####(101245,)####(101246,)####(101247,)####(101248,)####(101249,)####(101250,)####(101251,)####(101252,)####(101253,)####(101254,)####(101255,)####(101256,)####(101257,)####(101258,)####(101259,)####(101260,)####(101261,)####(101262,)####(101283,)####(101284,)####(101285,)####(101286,)####(101287,)####(101288,)####(101289,)####(101290,)####(101291,)####(101292,)####(101293,)####(101294,)####(101295,)####(101296,)####(101297,)####(101298,)####(101299,)####(101300,)####(101301,)####(101302,)####(101303,)####(101304,)####(101305,)####(101306,)####(101307,)####(101308,)####(101309,)####(101310,)####(101311,)####(101312,)####(101313,)####(101314,)####(101315,)####(101316,)####(101317,)####(101318,)####(101319,)####(101320,)####(101321,)####(101322,)####(101323,)####(101324,)####(101263,)####(101264,)####(101265,)####(101266,)####(101267,)####(101268,)####(101269,)####(101270,)####(101271,)####(101272,)####(101273,)####(101274,)####(101275,)####(101276,)####(101277,)####(101278,)####(101279,)####(101280,)####(101281,)####(101282,)####(101066,)####(101067,)####(101068,)####(101069,)####(101070,)####(101071,)####(101072,)####(101073,)####(101074,)####(101075,)####(101076,)####(101077,)####(101078,)####(101079,)####(101080,)####(101081,)####(101082,)####(101083,)####(101084,)####(101085,)####(101086,)####(101087,)####(101088,)####(101089,)####(101090,)####(101091,)####(101092,)####(101093,)####(101094,)####(101095,)####(101096,)####(101097,)####(101098,)####(101099,)####(101100,)####(101101,)####(101103,)####(101104,)####(101105,)####(101106,)####(101107,)####(101110,)####(101111,)####(101112,)####(101113,)####(101115,)####(101116,)####(101117,)####(101118,)####(101119,)####(101132,)####(101133,)####(101134,)####(101135,)####(101136,)####(101137,)####(101138,)####(101139,)####(101140,)####(101141,)####(101142,)####(101143,)####(101144,)####(101145,)####(101146,)####(101147,)####(101148,)####(101149,)####(101150,)####(101151,)####(101152,)####(101153,)####(101154,)####(101155,)####(101164,)####(101165,)####(101166,)####(101167,)####(101168,)####(101169,)####(101170,)####(101171,)####(101172,)####(101174,)####(101175,)####(114004,)####(113876,)####(113880,)####(113881,)####(113882,)####(113883,)####(113884,)####(113885,)####(113886,)####(113887,)####(113888,)####(113889,)####(113890,)####(113891,)####(113924,)####(113925,)####(113926,)####(113927,)####(113928,)####(113929,)####(113930,)####(113931,)####(113932,)####(113933,)####(113934,)####(113935,)####(113936,)####(113937,)####(113938,)####(113939,)####(113940,)####(113941,)####(113942,)####(113943,)####(113944,)####(113945,)####(113946,)####(113947,)####(113948,)####(113956,)####(113957,)####(113958,)####(113959,)####(113960,)####(113961,)####(113962,)####(113963,)####(113964,)####(113965,)####(113966,)####(113967,)####(113968,)####(113969,)####(113970,)####(113971,)####(113972,)####(113973,)####(113974,)####(113975,)####(113976,)####(113977,)####(113978,)####(113979,)####(113980,)####(113981,)####(113982,)####(113983,)####(113984,)####(113985,)####(113986,)####(113987,)####(113988,)####(113989,)####(113990,)####(113991,)####(113992,)####(113993,)####(113994,)####(113995,)####(113996,)####(113997,)####(113998,)####(113999,)####(114000,)####(114001,)####(114002,)####(114003,)####(113892,)####(113893,)####(113894,)####(113895,)####(113896,)####(113897,)####(113898,)####(113899,)####(113900,)####(113901,)####(113902,)####(113903,)####(113904,)####(113905,)####(113906,)####(113907,)####(113908,)####(113909,)####(113910,)####(113911,)####(113912,)####(113913,)####(113914,)####(113915,)####(113916,)####(113917,)####(113918,)####(113919,)####(113920,)####(113921,)####(113922,)####(113923,)####(114086,)####(114087,)####(114088,)####(114089,)####(114090,)####(114091,)####(114092,)####(114101,)####(114102,)####(114103,)####(114104,)####(114105,)####(114106,)####(114107,)####(114108,)####(114109,)####(114110,)####(114164,)####(114166,)####(114167,)####(114168,)####(114170,)####(114172,)####(114173,)####(114174,)####(114175,)####(114176,)####(114177,)####(114178,)####(114179,)####(114180,)####(114181,)####(114182,)####(114189,)####(114190,)####(114191,)####(114192,)####(114193,)####(114194,)####(114195,)####(114196,)####(114197,)####(114198,)####(114199,)####(114200,)####(114201,)####(114202,)####(114203,)####(114204,)####(114205,)####(114206,)####(114207,)####(114208,)####(114209,)####(114210,)####(114211,)####(114212,)####(114213,)####(114214,)####(114215,)####(114216,)####(114217,)####(114218,)####(114224,)####(114227,)####(114228,)####(114229,)####(114230,)####(114231,)####(114232,)####(114233,)####(114234,)####(114235,)####(114236,)####(114237,)####(114238,)####(114239,)####(114240,)####(114241,)####(114242,)####(114243,)####(114244,)####(114245,)####(114246,)####(114247,)####(114248,)####(114249,)####(114250,)####(114251,)####(114252,)####(114253,)####(114254,)####(114255,)####(114264,)####(114265,)####(114266,)####(114267,)####(114268,)####(114269,)####(114270,)####(114271,)####(114272,)####(114273,)####(114274,)####(114275,)####(114276,)####(114277,)####(114278,)####(114279,)####(114280,)####(114281,)####(114282,)####(114111,)####(114112,)####(114113,)####(114114,)####(114115,)####(114116,)####(114117,)####(114118,)####(114119,)####(114120,)####(114121,)####(114122,)####(114123,)####(114124,)####(114125,)####(114126,)####(114127,)####(114128,)####(114129,)####(114130,)####(114131,)####(114132,)####(114133,)####(114134,)####(114135,)####(114136,)####(114137,)####(114138,)####(114139,)####(114140,)####(114141,)####(114142,)####(114143,)####(114144,)####(114145,)####(114146,)####(114147,)####(114148,)####(114149,)####(114150,)####(114151,)####(114152,)####(114153,)####(114154,)####(114155,)####(114156,)####(114157,)####(114158,)####(114159,)####(114160,)####(114161,)####(114162,)####(114163,)####(114283,)####(114284,)####(114285,)####(114286,)####(114287,)####(114288,)####(114289,)####(114290,)####(114291,)####(114292,)####(114293,)####(114294,)####(114295,)####(114296,)####(114297,)####(114298,)####(114299,)####(114300,)####(114301,)####(114302,)####(114303,)####(114304,)####(114305,)####(114306,)####(114307,)####(114308,)####(114309,)####(114310,)####(114311,)####(114312,)####(114313,)####(114314,)####(114315,)####(114316,)####(114317,)####(114318,)####(114319,)####(114320,)####(114321,)####(114322,)####(114323,)####(114324,)####(114325,)####(114326,)####(114327,)####(114005,)####(114006,)####(114007,)####(114008,)####(114009,)####(114010,)####(114011,)####(114012,)####(114013,)####(114014,)####(114015,)####(114016,)####(114017,)####(114018,)####(114019,)####(114020,)####(114028,)####(114029,)####(114030,)####(114031,)####(114032,)####(114033,)####(114034,)####(114035,)####(114036,)####(114037,)####(114038,)####(114039,)####(114040,)####(114041,)####(114042,)####(114043,)####(114044,)####(114045,)####(114046,)####(114047,)####(114048,)####(114049,)####(114050,)####(114051,)####(114052,)####(114053,)####(114054,)####(114055,)####(114056,)####(114057,)####(114058,)####(114059,)####(114060,)####(114061,)####(114062,)####(114063,)####(114064,)####(114065,)####(114066,)####(114067,)####(114068,)####(114069,)####(114070,)####(114071,)####(114072,)####(114073,)####(114074,)####(114075,)####(114076,)####(114077,)####(114078,)####(114079,)####(114080,)####(114081,)####(114082,)####(114083,)####(114084,)####(114085,)####(133530,)####(133531,)####(133532,)####(133533,)####(133534,)####(133535,)####(133536,)####(133537,)####(133538,)####(133539,)####(133540,)####(133683,)####(133684,)####(133541,)####(133685,)####(133686,)####(133687,)####(133519,)####(133528,)####(133529,)####(133688,)####(133689,)####(133690,)####(133691,)####(133692,)####(133693,)####(133694,)####(133695,)####(133696,)####(133697,)####(133698,)####(133699,)####(133700,)####(133701,)####(133702,)####(133542,)####(133703,)####(133704,)####(133705,)####(133706,)####(133707,)####(133708,)####(133543,)####(133709,)####(133544,)####(133545,)####(133546,)####(133547,)####(133548,)####(133549,)####(133550,)####(133551,)####(133552,)####(133553,)####(133554,)####(133555,)####(133569,)####(133570,)####(133571,)####(133572,)####(133573,)####(133574,)####(133575,)####(133576,)####(133577,)####(133578,)####(133579,)####(133580,)####(133581,)####(133582,)####(133583,)####(133584,)####(133585,)####(133586,)####(133587,)####(133588,)####(133589,)####(133590,)####(133591,)####(133629,)####(133630,)####(133635,)####(133636,)####(133637,)####(133638,)####(133639,)####(133640,)####(133641,)####(133642,)####(133643,)####(133644,)####(133645,)####(133659,)####(133660,)####(133661,)####(133662,)####(133663,)####(133664,)####(133665,)####(133666,)####(133667,)####(133668,)####(133669,)####(133670,)####(133671,)####(133672,)####(133673,)####(133674,)####(133675,)####(133676,)####(133677,)####(133678,)####(133679,)####(133680,)####(133681,)####(133682,)####(146788,)####(146798,)####(146799,)####(146827,)####(146828,)####(146829,)####(146830,)####(146800,)####(146831,)####(146832,)####(146833,)####(146834,)####(146835,)####(146801,)####(146836,)####(146837,)####(146838,)####(146839,)####(146840,)####(146953,)####(146954,)####(146955,)####(146956,)####(146957,)####(146841,)####(146842,)####(146843,)####(146844,)####(146845,)####(146846,)####(146847,)####(146848,)####(146849,)####(146850,)####(146851,)####(146852,)####(146853,)####(146854,)####(146855,)####(146856,)####(146857,)####(146858,)####(146859,)####(146860,)####(146862,)####(146863,)####(146864,)####(146865,)####(146958,)####(146959,)####(146960,)####(146961,)####(146962,)####(146963,)####(146964,)####(146965,)####(146966,)####(146967,)####(146968,)####(146969,)####(146970,)####(146971,)####(146972,)####(146973,)####(146974,)####(146975,)####(146976,)####(146977,)####(146978,)####(146979,)####(146980,)####(146981,)####(146982,)####(146983,)####(146984,)####(146866,)####(146879,)####(146880,)####(146881,)####(146882,)####(146883,)####(146884,)####(146885,)####(146886,)####(146887,)####(146888,)####(146889,)####(146890,)####(146891,)####(146892,)####(146893,)####(146894,)####(146895,)####(146896,)####(146897,)####(146898,)####(146899,)####(146900,)####(146901,)####(146902,)####(146909,)####(146910,)####(146911,)####(146912,)####(146913,)####(146914,)####(146915,)####(146916,)####(146917,)####(146918,)####(146919,)####(146920,)####(146921,)####(146922,)####(146923,)####(146924,)####(146925,)####(146926,)####(146927,)####(146928,)####(146929,)####(146930,)####(146931,)####(146932,)####(146933,)####(146934,)####(146935,)####(146936,)####(146937,)####(146938,)####(146985,)####(146986,)####(146987,)####(146988,)####(146989,)####(146990,)####(146991,)####(146992,)####(146993,)####(146994,)####(146995,)####(146996,)####(146997,)####(146998,)####(146999,)####(147000,)####(147001,)####(147002,)####(147003,)####(147004,)####(147005,)####(147006,)####(147007,)####(147008,)####(147009,)####(147010,)####(147011,)####(147012,)####(147013,)####(147014,)####(147015,)####(147016,)####(147017,)####(147018,)####(147019,)####(147020,)####(147021,)####(147022,)####(147023,)####(147024,)####(147025,)####(147026,)####(147027,)####(147028,)####(147029,)####(147030,)####(147031,)####(147032,)####(147033,)####(147034,)####(147035,)####(147036,)####(147037,)####(147038,)####(147039,)####(147040,)####(147041,)####(147042,)####(147043,)####(147044,)####(147045,)####(147046,)####(147047,)####(147048,)####(147049,)####(147050,)####(147051,)####(147052,)####(147053,)####(147054,)####(147055,)####(147056,)####(147057,)####(147058,)####(147059,)####(147060,)####(147061,)####(147062,)####(147063,)####(147064,)####(147065,)####(147066,)####(147067,)####(147068,)####(147069,)####(147070,)####(147071,)####(147072,)####(146802,)####(146803,)####(146804,)####(146805,)####(146806,)####(146807,)####(146808,)####(146809,)####(146810,)####(146811,)####(146812,)####(146813,)####(146814,)####(146815,)####(146816,)####(146817,)####(146818,)####(146819,)####(146820,)####(146821,)####(146822,)####(146823,)####(146824,)####(146825,)####(146826,)####(168792,)####(164294,)####(164295,)####(164296,)####(164297,)####(164298,)####(164299,)####(164300,)####(164301,)####(164307,)####(164309,)####(164310,)####(164323,)####(164324,)####(164325,)####(164326,)####(164327,)####(164328,)####(164334,)####(164335,)####(164336,)####(164337,)####(164338,)####(164339,)####(164340,)####(164341,)####(164358,)####(164359,)####(164360,)####(164361,)####(164362,)####(164363,)####(164364,)####(164365,)####(164366,)####(164367,)####(164368,)####(164369,)####(164370,)####(164371,)####(164372,)####(164373,)####(164374,)####(164375,)####(164376,)####(164377,)####(164378,)####(164379,)####(164380,)####(164381,)####(164382,)####(164383,)####(164384,)####(164385,)####(164386,)####(164387,)####(164388,)####(164389,)####(164390,)####(164391,)####(164392,)####(164393,)####(164394,)####(164395,)####(164396,)####(164397,)####(164398,)####(164399,)####(164400,)####(164401,)####(164402,)####(164193,)####(164205,)####(164206,)####(164207,)####(164208,)####(164209,)####(164210,)####(164211,)####(164212,)####(164213,)####(164214,)####(164215,)####(164216,)####(164217,)####(164218,)####(164219,)####(164220,)####(164221,)####(164222,)####(164223,)####(164224,)####(164225,)####(164226,)####(164227,)####(164228,)####(164229,)####(164246,)####(164247,)####(164248,)####(164249,)####(164250,)####(164251,)####(164252,)####(164253,)####(164254,)####(164255,)####(164256,)####(164257,)####(164258,)####(164259,)####(164260,)####(164261,)####(164262,)####(164263,)####(164264,)####(164265,)####(164281,)####(164282,)####(164283,)####(164284,)####(164285,)####(164286,)####(164287,)####(164288,)####(164289,)####(164290,)####(164291,)####(164292,)####(164293,)####(168749,)####(168762,)####(168763,)####(168764,)####(168765,)####(168766,)####(168767,)####(168768,)####(168769,)####(168770,)####(168771,)####(168772,)####(168773,)####(168791,)####(168793,)####(168794,)####(168795,)####(168796,)####(168797,)####(168815,)####(168816,)####(168817,)####(168818,)####(168819,)####(168820,)####(168821,)####(168823,)####(168827,)####(168830,)####(168831,)####(168832,)####(168850,)####(168851,)####(168852,)####(168853,)####(168854,)####(168855,)####(168856,)####(168857,)####(168858,)####(168859,)####(168860,)####(168861,)####(168862,)####(168863,)####(168864,)####(168865,)####(168866,)####(168867,)####(204624,)####(204642,)####(204643,)####(204644,)####(204645,)####(204646,)####(204647,)####(204648,)####(204649,)####(204650,)####(204651,)####(204652,)####(204653,)####(204654,)####(204629,)####(204655,)####(204656,)####(204657,)####(204760,)####(204761,)####(204762,)####(204658,)####(204659,)####(204660,)####(204669,)####(204670,)####(204671,)####(204672,)####(204673,)####(204674,)####(204675,)####(204676,)####(204677,)####(204678,)####(204763,)####(204764,)####(204765,)####(204766,)####(204767,)####(204768,)####(204769,)####(204770,)####(204771,)####(204772,)####(204773,)####(204774,)####(204775,)####(204776,)####(204679,)####(204680,)####(204681,)####(204682,)####(204683,)####(204684,)####(204685,)####(204686,)####(204687,)####(204688,)####(204689,)####(204690,)####(204691,)####(204692,)####(204693,)####(204694,)####(204695,)####(204696,)####(204699,)####(204701,)####(204702,)####(204703,)####(204704,)####(204705,)####(204712,)####(204713,)####(204714,)####(204715,)####(204716,)####(204717,)####(204722,)####(204724,)####(204725,)####(204726,)####(204727,)####(204728,)####(204729,)####(204730,)####(204731,)####(204732,)####(204744,)####(204745,)####(204746,)####(204747,)####(204748,)####(204749,)####(204750,)####(204751,)####(204752,)####(204753,)####(204754,)####(204755,)####(204756,)####(204757,)####(204758,)####(204759,)####(204777,)####(204778,)####(204779,)####(204780,)####(204781,)####(204782,)####(204783,)####(204784,)####(204785,)####(204786,)####(204787,)####(204788,)####(204789,)####(204790,)####(204791,)####(204792,)####(204793,)####(204794,)####(204795,)####(204796,)####(204797,)####(204798,)####(204799,)####(204800,)####(204801,)####(204802,)####(204630,)####(204631,)####(204632,)####(204633,)####(204634,)####(204635,)####(204636,)####(204637,)####(204638,)####(204639,)####(204640,)####(204641,)####(215364,)####(215295,)####(215302,)####(215303,)####(215304,)####(215305,)####(215306,)####(215307,)####(215308,)####(215309,)####(215310,)####(215311,)####(215312,)####(215313,)####(215496,)####(215497,)####(215498,)####(215499,)####(215500,)####(215501,)####(215502,)####(215503,)####(215504,)####(215505,)####(215506,)####(215314,)####(215315,)####(215316,)####(215317,)####(215318,)####(215319,)####(215320,)####(215321,)####(215322,)####(215323,)####(215365,)####(215324,)####(215325,)####(215366,)####(215326,)####(215327,)####(215328,)####(215367,)####(215329,)####(215368,)####(215330,)####(215331,)####(215332,)####(215369,)####(215333,)####(215370,)####(215334,)####(215335,)####(215371,)####(215336,)####(215337,)####(215372,)####(215338,)####(215339,)####(215373,)####(215340,)####(215341,)####(215374,)####(215342,)####(215375,)####(215343,)####(215355,)####(215356,)####(215376,)####(215357,)####(215358,)####(215377,)####(215359,)####(215378,)####(215360,)####(215379,)####(215361,)####(215380,)####(215362,)####(215363,)####(215381,)####(215382,)####(215383,)####(215384,)####(215385,)####(215386,)####(215387,)####(215388,)####(215389,)####(215390,)####(215391,)####(216056,)####(216060,)####(216061,)####(216062,)####(216063,)####(216064,)####(216065,)####(216066,)####(216067,)####(216068,)####(216069,)####(216070,)####(216071,)####(216072,)####(216073,)####(216074,)####(216075,)####(216076,)####(216077,)####(216078,)####(216079,)####(216080,)####(216081,)####(216082,)####(216083,)####(216084,)####(216085,)####(216086,)####(216087,)####(216088,)####(216089,)####(216090,)####(216091,)####(216092,)####(216101,)####(216102,)####(216103,)####(216104,)####(216105,)####(216106,)####(216107,)####(216108,)####(216109,)####(216110,)####(216111,)####(216112,)####(216113,)####(216114,)####(216115,)####(216116,)####(216117,)####(216118,)####(216119,)####(216120,)####(216121,)####(216122,)####(216123,)####(216124,)####(216125,)####(216126,)####(216127,)####(216128,)####(216132,)####(216133,)####(216134,)####(216141,)####(216142,)####(216143,)####(216144,)####(216145,)####(216146,)####(216147,)####(216148,)####(216149,)####(216150,)####(216151,)####(216152,)####(216154,)####(216155,)####(216159,)####(216160,)####(216161,)####(216162,)####(216163,)####(216164,)####(216165,)####(216166,)####(216175,)####(216176,)####(216177,)####(216178,)####(216179,)####(216180,)####(216181,)####(216182,)####(216183,)####(216184,)####(216185,)####(216186,)####(216187,)####(216188,)####(216189,)####(216190,)####(216191,)####(216192,)####(216193,)####(216194,)####(216195,)####(216196,)####(216197,)####(216198,)####(216199,)####(216200,)####(216201,)####(216202,)####(216203,)####(216204,)####(216205,)####(216206,)####(216207,)####(216208,)####(216209,)####(216210,)####(215397,)####(215400,)####(215403,)####(215410,)####(215411,)####(215413,)####(215414,)####(215416,)####(215418,)####(215419,)####(215421,)####(215425,)####(215431,)####(215433,)####(215434,)####(215435,)####(215436,)####(215437,)####(215443,)####(215446,)####(215447,)####(215448,)####(215449,)####(215462,)####(215463,)####(215464,)####(215465,)####(215466,)####(215467,)####(215468,)####(215469,)####(215470,)####(215471,)####(215472,)####(215473,)####(215474,)####(215475,)####(215476,)####(215477,)####(215478,)####(215479,)####(215480,)####(215481,)####(215482,)####(215483,)####(215484,)####(215485,)####(215486,)####(215487,)####(215488,)####(215489,)####(215490,)####(215491,)####(215492,)####(215493,)####(215494,)####(215495,)####(239271,)####(239303,)####(239304,)####(239305,)####(239306,)####(239307,)####(239308,)####(239309,)####(239310,)####(239311,)####(239312,)####(239313,)####(239314,)####(239315,)####(239316,)####(239317,)####(239318,)####(239319,)####(239320,)####(239321,)####(239322,)####(239323,)####(239324,)####(239325,)####(239326,)####(239327,)####(239328,)####(239329,)####(239330,)####(239331,)####(239339,)####(239340,)####(239341,)####(239389,)####(239390,)####(239391,)####(239399,)####(239400,)####(239401,)####(239402,)####(239403,)####(239404,)####(239405,)####(239406,)####(239407,)####(239408,)####(239409,)####(239410,)####(239411,)####(239412,)####(239413,)####(239342,)####(239343,)####(239344,)####(239345,)####(239346,)####(239347,)####(239348,)####(239349,)####(239350,)####(239351,)####(239352,)####(239353,)####(239354,)####(239355,)####(239356,)####(239357,)####(239358,)####(239359,)####(239360,)####(239361,)####(239362,)####(239363,)####(239364,)####(239365,)####(239366,)####(239367,)####(239368,)####(239369,)####(239370,)####(239371,)####(239372,)####(239373,)####(239374,)####(239375,)####(239376,)####(239377,)####(239378,)####(239379,)####(239380,)####(239381,)####(239382,)####(239383,)####(239384,)####(239385,)####(239386,)####(239387,)####(239388,)####(239487,)####(239414,)####(239415,)####(239416,)####(239417,)####(239418,)####(239419,)####(239420,)####(239421,)####(239422,)####(239423,)####(239424,)####(239425,)####(239426,)####(239427,)####(239428,)####(239429,)####(239430,)####(239431,)####(239432,)####(239433,)####(239434,)####(239435,)####(239436,)####(239437,)####(239438,)####(239439,)####(239440,)####(239441,)####(239442,)####(239443,)####(239444,)####(239445,)####(239446,)####(239447,)####(239448,)####(239449,)####(239450,)####(239451,)####(239456,)####(239458,)####(239460,)####(239461,)####(239462,)####(239463,)####(239464,)####(239465,)####(239466,)####(239468,)####(239469,)####(239470,)####(239471,)####(239473,)####(239474,)####(239475,)####(239476,)####(239477,)####(239488,)####(239490,)####(239491,)####(239492,)####(239501,)####(239502,)####(239503,)####(239504,)####(239505,)####(239506,)####(239507,)####(239508,)####(239509,)####(239510,)####(239511,)####(239512,)####(239513,)####(239514,)####(239515,)####(239516,)####(239517,)####(239518,)####(239519,)####(239520,)####(239521,)####(239522,)####(239523,)####(239524,)####(239525,)####(239526,)####(239527,)####(239528,)####(239529,)####(239530,)####(239531,)####(239532,)####(239533,)####(239534,)####(239535,)####(239536,)####(239537,)####(239538,)####(239539,)####(239540,)####(239541,)####(239542,)####(239543,)####(239544,)####(239545,)####(239546,)####(239547,)####(239548,)####(239549,)####(239550,)####(239551,)####(239552,)####(239553,)####(239554,)####(239555,)####(239556,)####(239557,)####(239558,)####(239559,)####(239560,)####(239561,)####(239562,)####(239563,)####(239564,)####(239565,)####(239566,)####(239567,)####(239568,)####(239569,)####(239570,)####(239571,)####(239572,)####(239573,)####(239574,)####(239575,)####(239576,)####(239577,)####(239578,)####(239579,)####(239580,)####(239581,)####(239582,)####(239583,)####(239584,)####(239585,)####(239586,)####(239587,)####(239588,)####(239478,)####(239479,)####(239480,)####(239481,)####(239482,)####(239483,)####(239484,)####(239485,)####(239589,)####(239590,)####(239591,)####(239592,)####(239593,)####(239594,)####(239595,)####(239596,)####(239597,)####(239598,)####(239599,)####(239600,)####(239601,)####(239602,)####(239603,)####(239604,)####(239605,)####(239606,)####(239607,)####(239608,)####(239609,)####(239275,)####(239276,)####(239277,)####(239278,)####(239279,)####(239280,)####(239281,)####(239282,)####(239283,)####(239284,)####(239285,)####(239286,)####(239287,)####(239288,)####(239289,)####(239290,)####(239291,)####(239292,)####(239293,)####(239294,)####(239295,)####(239296,)####(239297,)####(239298,)####(239299,)####(239300,)####(239301,)####(239302,)####(325232,)####(325293,)####(325294,)####(325235,)####(325295,)####(325296,)####(325297,)####(325236,)####(325298,)####(325237,)####(325299,)####(325300,)####(325238,)####(325301,)####(325402,)####(325403,)####(325404,)####(325405,)####(325406,)####(325302,)####(325303,)####(325304,)####(325239,)####(325306,)####(325307,)####(325240,)####(325308,)####(325309,)####(325310,)####(325241,)####(325317,)####(325242,)####(325318,)####(325243,)####(325319,)####(325244,)####(325320,)####(325245,)####(325321,)####(325322,)####(325246,)####(325323,)####(325324,)####(325247,)####(325325,)####(325326,)####(325248,)####(325327,)####(325328,)####(325249,)####(325331,)####(325332,)####(325250,)####(325333,)####(325251,)####(325335,)####(325337,)####(325252,)####(325345,)####(325253,)####(325346,)####(325254,)####(325255,)####(325256,)####(325257,)####(325258,)####(325259,)####(325260,)####(325261,)####(325347,)####(325348,)####(325349,)####(325350,)####(325351,)####(325352,)####(325353,)####(325354,)####(325355,)####(325356,)####(325357,)####(325358,)####(325262,)####(325359,)####(325360,)####(325263,)####(325361,)####(325362,)####(325264,)####(325363,)####(325364,)####(325365,)####(325265,)####(325366,)####(325367,)####(325368,)####(325266,)####(325369,)####(325370,)####(325267,)####(325371,)####(325268,)####(325372,)####(325373,)####(325276,)####(325374,)####(325375,)####(325277,)####(325376,)####(325377,)####(325278,)####(325378,)####(325379,)####(325279,)####(325380,)####(325381,)####(325280,)####(325382,)####(325383,)####(325384,)####(325281,)####(325385,)####(325386,)####(325282,)####(325387,)####(325388,)####(325283,)####(325389,)####(325284,)####(325390,)####(325391,)####(325285,)####(325392,)####(325286,)####(325393,)####(325287,)####(325394,)####(325395,)####(325288,)####(325396,)####(325397,)####(325289,)####(325398,)####(325290,)####(325399,)####(325400,)####(325401,)####(325291,)####(325292,)####(368628,)####(340240,)####(340241,)####(340244,)####(340248,)####(340254,)####(340255,)####(340256,)####(340257,)####(340262,)####(340264,)####(340265,)####(340266,)####(340267,)####(340268,)####(340269,)####(340270,)####(340271,)####(340272,)####(340273,)####(340274,)####(340275,)####(340276,)####(340277,)####(340278,)####(340279,)####(340280,)####(340281,)####(340282,)####(340283,)####(340284,)####(340285,)####(340286,)####(340287,)####(340288,)####(340289,)####(340290,)####(340291,)####(340302,)####(340303,)####(340304,)####(340305,)####(340306,)####(340307,)####(340308,)####(340309,)####(340310,)####(340311,)####(340312,)####(340313,)####(340314,)####(340315,)####(340316,)####(340319,)####(340320,)####(340327,)####(340328,)####(340329,)####(340330,)####(340331,)####(340332,)####(340333,)####(340338,)####(340339,)####(340340,)####(340341,)####(340342,)####(340343,)####(340344,)####(340355,)####(340356,)####(340357,)####(340358,)####(340359,)####(340360,)####(340361,)####(340362,)####(340363,)####(340364,)####(340365,)####(340366,)####(340367,)####(340368,)####(340369,)####(340370,)####(368609,)####(368614,)####(368615,)####(368616,)####(368617,)####(368618,)####(368619,)####(368620,)####(368621,)####(368622,)####(368623,)####(368624,)####(368625,)####(368626,)####(368627,)####(340371,)####(340372,)####(340373,)####(340374,)####(340375,)####(340376,)####(340377,)####(340378,)####(340379,)####(340380,)####(340381,)####(340382,)####(340383,)####(340384,)####(340385,)####(340386,)####(340387,)####(340388,)####(340389,)####(340390,)####(340391,)####(340392,)####(340393,)####(340394,)####(368747,)####(368748,)####(368749,)####(368750,)####(368629,)####(368630,)####(368631,)####(368632,)####(368633,)####(368642,)####(368643,)####(368644,)####(368645,)####(368646,)####(368647,)####(368648,)####(368649,)####(368650,)####(368651,)####(368652,)####(368653,)####(368654,)####(368655,)####(368656,)####(368657,)####(368666,)####(368667,)####(368668,)####(368669,)####(368670,)####(368671,)####(368672,)####(368673,)####(368674,)####(368675,)####(368676,)####(368677,)####(368678,)####(368679,)####(368680,)####(368681,)####(368683,)####(368684,)####(368685,)####(368692,)####(368693,)####(368694,)####(368695,)####(368696,)####(368697,)####(368698,)####(368699,)####(368703,)####(368704,)####(368705,)####(368706,)####(368716,)####(368717,)####(368718,)####(368719,)####(368720,)####(368721,)####(368722,)####(368723,)####(368724,)####(368725,)####(368726,)####(368727,)####(368728,)####(368729,)####(368730,)####(368731,)####(368732,)####(368733,)####(368734,)####(368735,)####(368736,)####(368737,)####(368738,)####(368739,)####(368740,)####(368741,)####(368742,)####(368743,)####(368744,)####(368745,)####(368746,)####(370576,)####(370401,)####(370413,)####(370414,)####(370415,)####(370416,)####(370417,)####(370418,)####(370419,)####(370420,)####(370421,)####(370422,)####(370423,)####(370424,)####(370425,)####(370426,)####(370427,)####(370428,)####(370429,)####(370430,)####(370431,)####(370432,)####(370433,)####(370434,)####(370435,)####(370436,)####(370437,)####(370453,)####(370454,)####(370455,)####(370456,)####(370457,)####(370458,)####(370459,)####(370460,)####(370461,)####(370462,)####(370463,)####(370464,)####(370465,)####(370466,)####(370467,)####(370468,)####(370469,)####(370470,)####(370471,)####(370472,)####(370473,)####(370475,)####(370481,)####(370482,)####(370495,)####(370496,)####(370497,)####(370498,)####(370499,)####(370500,)####(370505,)####(370507,)####(370509,)####(370511,)####(370512,)####(370513,)####(370515,)####(370516,)####(370517,)####(370518,)####(370519,)####(370536,)####(370537,)####(370538,)####(370539,)####(370540,)####(370541,)####(370542,)####(370543,)####(370544,)####(370545,)####(370546,)####(370547,)####(370548,)####(370549,)####(370550,)####(370551,)####(370552,)####(370553,)####(370554,)####(370555,)####(370556,)####(370557,)####(370558,)####(370559,)####(370560,)####(370561,)####(370562,)####(370563,)####(370564,)####(370565,)####(370566,)####(370567,)####(370568,)####(370569,)####(370570,)####(370571,)####(370572,)####(370573,)####(370574,)####(370575,)####(370577,)####(370578,)####(370579,)####(370580,)####(392930,)####(392937,)####(392938,)####(392939,)####(392940,)####(392941,)####(392942,)####(392943,)####(392944,)####(392945,)####(392946,)####(392947,)####(393047,)####(393048,)####(393049,)####(393050,)####(393051,)####(393052,)####(392948,)####(392949,)####(392950,)####(392951,)####(392952,)####(392953,)####(392954,)####(392961,)####(392962,)####(392963,)####(392964,)####(392965,)####(392966,)####(392967,)####(392968,)####(392969,)####(392970,)####(392971,)####(392972,)####(392973,)####(392974,)####(393053,)####(393054,)####(393055,)####(393056,)####(392975,)####(392976,)####(392977,)####(392978,)####(392980,)####(392982,)####(392983,)####(392984,)####(392991,)####(392992,)####(392993,)####(392994,)####(392995,)####(392996,)####(392933,)####(392997,)####(392998,)####(392999,)####(393001,)####(393002,)####(393003,)####(393004,)####(392934,)####(393005,)####(393006,)####(393007,)####(393008,)####(393009,)####(393010,)####(393018,)####(393019,)####(392935,)####(393020,)####(393021,)####(393022,)####(393023,)####(393024,)####(393025,)####(393026,)####(392936,)####(393027,)####(393028,)####(393029,)####(393030,)####(393031,)####(393032,)####(435111,)####(393033,)####(393034,)####(393035,)####(393036,)####(393037,)####(393038,)####(393039,)####(393040,)####(393041,)####(393042,)####(393043,)####(393044,)####(393045,)####(393046,)####(435021,)####(435032,)####(435038,)####(435044,)####(435045,)####(435046,)####(435047,)####(435048,)####(435049,)####(435050,)####(435051,)####(435052,)####(435053,)####(435054,)####(435055,)####(435056,)####(435057,)####(435058,)####(435059,)####(435060,)####(435061,)####(435062,)####(435063,)####(435114,)####(435064,)####(435065,)####(435081,)####(435115,)####(435082,)####(435083,)####(435116,)####(435084,)####(435119,)####(435085,)####(435086,)####(435087,)####(435120,)####(435088,)####(435089,)####(435122,)####(435090,)####(435091,)####(435123,)####(435092,)####(435093,)####(435129,)####(435094,)####(435095,)####(435130,)####(435096,)####(435097,)####(435143,)####(435098,)####(435144,)####(435099,)####(435100,)####(435145,)####(435101,)####(435146,)####(435148,)####(435157,)####(435158,)####(435159,)####(435160,)####(435161,)####(435162,)####(435163,)####(435179,)####(435180,)####(435181,)####(435182,)####(435183,)####(435184,)####(435185,)####(435186,)####(435187,)####(435188,)####(435189,)####(435190,)####(435191,)####(435192,)####(435193,)####(435194,)####(435195,)####(435196,)####(435197,)####(435198,)####(435199,)####(435200,)####(435201,)####(435202,)####(435203,)####(435204,)####(435205,)####(435206,)####(435207,)####(435208,)####(435209,)####(435210,)####(435211,)####(435212,)####(435213,)####(435214,)####(435215,)####(435216,)####(435217,)####(435218,)####(435219,)####(435220,)####(435221,)####(435222,)####(435223,)####(435224,)####(435225,)####(435226,)####(435227,)####(435228,)####(435229,)####(438476,)####(438478,)####(438480,)####(438483,)####(438485,)####(438487,)####(438641,)####(438642,)####(438489,)####(438643,)####(438644,)####(438645,)####(438646,)####(438647,)####(438491,)####(438648,)####(438649,)####(438650,)####(438651,)####(438652,)####(438492,)####(438653,)####(438654,)####(438655,)####(438656,)####(438657,)####(438658,)####(438659,)####(438493,)####(438660,)####(438661,)####(438662,)####(438663,)####(438664,)####(438494,)####(438665,)####(438666,)####(438667,)####(438668,)####(438495,)####(438669,)####(438670,)####(438671,)####(438672,)####(438673,)####(438674,)####(438496,)####(438675,)####(438676,)####(438677,)####(438678,)####(438679,)####(438680,)####(438681,)####(438497,)####(438682,)####(438683,)####(438684,)####(438685,)####(438686,)####(438687,)####(438498,)####(438688,)####(438689,)####(438690,)####(438691,)####(438692,)####(438693,)####(438694,)####(438499,)####(438695,)####(438696,)####(438697,)####(438501,)####(438502,)####(438503,)####(438504,)####(438505,)####(438506,)####(438507,)####(438508,)####(438509,)####(438510,)####(438511,)####(438512,)####(438513,)####(438514,)####(438515,)####(438516,)####(438517,)####(438518,)####(438519,)####(438520,)####(438521,)####(438522,)####(438523,)####(438568,)####(438571,)####(438573,)####(438575,)####(438578,)####(438579,)####(438581,)####(438585,)####(438586,)####(438591,)####(438593,)####(438600,)####(438601,)####(438602,)####(438603,)####(438604,)####(438605,)####(438606,)####(438608,)####(438612,)####(438615,)####(438617,)####(438618,)####(438619,)####(438620,)####(438623,)####(438624,)####(438634,)####(438635,)####(438636,)####(438637,)####(438638,)####(438639,)####(438640,)####(446786,)####(446787,)####(446788,)####(446789,)####(446790,)####(446791,)####(446792,)####(446721,)####(446793,)####(446730,)####(446798,)####(446731,)####(446732,)####(446733,)####(446799,)####(446734,)####(446735,)####(446736,)####(446801,)####(446737,)####(446802,)####(446738,)####(446739,)####(446740,)####(446809,)####(446741,)####(446742,)####(446743,)####(446810,)####(446744,)####(446814,)####(446745,)####(446746,)####(446815,)####(446747,)####(446748,)####(446817,)####(446749,)####(446818,)####(446750,)####(446820,)####(446751,)####(446752,)####(446753,)####(446826,)####(446754,)####(446755,)####(446827,)####(446756,)####(446828,)####(446757,)####(446771,)####(446772,)####(446830,)####(446773,)####(446774,)####(446831,)####(446775,)####(446776,)####(446832,)####(446777,)####(446778,)####(446833,)####(446779,)####(446834,)####(446780,)####(446835,)####(446781,)####(446782,)####(446836,)####(446783,)####(446784,)####(446785,)####(446850,)####(446851,)####(446852,)####(446853,)####(446854,)####(446855,)####(446856,)####(446857,)####(446858,)####(446859,)####(446860,)####(446861,)####(446862,)####(446863,)####(446864,)####(446865,)####(446866,)####(446867,)####(446868,)####(446869,)####(446870,)####(446871,)####(446872,)####(446873,)####(446874,)####(446875,)####(446876,)####(446877,)####(446878,)####(446879,)####(536128,)####(489900,)####(489905,)####(489906,)####(489907,)####(489908,)####(489909,)####(489910,)####(489911,)####(489912,)####(489913,)####(489914,)####(489915,)####(489916,)####(489917,)####(489918,)####(489919,)####(489920,)####(489921,)####(489922,)####(489923,)####(489924,)####(489925,)####(489926,)####(489927,)####(489928,)####(489929,)####(489930,)####(489931,)####(489932,)####(489933,)####(489934,)####(489935,)####(489936,)####(489945,)####(489946,)####(489947,)####(489948,)####(489949,)####(489950,)####(489951,)####(489952,)####(489953,)####(489954,)####(489955,)####(489956,)####(489957,)####(489958,)####(489959,)####(489960,)####(489961,)####(489962,)####(489963,)####(489964,)####(489965,)####(489966,)####(489967,)####(489968,)####(489969,)####(489970,)####(489971,)####(489972,)####(489974,)####(489975,)####(489977,)####(489978,)####(489979,)####(489980,)####(489981,)####(489988,)####(489989,)####(489990,)####(489991,)####(489992,)####(489993,)####(489994,)####(489995,)####(489996,)####(489997,)####(536279,)####(536280,)####(536281,)####(536282,)####(536283,)####(536284,)####(536285,)####(536286,)####(536297,)####(536298,)####(536299,)####(536300,)####(536301,)####(536302,)####(536303,)####(536304,)####(536305,)####(536306,)####(536307,)####(536308,)####(536309,)####(536310,)####(536311,)####(536312,)####(536313,)####(536314,)####(536315,)####(536316,)####(536317,)####(536318,)####(536319,)####(536320,)####(536321,)####(536322,)####(536323,)####(536324,)####(489998,)####(489999,)####(490000,)####(490003,)####(490004,)####(490005,)####(490006,)####(490007,)####(490008,)####(490009,)####(490010,)####(490011,)####(490012,)####(490013,)####(490014,)####(490015,)####(490016,)####(490017,)####(490027,)####(490028,)####(490029,)####(490030,)####(490031,)####(490032,)####(490033,)####(490034,)####(490035,)####(490036,)####(490037,)####(490038,)####(490039,)####(490040,)####(490041,)####(490042,)####(490043,)####(490044,)####(490045,)####(490046,)####(490047,)####(490048,)####(490049,)####(490050,)####(490051,)####(490052,)####(490053,)####(490054,)####(490055,)####(490056,)####(490057,)####(490058,)####(490059,)####(490060,)####(490061,)####(490062,)####(490063,)####(490064,)####(490065,)####(490066,)####(490067,)####(490068,)####(490069,)####(490070,)####(490071,)####(490072,)####(490073,)####(490074,)####(490075,)####(490076,)####(490077,)####(490078,)####(490079,)####(490080,)####(490081,)####(490082,)####(490083,)####(490084,)####(536185,)####(536186,)####(536187,)####(536188,)####(536198,)####(536199,)####(536200,)####(536201,)####(536202,)####(536203,)####(536204,)####(536205,)####(536206,)####(536207,)####(536208,)####(536209,)####(536210,)####(536211,)####(536212,)####(536213,)####(536214,)####(536215,)####(536216,)####(536217,)####(536218,)####(536219,)####(536220,)####(536221,)####(536222,)####(536223,)####(536224,)####(536225,)####(536226,)####(536227,)####(536228,)####(536229,)####(536230,)####(536231,)####(536232,)####(536233,)####(536234,)####(536235,)####(536236,)####(536237,)####(536238,)####(536239,)####(536240,)####(536241,)####(536242,)####(536243,)####(536244,)####(536245,)####(536246,)####(536247,)####(536248,)####(536250,)####(536251,)####(536252,)####(536253,)####(536255,)####(536256,)####(536257,)####(536258,)####(536259,)####(536260,)####(536261,)####(536262,)####(536263,)####(536266,)####(536267,)####(536268,)####(536269,)####(536270,)####(536271,)####(536272,)####(536273,)####(536274,)####(536275,)####(536276,)####(536277,)####(536278,)####(536379,)####(536380,)####(536381,)####(536382,)####(536383,)####(536384,)####(536385,)####(536386,)####(536387,)####(536388,)####(536389,)####(536390,)####(536391,)####(536392,)####(536393,)####(536394,)####(536395,)####(536396,)####(536397,)####(536398,)####(536399,)####(536400,)####(536401,)####(536325,)####(536326,)####(536327,)####(536328,)####(536329,)####(536330,)####(536331,)####(536332,)####(536333,)####(536334,)####(536335,)####(536336,)####(536337,)####(536338,)####(536339,)####(536340,)####(536341,)####(536342,)####(536343,)####(536344,)####(536345,)####(536346,)####(536347,)####(536348,)####(536349,)####(536350,)####(536351,)####(536352,)####(536353,)####(536354,)####(536355,)####(536356,)####(536357,)####(536358,)####(536359,)####(536360,)####(536361,)####(536362,)####(536363,)####(536364,)####(536365,)####(536366,)####(536367,)####(536368,)####(536369,)####(536370,)####(536371,)####(536372,)####(536373,)####(536374,)####(536375,)####(536376,)####(536377,)####(536378,)####(536134,)####(536135,)####(536136,)####(536137,)####(536138,)####(536139,)####(536140,)####(536141,)####(536142,)####(536143,)####(536144,)####(536145,)####(536146,)####(536147,)####(536148,)####(536149,)####(536150,)####(536151,)####(536152,)####(536153,)####(536154,)####(536155,)####(536156,)####(536157,)####(536158,)####(536159,)####(536160,)####(536161,)####(536162,)####(536163,)####(536164,)####(536165,)####(536166,)####(536167,)####(536168,)####(536169,)####(536170,)####(536171,)####(536172,)####(536173,)####(536174,)####(536175,)####(536176,)####(536177,)####(536178,)####(536179,)####(536180,)####(536181,)####(536182,)####(536183,)####(536184,)####(578256,)####(541074,)####(541081,)####(541082,)####(541083,)####(541084,)####(541085,)####(541086,)####(541087,)####(541088,)####(541089,)####(541090,)####(541091,)####(541092,)####(541093,)####(541094,)####(541095,)####(541096,)####(541097,)####(541098,)####(541110,)####(541111,)####(541112,)####(541113,)####(541114,)####(541115,)####(541116,)####(541117,)####(541118,)####(541119,)####(541120,)####(541121,)####(541122,)####(541124,)####(541126,)####(541127,)####(541128,)####(541130,)####(541133,)####(541134,)####(541135,)####(541136,)####(541148,)####(541149,)####(541150,)####(541151,)####(541152,)####(541153,)####(541154,)####(541155,)####(541156,)####(541157,)####(541158,)####(541159,)####(541160,)####(541161,)####(541162,)####(541163,)####(541164,)####(541165,)####(541166,)####(541167,)####(541168,)####(541169,)####(541170,)####(541171,)####(541172,)####(541173,)####(541174,)####(541175,)####(541176,)####(541177,)####(578272,)####(578273,)####(578274,)####(578275,)####(578276,)####(578277,)####(578278,)####(578279,)####(578280,)####(578281,)####(578282,)####(578283,)####(578284,)####(578285,)####(578286,)####(578287,)####(578288,)####(578289,)####(578290,)####(578291,)####(578292,)####(578293,)####(578294,)####(578295,)####(578296,)####(578297,)####(578298,)####(578299,)####(578300,)####(578301,)####(578302,)####(578303,)####(578304,)####(578305,)####(578306,)####(578307,)####(578308,)####(578309,)####(578310,)####(578311,)####(578312,)####(578313,)####(578314,)####(578315,)####(578316,)####(578325,)####(578326,)####(578327,)####(578328,)####(578329,)####(578330,)####(578331,)####(578332,)####(578333,)####(578334,)####(578335,)####(578336,)####(578337,)####(578338,)####(578339,)####(578340,)####(578341,)####(578342,)####(578343,)####(578344,)####(578345,)####(578346,)####(578347,)####(578348,)####(578349,)####(578350,)####(578351,)####(578352,)####(578353,)####(578354,)####(578355,)####(578356,)####(578357,)####(578358,)####(578359,)####(578360,)####(578361,)####(578482,)####(578483,)####(578484,)####(578485,)####(578486,)####(578487,)####(578488,)####(578489,)####(578490,)####(578491,)####(578492,)####(578493,)####(578494,)####(578495,)####(578496,)####(578497,)####(578498,)####(578499,)####(578500,)####(578501,)####(578502,)####(578503,)####(578504,)####(578505,)####(578506,)####(578507,)####(578508,)####(578509,)####(578510,)####(578511,)####(578512,)####(578513,)####(578514,)####(578515,)####(578516,)####(578517,)####(578518,)####(578519,)####(578520,)####(578521,)####(578522,)####(578523,)####(578524,)####(578525,)####(578526,)####(578527,)####(578528,)####(578529,)####(578530,)####(578531,)####(578532,)####(578533,)####(578534,)####(578535,)####(578536,)####(578537,)####(578538,)####(578539,)####(578540,)####(578541,)####(578362,)####(578363,)####(578364,)####(578365,)####(578366,)####(578367,)####(578368,)####(578369,)####(578370,)####(578371,)####(578372,)####(578373,)####(578374,)####(578375,)####(578376,)####(578378,)####(578381,)####(578383,)####(578386,)####(578387,)####(578388,)####(578390,)####(578398,)####(578399,)####(578400,)####(578401,)####(578402,)####(578403,)####(578404,)####(578405,)####(578406,)####(578407,)####(578408,)####(578409,)####(578416,)####(578417,)####(578418,)####(578419,)####(578420,)####(578421,)####(578423,)####(578424,)####(578426,)####(578427,)####(578428,)####(578429,)####(578431,)####(578432,)####(578433,)####(578434,)####(578435,)####(578436,)####(578445,)####(578446,)####(578447,)####(578448,)####(578449,)####(578450,)####(578451,)####(578452,)####(578453,)####(578454,)####(578455,)####(578456,)####(578457,)####(578458,)####(578459,)####(578460,)####(578461,)####(578462,)####(578463,)####(578464,)####(578465,)####(578466,)####(578467,)####(578468,)####(578469,)####(578470,)####(578471,)####(578472,)####(578473,)####(578474,)####(578475,)####(578476,)####(578477,)####(578478,)####(578479,)####(578480,)####(578481,)####(578542,)####(578260,)####(578261,)####(578262,)####(578263,)####(578264,)####(578265,)####(578266,)####(578267,)####(578268,)####(578269,)####(578270,)####(578271,)####(587589,)####(587597,)####(587598,)####(587599,)####(587600,)####(587601,)####(587602,)####(587603,)####(587604,)####(587605,)####(587606,)####(587607,)####(587608,)####(587609,)####(587610,)####(587611,)####(587713,)####(587612,)####(587714,)####(587613,)####(587715,)####(587722,)####(587625,)####(587723,)####(587626,)####(587724,)####(587725,)####(587627,)####(587726,)####(587727,)####(587628,)####(587734,)####(587735,)####(587736,)####(587737,)####(587750,)####(587751,)####(587752,)####(587753,)####(587754,)####(587755,)####(587756,)####(587757,)####(587758,)####(587759,)####(587760,)####(587761,)####(587762,)####(587763,)####(587764,)####(587765,)####(587766,)####(587767,)####(587768,)####(587769,)####(587770,)####(587771,)####(587772,)####(587773,)####(587774,)####(587775,)####(587776,)####(587777,)####(587629,)####(587630,)####(587631,)####(587632,)####(587633,)####(587634,)####(587635,)####(587636,)####(587637,)####(587649,)####(587650,)####(587651,)####(587652,)####(587653,)####(587654,)####(587655,)####(587656,)####(587657,)####(587658,)####(587659,)####(587660,)####(587661,)####(587674,)####(587675,)####(587676,)####(587677,)####(587678,)####(587679,)####(587680,)####(587681,)####(587682,)####(587683,)####(587684,)####(587685,)####(587697,)####(587698,)####(587699,)####(587700,)####(587701,)####(587702,)####(587703,)####(587704,)####(587705,)####(587706,)####(587707,)####(587708,)####(587709,)####(587711,)####(603319,)####(603320,)####(603321,)####(603322,)####(603323,)####(603324,)####(603325,)####(603326,)####(603327,)####(603328,)####(603329,)####(603345,)####(603346,)####(603347,)####(603348,)####(603349,)####(603350,)####(603351,)####(603352,)####(603353,)####(603361,)####(603305,)####(603316,)####(603317,)####(603362,)####(603318,)####(603363,)####(603364,)####(603365,)####(603366,)####(603382,)####(603383,)####(603384,)####(603385,)####(603386,)####(603387,)####(603388,)####(603389,)####(603390,)####(603391,)####(603392,)####(603393,)####(603394,)####(603395,)####(603396,)####(603397,)####(603398,)####(603399,)####(603400,)####(603401,)####(603402,)####(603403,)####(603404,)####(686636,)####(686680,)####(686681,)####(686682,)####(686683,)####(686684,)####(686685,)####(686647,)####(686686,)####(686687,)####(686688,)####(686689,)####(686690,)####(686691,)####(686769,)####(686770,)####(686771,)####(686772,)####(686773,)####(686774,)####(686775,)####(686776,)####(686777,)####(686692,)####(686693,)####(686694,)####(686695,)####(686696,)####(686697,)####(686698,)####(686699,)####(686700,)####(686701,)####(686702,)####(686703,)####(686704,)####(686705,)####(686706,)####(686707,)####(686708,)####(686724,)####(686725,)####(686726,)####(686727,)####(686728,)####(686729,)####(686730,)####(686731,)####(686732,)####(686733,)####(686734,)####(686735,)####(686736,)####(686737,)####(686738,)####(686739,)####(686740,)####(686741,)####(686742,)####(686743,)####(686744,)####(686745,)####(686746,)####(686747,)####(686748,)####(686778,)####(686779,)####(686780,)####(686795,)####(686796,)####(686797,)####(686798,)####(686799,)####(686800,)####(686801,)####(686802,)####(686803,)####(686804,)####(686805,)####(686806,)####(686807,)####(686808,)####(686809,)####(686810,)####(686811,)####(686812,)####(686813,)####(686814,)####(686815,)####(686816,)####(686817,)####(686818,)####(686819,)####(686820,)####(686821,)####(686822,)####(686823,)####(686824,)####(686825,)####(686826,)####(686827,)####(686828,)####(686829,)####(686830,)####(686831,)####(686832,)####(686833,)####(686834,)####(686749,)####(686750,)####(686751,)####(686752,)####(686753,)####(686754,)####(686755,)####(686756,)####(686757,)####(686758,)####(686759,)####(686760,)####(686761,)####(686762,)####(686763,)####(686764,)####(686765,)####(686766,)####(686767,)####(686768,)####(686877,)####(686880,)####(686881,)####(686882,)####(686886,)####(686887,)####(686888,)####(686893,)####(686894,)####(686896,)####(686898,)####(686899,)####(686900,)####(686901,)####(686908,)####(686909,)####(686913,)####(686915,)####(686916,)####(686917,)####(686918,)####(686919,)####(686920,)####(686923,)####(686924,)####(686925,)####(686926,)####(686927,)####(686928,)####(686930,)####(686931,)####(686932,)####(686933,)####(686934,)####(686950,)####(686951,)####(686952,)####(686953,)####(686954,)####(686955,)####(686956,)####(686957,)####(686958,)####(686959,)####(686960,)####(686961,)####(686962,)####(686963,)####(686964,)####(686965,)####(686966,)####(686967,)####(686968,)####(686969,)####(686970,)####(686971,)####(686972,)####(686973,)####(686974,)####(686975,)####(686976,)####(686835,)####(686836,)####(686837,)####(686838,)####(686839,)####(686840,)####(686841,)####(686842,)####(686843,)####(686844,)####(686845,)####(686846,)####(686847,)####(686848,)####(686849,)####(686850,)####(686851,)####(686852,)####(686863,)####(686864,)####(686867,)####(686868,)####(686869,)####(686870,)####(686876,)####(687007,)####(687008,)####(687009,)####(687010,)####(687011,)####(687012,)####(687013,)####(687014,)####(687015,)####(687016,)####(687017,)####(687018,)####(687019,)####(687020,)####(687021,)####(687022,)####(687023,)####(687024,)####(687025,)####(687026,)####(687027,)####(687028,)####(687029,)####(687030,)####(687031,)####(687032,)####(687033,)####(687034,)####(687035,)####(687036,)####(687037,)####(687038,)####(687039,)####(687040,)####(687041,)####(687042,)####(687043,)####(687044,)####(687045,)####(687046,)####(687047,)####(686977,)####(686978,)####(686979,)####(686980,)####(686981,)####(686982,)####(686983,)####(686984,)####(686985,)####(686986,)####(686987,)####(686988,)####(686989,)####(686990,)####(686991,)####(686992,)####(686993,)####(686994,)####(686995,)####(686996,)####(686997,)####(686998,)####(686999,)####(687000,)####(687001,)####(687002,)####(687003,)####(687004,)####(687005,)####(687006,)####(687048,)####(687049,)####(687050,)####(687051,)####(687052,)####(687053,)####(687054,)####(687055,)####(687056,)####(687057,)####(687058,)####(687059,)####(687060,)####(687061,)####(687062,)####(687063,)####(687064,)####(687065,)####(687066,)####(687067,)####(687068,)####(686648,)####(686649,)####(686650,)####(686651,)####(686652,)####(686653,)####(686654,)####(686655,)####(686656,)####(686657,)####(686658,)####(686659,)####(686660,)####(686661,)####(686662,)####(686663,)####(686664,)####(686665,)####(686666,)####(686667,)####(686668,)####(686669,)####(686670,)####(686671,)####(686672,)####(686673,)####(686674,)####(686675,)####(686676,)####(686677,)####(686678,)####(686679,)####(705261,)####(702420,)####(702421,)####(702422,)####(702423,)####(702424,)####(702425,)####(702426,)####(702427,)####(702428,)####(702429,)####(702430,)####(702431,)####(702432,)####(702433,)####(702434,)####(702435,)####(702436,)####(702437,)####(702438,)####(702439,)####(702440,)####(702441,)####(702442,)####(702443,)####(702444,)####(702445,)####(702447,)####(702450,)####(702452,)####(702453,)####(702454,)####(702461,)####(702462,)####(702463,)####(702464,)####(702465,)####(702466,)####(702468,)####(702471,)####(702472,)####(702473,)####(702474,)####(702475,)####(702477,)####(702479,)####(702480,)####(702489,)####(702490,)####(702491,)####(702492,)####(702493,)####(702494,)####(702495,)####(702496,)####(702497,)####(702498,)####(702499,)####(702500,)####(702501,)####(702502,)####(702503,)####(702504,)####(702505,)####(702506,)####(702507,)####(702508,)####(702509,)####(702510,)####(702511,)####(702512,)####(702513,)####(702514,)####(702515,)####(702516,)####(705245,)####(705247,)####(705248,)####(705249,)####(705250,)####(705251,)####(705252,)####(705253,)####(705254,)####(705255,)####(705256,)####(705257,)####(705258,)####(705259,)####(705260,)####(702373,)####(702377,)####(702378,)####(702379,)####(702380,)####(702381,)####(702382,)####(702383,)####(702384,)####(702385,)####(702386,)####(702387,)####(702388,)####(702389,)####(702390,)####(702391,)####(702392,)####(702393,)####(702394,)####(702395,)####(702396,)####(702397,)####(702398,)####(702399,)####(702400,)####(702401,)####(702402,)####(702403,)####(702404,)####(702405,)####(702406,)####(702407,)####(702408,)####(702409,)####(702417,)####(702418,)####(702419,)####(705369,)####(705370,)####(705371,)####(705372,)####(705373,)####(705374,)####(705375,)####(705376,)####(705377,)####(705378,)####(705379,)####(705380,)####(705381,)####(705382,)####(705383,)####(705384,)####(705385,)####(705386,)####(705387,)####(705388,)####(705389,)####(705390,)####(705391,)####(705392,)####(705393,)####(705394,)####(705395,)####(705396,)####(705397,)####(705398,)####(705399,)####(705400,)####(705401,)####(705402,)####(705403,)####(705404,)####(705405,)####(705406,)####(705407,)####(705408,)####(705409,)####(705410,)####(705411,)####(705412,)####(705413,)####(705414,)####(705415,)####(705416,)####(705262,)####(705263,)####(705264,)####(705265,)####(705266,)####(705267,)####(705268,)####(705269,)####(705270,)####(705271,)####(705272,)####(705273,)####(705274,)####(705275,)####(705276,)####(705277,)####(705278,)####(705279,)####(705280,)####(705281,)####(705287,)####(705288,)####(705289,)####(705290,)####(705291,)####(705292,)####(705293,)####(705294,)####(705295,)####(705296,)####(705297,)####(705298,)####(705299,)####(705300,)####(705301,)####(705302,)####(705303,)####(705304,)####(705305,)####(705306,)####(705307,)####(705308,)####(705309,)####(705310,)####(705311,)####(705312,)####(705313,)####(705314,)####(705315,)####(705316,)####(705317,)####(705318,)####(705322,)####(705324,)####(705325,)####(705329,)####(705330,)####(705334,)####(705335,)####(705336,)####(705337,)####(705338,)####(705339,)####(705340,)####(705341,)####(705343,)####(705344,)####(705345,)####(705354,)####(705355,)####(705356,)####(705357,)####(705358,)####(705359,)####(705360,)####(705361,)####(705362,)####(705363,)####(705364,)####(705365,)####(705366,)####(705367,)####(705368,)####(727628,)####(727629,)####(727630,)####(727631,)####(727632,)####(727560,)####(727640,)####(727564,)####(727565,)####(727641,)####(727566,)####(727567,)####(727568,)####(727712,)####(727713,)####(727714,)####(727715,)####(727716,)####(727718,)####(727721,)####(727722,)####(727723,)####(727724,)####(727569,)####(727570,)####(727571,)####(727572,)####(727573,)####(727574,)####(727575,)####(727576,)####(727577,)####(727578,)####(727579,)####(727580,)####(727581,)####(727582,)####(727725,)####(727726,)####(727727,)####(727728,)####(727729,)####(727730,)####(727731,)####(727732,)####(727733,)####(727734,)####(727735,)####(727736,)####(727737,)####(727738,)####(727739,)####(727740,)####(727741,)####(727742,)####(727751,)####(727752,)####(727753,)####(727754,)####(727755,)####(727756,)####(727757,)####(727758,)####(727759,)####(727760,)####(727761,)####(727762,)####(727763,)####(727764,)####(727765,)####(727766,)####(727767,)####(727768,)####(727769,)####(727770,)####(727771,)####(727772,)####(727773,)####(727774,)####(727775,)####(727776,)####(727777,)####(727778,)####(727779,)####(727780,)####(727781,)####(727782,)####(727583,)####(727584,)####(727585,)####(727586,)####(727587,)####(727588,)####(727589,)####(727590,)####(727591,)####(727592,)####(727593,)####(727594,)####(727595,)####(727596,)####(727597,)####(727598,)####(727599,)####(727600,)####(727601,)####(727602,)####(727603,)####(727604,)####(727605,)####(727606,)####(727607,)####(727608,)####(727609,)####(727610,)####(727611,)####(727612,)####(727613,)####(727614,)####(727615,)####(727616,)####(727617,)####(727618,)####(727619,)####(727620,)####(727621,)####(727622,)####(727623,)####(727624,)####(727625,)####(727626,)####(727627,)####(727814,)####(727783,)####(727784,)####(727785,)####(727786,)####(727787,)####(727788,)####(727789,)####(727790,)####(727791,)####(727792,)####(727793,)####(727794,)####(727795,)####(727796,)####(727797,)####(727798,)####(727799,)####(727800,)####(727801,)####(727802,)####(727803,)####(727804,)####(727805,)####(727806,)####(727807,)####(727808,)####(727809,)####(727810,)####(727811,)####(727812,)####(727813,)####(727642,)####(727643,)####(727644,)####(727645,)####(727646,)####(727647,)####(727648,)####(727649,)####(727650,)####(727651,)####(727652,)####(727653,)####(727654,)####(727655,)####(727656,)####(727657,)####(727658,)####(727659,)####(727660,)####(727661,)####(727662,)####(727663,)####(727664,)####(727665,)####(727666,)####(727667,)####(727668,)####(727669,)####(727670,)####(727671,)####(727672,)####(727673,)####(727674,)####(727675,)####(727676,)####(727677,)####(727678,)####(727679,)####(727680,)####(727681,)####(727682,)####(727683,)####(727684,)####(727685,)####(727686,)####(727687,)####(727688,)####(727689,)####(727690,)####(727691,)####(727692,)####(727693,)####(727694,)####(727695,)####(727696,)####(727697,)####(727698,)####(727699,)####(727700,)####(727701,)####(727702,)####(727703,)####(727704,)####(727706,)####(727707,)####(727708,)####(727709,)####(727710,)####(727711,)####(747900,)####(747855,)####(747856,)####(748060,)####(748061,)####(747857,)####(747858,)####(747859,)####(747860,)####(748064,)####(748068,)####(748069,)####(748071,)####(748072,)####(748073,)####(748075,)####(748076,)####(748082,)####(748083,)####(748084,)####(748085,)####(748086,)####(748087,)####(748088,)####(748089,)####(747861,)####(747862,)####(747863,)####(747864,)####(747865,)####(747866,)####(747867,)####(747868,)####(747869,)####(747870,)####(747871,)####(747872,)####(747873,)####(747874,)####(748090,)####(748091,)####(748092,)####(748093,)####(748094,)####(748095,)####(748096,)####(748097,)####(748098,)####(748099,)####(748100,)####(748101,)####(748102,)####(748103,)####(748104,)####(748105,)####(748106,)####(748107,)####(748108,)####(748109,)####(748110,)####(748111,)####(748112,)####(748113,)####(748114,)####(748115,)####(748116,)####(748117,)####(748118,)####(748119,)####(748120,)####(748121,)####(748122,)####(748123,)####(748124,)####(748125,)####(748126,)####(748127,)####(748128,)####(748129,)####(748130,)####(748131,)####(748132,)####(748133,)####(748134,)####(748135,)####(748136,)####(748137,)####(747875,)####(747876,)####(747877,)####(747878,)####(747879,)####(747880,)####(747881,)####(747882,)####(747883,)####(747884,)####(747885,)####(747886,)####(747887,)####(747888,)####(747889,)####(747890,)####(747891,)####(747892,)####(747893,)####(747894,)####(747895,)####(747896,)####(747897,)####(747898,)####(747899,)####(748138,)####(748139,)####(748140,)####(748141,)####(748142,)####(748143,)####(748144,)####(748145,)####(748146,)####(748147,)####(748148,)####(748149,)####(748150,)####(748151,)####(747901,)####(747902,)####(747944,)####(747945,)####(747946,)####(747947,)####(747948,)####(747949,)####(747950,)####(747951,)####(747952,)####(747953,)####(747954,)####(747955,)####(747956,)####(747957,)####(747958,)####(747959,)####(747960,)####(747961,)####(747962,)####(747963,)####(747964,)####(747965,)####(747966,)####(747967,)####(747968,)####(747969,)####(747970,)####(747971,)####(747972,)####(747973,)####(747974,)####(747980,)####(747981,)####(747982,)####(747983,)####(747984,)####(747985,)####(747986,)####(747987,)####(747988,)####(747989,)####(747990,)####(747991,)####(747992,)####(747993,)####(747994,)####(747995,)####(747996,)####(747997,)####(747998,)####(747999,)####(748000,)####(748001,)####(748002,)####(748003,)####(748004,)####(748005,)####(748006,)####(748007,)####(748008,)####(748009,)####(748010,)####(748011,)####(748017,)####(748018,)####(748019,)####(748022,)####(748024,)####(748030,)####(748032,)####(748033,)####(748039,)####(748042,)####(748055,)####(748056,)####(748057,)####(758680,)####(758581,)####(758582,)####(758583,)####(758681,)####(758584,)####(758585,)####(758586,)####(758587,)####(758682,)####(758588,)####(758589,)####(758590,)####(758591,)####(758592,)####(758593,)####(758567,)####(758579,)####(758580,)####(758594,)####(758595,)####(758596,)####(758597,)####(758598,)####(758599,)####(758600,)####(758601,)####(758602,)####(758603,)####(758604,)####(758683,)####(758605,)####(758606,)####(758607,)####(758608,)####(758609,)####(758610,)####(758611,)####(758793,)####(758794,)####(758811,)####(758812,)####(758813,)####(758814,)####(758815,)####(758816,)####(758817,)####(758818,)####(758612,)####(758613,)####(758614,)####(758615,)####(758616,)####(758617,)####(758618,)####(758619,)####(758620,)####(758621,)####(758622,)####(758623,)####(758624,)####(758625,)####(758626,)####(758627,)####(758628,)####(758629,)####(758630,)####(758631,)####(758632,)####(758633,)####(758634,)####(758635,)####(758636,)####(758637,)####(758638,)####(758639,)####(758655,)####(758656,)####(758657,)####(758658,)####(758659,)####(758819,)####(758820,)####(758821,)####(758822,)####(758823,)####(758824,)####(758825,)####(758826,)####(758827,)####(758828,)####(758829,)####(758830,)####(758831,)####(758832,)####(758833,)####(758834,)####(758835,)####(758836,)####(758837,)####(758838,)####(758839,)####(758840,)####(758841,)####(758842,)####(758843,)####(758844,)####(758845,)####(758846,)####(758847,)####(758848,)####(758849,)####(758850,)####(758851,)####(758852,)####(758853,)####(758854,)####(758855,)####(758660,)####(758661,)####(758662,)####(758663,)####(758664,)####(758665,)####(758666,)####(758667,)####(758668,)####(758669,)####(758670,)####(758671,)####(758672,)####(758673,)####(758674,)####(758675,)####(758676,)####(758677,)####(758678,)####(758679,)####(758900,)####(758901,)####(758902,)####(758903,)####(758904,)####(758905,)####(758906,)####(758907,)####(758908,)####(758909,)####(758910,)####(758911,)####(758912,)####(758913,)####(758914,)####(758915,)####(758916,)####(758917,)####(758918,)####(758919,)####(758920,)####(758921,)####(758922,)####(758923,)####(758924,)####(758925,)####(758926,)####(758927,)####(758856,)####(758857,)####(758858,)####(758859,)####(758860,)####(758861,)####(758862,)####(758863,)####(758864,)####(758865,)####(758866,)####(758867,)####(758868,)####(758869,)####(758870,)####(758871,)####(758872,)####(758873,)####(758874,)####(758875,)####(758876,)####(758877,)####(758878,)####(758879,)####(758880,)####(758881,)####(758882,)####(758883,)####(758884,)####(758885,)####(758886,)####(758887,)####(758888,)####(758889,)####(758890,)####(758891,)####(758892,)####(758893,)####(758894,)####(758895,)####(758896,)####(758897,)####(758898,)####(758899,)####(758684,)####(758685,)####(758686,)####(758687,)####(758688,)####(758689,)####(758690,)####(758691,)####(758692,)####(758693,)####(758694,)####(758695,)####(758696,)####(758697,)####(758698,)####(758699,)####(758700,)####(758701,)####(758702,)####(758703,)####(758704,)####(758705,)####(758706,)####(758707,)####(758708,)####(758709,)####(758710,)####(758711,)####(758714,)####(758715,)####(758716,)####(758717,)####(758720,)####(758721,)####(758722,)####(758723,)####(758725,)####(758726,)####(758727,)####(758728,)####(758729,)####(758742,)####(758743,)####(758744,)####(758745,)####(758746,)####(758747,)####(758748,)####(758749,)####(758750,)####(758751,)####(758752,)####(758753,)####(758754,)####(758755,)####(758756,)####(758757,)####(758758,)####(758759,)####(758760,)####(758761,)####(758762,)####(758763,)####(758764,)####(758765,)####(758773,)####(758775,)####(758776,)####(758778,)####(760155,)####(760156,)####(760157,)####(760158,)####(758779,)####(760159,)####(760160,)####(760161,)####(760162,)####(760163,)####(760164,)####(760165,)####(760166,)####(760167,)####(760168,)####(760097,)####(760104,)####(760105,)####(760106,)####(760107,)####(760108,)####(760109,)####(760110,)####(760111,)####(760112,)####(760169,)####(760171,)####(760174,)####(760176,)####(760177,)####(760178,)####(760185,)####(760186,)####(760187,)####(760188,)####(760189,)####(760190,)####(760191,)####(760192,)####(760193,)####(760194,)####(760195,)####(760196,)####(760197,)####(760202,)####(760203,)####(760204,)####(760205,)####(760206,)####(760207,)####(760208,)####(760209,)####(760210,)####(760211,)####(760212,)####(760113,)####(760114,)####(760115,)####(760116,)####(760117,)####(760118,)####(760119,)####(760120,)####(760121,)####(760122,)####(760123,)####(760124,)####(760125,)####(760126,)####(760127,)####(760128,)####(760129,)####(760130,)####(760131,)####(760132,)####(760133,)####(760144,)####(760145,)####(760146,)####(760147,)####(760148,)####(760149,)####(760150,)####(760151,)####(760213,)####(760214,)####(760215,)####(760216,)####(760228,)####(760229,)####(760230,)####(760231,)####(760232,)####(760233,)####(760234,)####(760235,)####(760236,)####(760237,)####(760238,)####(760239,)####(760240,)####(760241,)####(760242,)####(760243,)####(760244,)####(760245,)####(760246,)####(760247,)####(760248,)####(760249,)####(760250,)####(760251,)####(760252,)####(760253,)####(760254,)####(760255,)####(760256,)####(760257,)####(760258,)####(760259,)####(760260,)####(760261,)####(760262,)####(760263,)####(760264,)####(760265,)####(760266,)####(760267,)####(760268,)####(760269,)####(760152,)####(760153,)####(760154,)####(760270,)####(760271,)####(760272,)####(760273,)####(760274,)####(760275,)####(760276,)####(760277,)####(760278,)####(760279,)####(760280,)####(760281,)####(758780,)####(758781,)####(758782,)####(758783,)####(758784,)####(758785,)####(758786,)####(758787,)####(758788,)####(758789,)####(758790,)####(758791,)####(758792,)####(772511,)####(772517,)####(772518,)####(772519,)####(772520,)####(772521,)####(772522,)####(772523,)####(772524,)####(772525,)####(772526,)####(772527,)####(772528,)####(772529,)####(772530,)####(772555,)####(772653,)####(772654,)####(772655,)####(772656,)####(772657,)####(772658,)####(772659,)####(772660,)####(772556,)####(772557,)####(772558,)####(772559,)####(772560,)####(772561,)####(772562,)####(772563,)####(772564,)####(772565,)####(772566,)####(772661,)####(772662,)####(772663,)####(772664,)####(772665,)####(772666,)####(772667,)####(772668,)####(772669,)####(772670,)####(772671,)####(772672,)####(772673,)####(772674,)####(772675,)####(772676,)####(772677,)####(772678,)####(772679,)####(772680,)####(772681,)####(772682,)####(772683,)####(772684,)####(772685,)####(772686,)####(772687,)####(772688,)####(772689,)####(772690,)####(772691,)####(772702,)####(772703,)####(772704,)####(772705,)####(772706,)####(772707,)####(772708,)####(772709,)####(772710,)####(772711,)####(772567,)####(772568,)####(772569,)####(772570,)####(772571,)####(772582,)####(772583,)####(772584,)####(772585,)####(772586,)####(772587,)####(772588,)####(772589,)####(772590,)####(772591,)####(772592,)####(772593,)####(772594,)####(772595,)####(772596,)####(772597,)####(772598,)####(772599,)####(772600,)####(772601,)####(772602,)####(772603,)####(772604,)####(772605,)####(772606,)####(772607,)####(772608,)####(772609,)####(772610,)####(772611,)####(772612,)####(772613,)####(772614,)####(772615,)####(772616,)####(772617,)####(772618,)####(772619,)####(772620,)####(772712,)####(772713,)####(772714,)####(772715,)####(772716,)####(772717,)####(772718,)####(772719,)####(772720,)####(772721,)####(772722,)####(772723,)####(772724,)####(772725,)####(772726,)####(772727,)####(772728,)####(772729,)####(772730,)####(772731,)####(772732,)####(772733,)####(772734,)####(772735,)####(772736,)####(772737,)####(772738,)####(772739,)####(772740,)####(772741,)####(772742,)####(772743,)####(772744,)####(772745,)####(772746,)####(772747,)####(772748,)####(772749,)####(772750,)####(772751,)####(772752,)####(772753,)####(772754,)####(772755,)####(772756,)####(772763,)####(772764,)####(772765,)####(772766,)####(772767,)####(772768,)####(772769,)####(772770,)####(772771,)####(772772,)####(772773,)####(772774,)####(772775,)####(772776,)####(772777,)####(772778,)####(772779,)####(772780,)####(772781,)####(772782,)####(772783,)####(772784,)####(772785,)####(772786,)####(772788,)####(772793,)####(772794,)####(772795,)####(772796,)####(772797,)####(772798,)####(772799,)####(772800,)####(772801,)####(772802,)####(772803,)####(772804,)####(772805,)####(772806,)####(772807,)####(772808,)####(772809,)####(772810,)####(772811,)####(772812,)####(772813,)####(772824,)####(772825,)####(772826,)####(772827,)####(772828,)####(772829,)####(772830,)####(772831,)####(772832,)####(772833,)####(772834,)####(772835,)####(772836,)####(772837,)####(772838,)####(772621,)####(772622,)####(772623,)####(772624,)####(772625,)####(772626,)####(772627,)####(772628,)####(772629,)####(772630,)####(772631,)####(772642,)####(772643,)####(772644,)####(772645,)####(772646,)####(772647,)####(772648,)####(772649,)####(772650,)####(772651,)####(772652,)####(772839,)####(772840,)####(772841,)####(772842,)####(772843,)####(772844,)####(772845,)####(772846,)####(772847,)####(772848,)####(772849,)####(772850,)####(772851,)####(772852,)####(772853,)####(772854,)####(772855,)####(772856,)####(772857,)####(772858,)####(772859,)####(772860,)####(772861,)####(772862,)####(772863,)####(772864,)####(772865,)####(772866,)####(772867,)####(772868,)####(772869,)####(772870,)####(772871,)####(772872,)####(772873,)####(772874,)####(772875,)####(772876,)####(772877,)####(772878,)####(772879,)####(772880,)####(772881,)####(772882,)####(772883,)####(772884,)####(772885,)####(772886,)####(772887,)####(772888,)####(772889,)####(772890,)####(772891,)####(772892,)####(772893,)####(772894,)####(772895,)####(772896,)####(772897,)####(772898,)####(772899,)####(772900,)####(772901,)####(772902,)####(772903,)####(772904,)####(772905,)####(772906,)####(772907,)####(772908,)####(772909,)####(772910,)####(772911,)####(772912,)####(772913,)####(772914,)####(772915,)####(772916,)####(772917,)####(772918,)####(772919,)####(772920,)####(772921,)####(772922,)####(772923,)####(772924,)####(772925,)####(772926,)####(772531,)####(772927,)####(772928,)####(772532,)####(772533,)####(772534,)####(772535,)####(772536,)####(772537,)####(772538,)####(772539,)####(772540,)####(772541,)####(772542,)####(772543,)####(772544,)####(772545,)####(772546,)####(772547,)####(772548,)####(772549,)####(772550,)####(772551,)####(772552,)####(772553,)####(772554,)####(786009,)####(785881,)####(785882,)####(785774,)####(785783,)####(785784,)####(785785,)####(785786,)####(785787,)####(785788,)####(785789,)####(785790,)####(785791,)####(785792,)####(785793,)####(785794,)####(785795,)####(785796,)####(785797,)####(785798,)####(785799,)####(785800,)####(785801,)####(785802,)####(785803,)####(785804,)####(785805,)####(785806,)####(785807,)####(785808,)####(785809,)####(785810,)####(785824,)####(785825,)####(785826,)####(785827,)####(785828,)####(785829,)####(785830,)####(785831,)####(785832,)####(785833,)####(785834,)####(785835,)####(785836,)####(785837,)####(785838,)####(785839,)####(785840,)####(785841,)####(785842,)####(785843,)####(785844,)####(785845,)####(785846,)####(785860,)####(785861,)####(785862,)####(785863,)####(785864,)####(785865,)####(785866,)####(785867,)####(785868,)####(785869,)####(785896,)####(785897,)####(785898,)####(785899,)####(785900,)####(785901,)####(785902,)####(785903,)####(785904,)####(785905,)####(785906,)####(785907,)####(785908,)####(785909,)####(785910,)####(785911,)####(785912,)####(785913,)####(785914,)####(785915,)####(785916,)####(785917,)####(785918,)####(785931,)####(785932,)####(785933,)####(785934,)####(785935,)####(785936,)####(785937,)####(785938,)####(785939,)####(785940,)####(785941,)####(785942,)####(785943,)####(785944,)####(785945,)####(785946,)####(785947,)####(785948,)####(785949,)####(785950,)####(785951,)####(785952,)####(785953,)####(785954,)####(785956,)####(785962,)####(785963,)####(785969,)####(785970,)####(785971,)####(785972,)####(785973,)####(785974,)####(785975,)####(785976,)####(785977,)####(785978,)####(785992,)####(785993,)####(785994,)####(785995,)####(785996,)####(785997,)####(785998,)####(785999,)####(786000,)####(786001,)####(786002,)####(785870,)####(785871,)####(785872,)####(785873,)####(785874,)####(785875,)####(785876,)####(785877,)####(785878,)####(785879,)####(785880,)####(786003,)####(786004,)####(786005,)####(786006,)####(786007,)####(786008,)####(786010,)####(786011,)####(786012,)####(786013,)####(786014,)####(786015,)####(786016,)####(786017,)####(786018,)####(786019,)####(786020,)####(786021,)####(786022,)####(786023,)####(786024,)####(786025,)####(786026,)####(786027,)####(786028,)####(786029,)####(786030,)####(786031,)####(786032,)####(786033,)####(786034,)####(786035,)####(786036,)####(786037,)####(786038,)####(786039,)####(786040,)####(786041,)####(800847,)####(800916,)####(800917,)####(800918,)####(800919,)####(800920,)####(800921,)####(800922,)####(800923,)####(800924,)####(800925,)####(800926,)####(800927,)####(800928,)####(800929,)####(800930,)####(800931,)####(800932,)####(800933,)####(800934,)####(800935,)####(800936,)####(800937,)####(800938,)####(800939,)####(800940,)####(800941,)####(800942,)####(800943,)####(800944,)####(800945,)####(800946,)####(800947,)####(800948,)####(800949,)####(800950,)####(800951,)####(800952,)####(800953,)####(800954,)####(800955,)####(800956,)####(800957,)####(801034,)####(801035,)####(801036,)####(801038,)####(801040,)####(801041,)####(801044,)####(801050,)####(801053,)####(801055,)####(801056,)####(801057,)####(801058,)####(801059,)####(801060,)####(801061,)####(801062,)####(801063,)####(801064,)####(801065,)####(801066,)####(801067,)####(801068,)####(801069,)####(801070,)####(801072,)####(801073,)####(801076,)####(801084,)####(801086,)####(801087,)####(801088,)####(801089,)####(801096,)####(801097,)####(801098,)####(801099,)####(801100,)####(801101,)####(801102,)####(801103,)####(801104,)####(801105,)####(801107,)####(801108,)####(801111,)####(801113,)####(801114,)####(801115,)####(801116,)####(801117,)####(801118,)####(801119,)####(801120,)####(801121,)####(801122,)####(801123,)####(801124,)####(801125,)####(801131,)####(801132,)####(801133,)####(801134,)####(801135,)####(801136,)####(801137,)####(801138,)####(801139,)####(801140,)####(801141,)####(801143,)####(801144,)####(801145,)####(801147,)####(801148,)####(801149,)####(801150,)####(801151,)####(801152,)####(801154,)####(801156,)####(801157,)####(801169,)####(801170,)####(801171,)####(801172,)####(801173,)####(801174,)####(801175,)####(801176,)####(801177,)####(801178,)####(801179,)####(801180,)####(801181,)####(801182,)####(801183,)####(801184,)####(801185,)####(801186,)####(801187,)####(801188,)####(801189,)####(801190,)####(801191,)####(801192,)####(801193,)####(801194,)####(801195,)####(801196,)####(801197,)####(801198,)####(801199,)####(801200,)####(801201,)####(801202,)####(801203,)####(801204,)####(801205,)####(801206,)####(801207,)####(801208,)####(801209,)####(801210,)####(801211,)####(801212,)####(801213,)####(801214,)####(801215,)####(801216,)####(801217,)####(801218,)####(801219,)####(801220,)####(801221,)####(801222,)####(801223,)####(801224,)####(801225,)####(801226,)####(801227,)####(801228,)####(801229,)####(801230,)####(801231,)####(801232,)####(801233,)####(801234,)####(801235,)####(801236,)####(801237,)####(801238,)####(801239,)####(801240,)####(801241,)####(801242,)####(801243,)####(801244,)####(801245,)####(801246,)####(801247,)####(801248,)####(801249,)####(801250,)####(801251,)####(801252,)####(800849,)####(801253,)####(801254,)####(801255,)####(800851,)####(801256,)####(800852,)####(801257,)####(800854,)####(801258,)####(800856,)####(801259,)####(801260,)####(800857,)####(801261,)####(801262,)####(801263,)####(800859,)####(801264,)####(801265,)####(800862,)####(801266,)####(801267,)####(800863,)####(801268,)####(801269,)####(800868,)####(801270,)####(800869,)####(801271,)####(801272,)####(800873,)####(801273,)####(801274,)####(800874,)####(801275,)####(801276,)####(800875,)####(801277,)####(801278,)####(800876,)####(801279,)####(801280,)####(801281,)####(800877,)####(801282,)####(801283,)####(800878,)####(801284,)####(800879,)####(801285,)####(801286,)####(800880,)####(801287,)####(801288,)####(801289,)####(800881,)####(801290,)####(801291,)####(800882,)####(801292,)####(801293,)####(800883,)####(801294,)####(800884,)####(801295,)####(801296,)####(800885,)####(801297,)####(801298,)####(801299,)####(800886,)####(801300,)####(800887,)####(801301,)####(801302,)####(800888,)####(801303,)####(801304,)####(800889,)####(801305,)####(800890,)####(801306,)####(801307,)####(800891,)####(801308,)####(800892,)####(801309,)####(800893,)####(801310,)####(800894,)####(801311,)####(800895,)####(801312,)####(800896,)####(800897,)####(800898,)####(800899,)####(800900,)####(800901,)####(800902,)####(800903,)####(800904,)####(800905,)####(800906,)####(800907,)####(800908,)####(800909,)####(800910,)####(800911,)####(800912,)####(800913,)####(800914,)####(800915,)####(820961,)####(821033,)####(821045,)####(821046,)####(821049,)####(821053,)####(821073,)####(821074,)####(821075,)####(821077,)####(821078,)####(821095,)####(821096,)####(821097,)####(821098,)####(821099,)####(821100,)####(821101,)####(821102,)####(821103,)####(821104,)####(821105,)####(821106,)####(821107,)####(821108,)####(821109,)####(821110,)####(821111,)####(821112,)####(821113,)####(821114,)####(821115,)####(821116,)####(821117,)####(821118,)####(820973,)####(820974,)####(820975,)####(820976,)####(820977,)####(820978,)####(820979,)####(820980,)####(820981,)####(820982,)####(820983,)####(820984,)####(820985,)####(821002,)####(821003,)####(821004,)####(821005,)####(821006,)####(821007,)####(821008,)####(821009,)####(821025,)####(821026,)####(821027,)####(821028,)####(821029,)####(821030,)####(821031,)####(821032,)####(863359,)####(863360,)####(863361,)####(863362,)####(863363,)####(863364,)####(863277,)####(863284,)####(863365,)####(863285,)####(863286,)####(863366,)####(863287,)####(863288,)####(863367,)####(863289,)####(863368,)####(863290,)####(863291,)####(863369,)####(863292,)####(863370,)####(863293,)####(888885,)####(863294,)####(863295,)####(863296,)####(863297,)####(863298,)####(863299,)####(863300,)####(863301,)####(863313,)####(863314,)####(863315,)####(863316,)####(863317,)####(863318,)####(863319,)####(863320,)####(863321,)####(863322,)####(863323,)####(863324,)####(863325,)####(863326,)####(863327,)####(863332,)####(863333,)####(863334,)####(863335,)####(863336,)####(863348,)####(863349,)####(863350,)####(863351,)####(863352,)####(863353,)####(863354,)####(863355,)####(863356,)####(863357,)####(863358,)####(888968,)####(888969,)####(888970,)####(888971,)####(888972,)####(888973,)####(888974,)####(888975,)####(888976,)####(888977,)####(888978,)####(888979,)####(888980,)####(888981,)####(888982,)####(888983,)####(888984,)####(888985,)####(888986,)####(888993,)####(888994,)####(888995,)####(888996,)####(888997,)####(888998,)####(888999,)####(889000,)####(889001,)####(889002,)####(889003,)####(889004,)####(889005,)####(889006,)####(889007,)####(889008,)####(889009,)####(889010,)####(889011,)####(889012,)####(889013,)####(889014,)####(889016,)####(889017,)####(889018,)####(889019,)####(889021,)####(889022,)####(889028,)####(889029,)####(889030,)####(889031,)####(889032,)####(889033,)####(889034,)####(888836,)####(888843,)####(888844,)####(888845,)####(888846,)####(888847,)####(888848,)####(888849,)####(888850,)####(888851,)####(888852,)####(888853,)####(888854,)####(888855,)####(888856,)####(888857,)####(888858,)####(888859,)####(888860,)####(888861,)####(888862,)####(888863,)####(888864,)####(888865,)####(888866,)####(888867,)####(888868,)####(888869,)####(888870,)####(888871,)####(888872,)####(888873,)####(888874,)####(888875,)####(888876,)####(888877,)####(888878,)####(888879,)####(888880,)####(888881,)####(888882,)####(888883,)####(888884,)####(889084,)####(889085,)####(889086,)####(889087,)####(889088,)####(889089,)####(889090,)####(889091,)####(889092,)####(889093,)####(889094,)####(889095,)####(889096,)####(889097,)####(889098,)####(889099,)####(889100,)####(889101,)####(889102,)####(889103,)####(889104,)####(889105,)####(889106,)####(889107,)####(889108,)####(889109,)####(889110,)####(889111,)####(889112,)####(889113,)####(889114,)####(889115,)####(889116,)####(889117,)####(889118,)####(889119,)####(889120,)####(889121,)####(889122,)####(889123,)####(889124,)####(889125,)####(889126,)####(889127,)####(889128,)####(889129,)####(889130,)####(889131,)####(889132,)####(889133,)####(889134,)####(889135,)####(889136,)####(889137,)####(889138,)####(889139,)####(889140,)####(889141,)####(889142,)####(889143,)####(889144,)####(889145,)####(889146,)####(889147,)####(889148,)####(889149,)####(889150,)####(889151,)####(889152,)####(889153,)####(889154,)####(889155,)####(889156,)####(889035,)####(889036,)####(889037,)####(889038,)####(889039,)####(889040,)####(889041,)####(889042,)####(889043,)####(889044,)####(889045,)####(889046,)####(889047,)####(889049,)####(889050,)####(889051,)####(889052,)####(889064,)####(889065,)####(889066,)####(889067,)####(889068,)####(889069,)####(889070,)####(889071,)####(889072,)####(889073,)####(889074,)####(889075,)####(889076,)####(889077,)####(889078,)####(889079,)####(889080,)####(889081,)####(889082,)####(889083,)####(888886,)####(888887,)####(888888,)####(888889,)####(888890,)####(888891,)####(888892,)####(888893,)####(888894,)####(888895,)####(888896,)####(888897,)####(888898,)####(888899,)####(888900,)####(888901,)####(888902,)####(888903,)####(888904,)####(888905,)####(888906,)####(888907,)####(888908,)####(888919,)####(888920,)####(888921,)####(888922,)####(888923,)####(888924,)####(888925,)####(888926,)####(888927,)####(888928,)####(888929,)####(888930,)####(888931,)####(888932,)####(888933,)####(888934,)####(888935,)####(888936,)####(888937,)####(888938,)####(888939,)####(888940,)####(888941,)####(888942,)####(888943,)####(888944,)####(888945,)####(888946,)####(888947,)####(888948,)####(888949,)####(888950,)####(888951,)####(888952,)####(888953,)####(888954,)####(888955,)####(888956,)####(888957,)####(888958,)####(888959,)####(888960,)####(888961,)####(888962,)####(888963,)####(888964,)####(888965,)####(888966,)####(888967,)####(911203,)####(911137,)####(911149,)####(911150,)####(911151,)####(911152,)####(911153,)####(911154,)####(911155,)####(911156,)####(911157,)####(911158,)####(911159,)####(911160,)####(911161,)####(911162,)####(911163,)####(911164,)####(911165,)####(911166,)####(911310,)####(911311,)####(911312,)####(911313,)####(911314,)####(911315,)####(911316,)####(911317,)####(911318,)####(911319,)####(911320,)####(911321,)####(911322,)####(911323,)####(911324,)####(911325,)####(911326,)####(911327,)####(911328,)####(911329,)####(911335,)####(911336,)####(911337,)####(911338,)####(911339,)####(911340,)####(911341,)####(911342,)####(911343,)####(911344,)####(911345,)####(911346,)####(911347,)####(911348,)####(911349,)####(911350,)####(911351,)####(911352,)####(911353,)####(911354,)####(911355,)####(911356,)####(911357,)####(911358,)####(911359,)####(911360,)####(911361,)####(911362,)####(911363,)####(911364,)####(911365,)####(911382,)####(911383,)####(911384,)####(911385,)####(911386,)####(911387,)####(911388,)####(911389,)####(911390,)####(911391,)####(911392,)####(911167,)####(911168,)####(911169,)####(911170,)####(911171,)####(911172,)####(911173,)####(911174,)####(911175,)####(911176,)####(911177,)####(911178,)####(911179,)####(911180,)####(911181,)####(911182,)####(911183,)####(911184,)####(911185,)####(911186,)####(911187,)####(911188,)####(911189,)####(911190,)####(911191,)####(911192,)####(911193,)####(911194,)####(911195,)####(911196,)####(911197,)####(911198,)####(911199,)####(911200,)####(911201,)####(911202,)####(911424,)####(911425,)####(911426,)####(911427,)####(911428,)####(911429,)####(911430,)####(911431,)####(911432,)####(911433,)####(911393,)####(911394,)####(911395,)####(911396,)####(911397,)####(911398,)####(911399,)####(911400,)####(911401,)####(911402,)####(911403,)####(911404,)####(911405,)####(911406,)####(911407,)####(911408,)####(911409,)####(911410,)####(911411,)####(911412,)####(911413,)####(911414,)####(911415,)####(911416,)####(911417,)####(911418,)####(911419,)####(911420,)####(911421,)####(911422,)####(911423,)####(911434,)####(911435,)####(911436,)####(911437,)####(911438,)####(911439,)####(911440,)####(911441,)####(911442,)####(911443,)####(911444,)####(911445,)####(911446,)####(911447,)####(911448,)####(911449,)####(911204,)####(911205,)####(911206,)####(911207,)####(911208,)####(911209,)####(911225,)####(911226,)####(911227,)####(911228,)####(911229,)####(911230,)####(911231,)####(911232,)####(911233,)####(911234,)####(911235,)####(911236,)####(911237,)####(911238,)####(911239,)####(911240,)####(911241,)####(911242,)####(911243,)####(911244,)####(911245,)####(911246,)####(911247,)####(911248,)####(911249,)####(911250,)####(911251,)####(911252,)####(911253,)####(911254,)####(911255,)####(911256,)####(911257,)####(911258,)####(911259,)####(911260,)####(911261,)####(911262,)####(911263,)####(911264,)####(911265,)####(911266,)####(911267,)####(911268,)####(911269,)####(911270,)####(911271,)####(911272,)####(911273,)####(911274,)####(911275,)####(911276,)####(911277,)####(911278,)####(911279,)####(911280,)####(911281,)####(911283,)####(911284,)####(911285,)####(911286,)####(911287,)####(911289,)####(911290,)####(911291,)####(911292,)####(911293,)####(911306,)####(911307,)####(911308,)####(911309,)####(917788,)####(917789,)####(917790,)####(917791,)####(917792,)####(917793,)####(917794,)####(917795,)####(917796,)####(917797,)####(917734,)####(917746,)####(917747,)####(917748,)####(917749,)####(917750,)####(917751,)####(917798,)####(917752,)####(917753,)####(917754,)####(917755,)####(917756,)####(917757,)####(917758,)####(917759,)####(917878,)####(917880,)####(917882,)####(917883,)####(917884,)####(917887,)####(917888,)####(917889,)####(917760,)####(917761,)####(917762,)####(917763,)####(917764,)####(917765,)####(917766,)####(917767,)####(917768,)####(917769,)####(917770,)####(917771,)####(917772,)####(917773,)####(917774,)####(917775,)####(917776,)####(917777,)####(917778,)####(917779,)####(917780,)####(917781,)####(917782,)####(917783,)####(917784,)####(917785,)####(917786,)####(917787,)####(917890,)####(917892,)####(917893,)####(917894,)####(917895,)####(917896,)####(917909,)####(917910,)####(917911,)####(917912,)####(917913,)####(917914,)####(917915,)####(917916,)####(917917,)####(917918,)####(917919,)####(917920,)####(917921,)####(917922,)####(917923,)####(917924,)####(917925,)####(917926,)####(917927,)####(917928,)####(918012,)####(918013,)####(918014,)####(918015,)####(918016,)####(918017,)####(918018,)####(918019,)####(918020,)####(918021,)####(918022,)####(918023,)####(918024,)####(918025,)####(918026,)####(918027,)####(918028,)####(918029,)####(918030,)####(918031,)####(918032,)####(918033,)####(918034,)####(918035,)####(918036,)####(918037,)####(918038,)####(918039,)####(918040,)####(918041,)####(918042,)####(918043,)####(918044,)####(918045,)####(918046,)####(918047,)####(918048,)####(918049,)####(918050,)####(918051,)####(918052,)####(918053,)####(918054,)####(918055,)####(918056,)####(918057,)####(918058,)####(918059,)####(918060,)####(918061,)####(918062,)####(918063,)####(917929,)####(917930,)####(917931,)####(917932,)####(917938,)####(917939,)####(917941,)####(917942,)####(917944,)####(917945,)####(917946,)####(917947,)####(917948,)####(917949,)####(917950,)####(917951,)####(917952,)####(917953,)####(917954,)####(917955,)####(917956,)####(917957,)####(917958,)####(917959,)####(917960,)####(917961,)####(917962,)####(917963,)####(917964,)####(917965,)####(917966,)####(917967,)####(917984,)####(917985,)####(917986,)####(917987,)####(917988,)####(917989,)####(917990,)####(917991,)####(917992,)####(917993,)####(917994,)####(917995,)####(917996,)####(917997,)####(917998,)####(917999,)####(918000,)####(918001,)####(918002,)####(918003,)####(918004,)####(918005,)####(918064,)####(918065,)####(918066,)####(918067,)####(918068,)####(918069,)####(918070,)####(918071,)####(918072,)####(918073,)####(918074,)####(918075,)####(918076,)####(918077,)####(918006,)####(918007,)####(918008,)####(918009,)####(918010,)####(918011,)####(917799,)####(917800,)####(917801,)####(917802,)####(917803,)####(917804,)####(917805,)####(917806,)####(917823,)####(917824,)####(917825,)####(917826,)####(917827,)####(917828,)####(917829,)####(917830,)####(917831,)####(917832,)####(917833,)####(917834,)####(917835,)####(917836,)####(917837,)####(917838,)####(917839,)####(917840,)####(917841,)####(917842,)####(917843,)####(917844,)####(917845,)####(917846,)####(917847,)####(917848,)####(917849,)####(917850,)####(917851,)####(917852,)####(917853,)####(917854,)####(917855,)####(917856,)####(917857,)####(917858,)####(917859,)####(917860,)####(917861,)####(917862,)####(917863,)####(917864,)####(917865,)####(917866,)####(917867,)####(917868,)####(917869,)####(917870,)####(917871,)####(917872,)####(917873,)####(917874,)####(917875,)####(917876,)####(917877,)####(944309,)####(944256,)####(944265,)####(944266,)####(944267,)####(944268,)####(944269,)####(944270,)####(944271,)####(944272,)####(944273,)####(944274,)####(944275,)####(944276,)####(944277,)####(944278,)####(944279,)####(944280,)####(944281,)####(944282,)####(944283,)####(944395,)####(944396,)####(944397,)####(944398,)####(944399,)####(944400,)####(944414,)####(944415,)####(944416,)####(944417,)####(944418,)####(944419,)####(944284,)####(944285,)####(944286,)####(944287,)####(944288,)####(944289,)####(944290,)####(944291,)####(944292,)####(944293,)####(944294,)####(944295,)####(944296,)####(944297,)####(944298,)####(944299,)####(944300,)####(944301,)####(944302,)####(944303,)####(944304,)####(944305,)####(944306,)####(944307,)####(944308,)####(944490,)####(944497,)####(944498,)####(944499,)####(944500,)####(944501,)####(944503,)####(944504,)####(944505,)####(944506,)####(944507,)####(944508,)####(944509,)####(944510,)####(944511,)####(944512,)####(944513,)####(944514,)####(944515,)####(944516,)####(944517,)####(944518,)####(944519,)####(944520,)####(944521,)####(944522,)####(944523,)####(944524,)####(944525,)####(944526,)####(944530,)####(944420,)####(944421,)####(944422,)####(944423,)####(944424,)####(944425,)####(944426,)####(944427,)####(944428,)####(944429,)####(944430,)####(944431,)####(944432,)####(944433,)####(944434,)####(944435,)####(944436,)####(944437,)####(944438,)####(944439,)####(944440,)####(944441,)####(944442,)####(944443,)####(944444,)####(944445,)####(944446,)####(944447,)####(944448,)####(944449,)####(944450,)####(944451,)####(944452,)####(944453,)####(944454,)####(944455,)####(944456,)####(944457,)####(944458,)####(944459,)####(944460,)####(944461,)####(944462,)####(944463,)####(944464,)####(944465,)####(944466,)####(944467,)####(944468,)####(944469,)####(944470,)####(944471,)####(944472,)####(944474,)####(944476,)####(944477,)####(944478,)####(944480,)####(944481,)####(944482,)####(944483,)####(944484,)####(944486,)####(944487,)####(944488,)####(944489,)####(944604,)####(944605,)####(944606,)####(944607,)####(944608,)####(944609,)####(944610,)####(944611,)####(944612,)####(944613,)####(944614,)####(944615,)####(944616,)####(944617,)####(944618,)####(944619,)####(944620,)####(944531,)####(944532,)####(944533,)####(944534,)####(944535,)####(944536,)####(944537,)####(944538,)####(944539,)####(944540,)####(944541,)####(944542,)####(944543,)####(944544,)####(944545,)####(944546,)####(944547,)####(944548,)####(944549,)####(944550,)####(944551,)####(944565,)####(944566,)####(944567,)####(944568,)####(944569,)####(944570,)####(944571,)####(944572,)####(944573,)####(944574,)####(944575,)####(944576,)####(944577,)####(944578,)####(944579,)####(944580,)####(944581,)####(944582,)####(944583,)####(944584,)####(944585,)####(944586,)####(944587,)####(944588,)####(944589,)####(944590,)####(944591,)####(944592,)####(944593,)####(944594,)####(944595,)####(944596,)####(944597,)####(944598,)####(944599,)####(944600,)####(944601,)####(944602,)####(944603,)####(944621,)####(944622,)####(944623,)####(944624,)####(944625,)####(944626,)####(944627,)####(944628,)####(944629,)####(944630,)####(944631,)####(944632,)####(944633,)####(944634,)####(944635,)####(944636,)####(944637,)####(944638,)####(944639,)####(944640,)####(944641,)####(944642,)####(944643,)####(944644,)####(944645,)####(944646,)####(944647,)####(944648,)####(944649,)####(944650,)####(944310,)####(944311,)####(944312,)####(944313,)####(944314,)####(944315,)####(944316,)####(944317,)####(944318,)####(944319,)####(944320,)####(944321,)####(944322,)####(944323,)####(944324,)####(944325,)####(944326,)####(944327,)####(944328,)####(944341,)####(944342,)####(944343,)####(944344,)####(944345,)####(944346,)####(944347,)####(944348,)####(944349,)####(944350,)####(944351,)####(944352,)####(944353,)####(944354,)####(944355,)####(944356,)####(944357,)####(944358,)####(944359,)####(944360,)####(944361,)####(944362,)####(944363,)####(944364,)####(944365,)####(944366,)####(944367,)####(944368,)####(944369,)####(944370,)####(944371,)####(944372,)####(944373,)####(944374,)####(944375,)####(944376,)####(944377,)####(944378,)####(944379,)####(944380,)####(944381,)####(944382,)####(944383,)####(944384,)####(944385,)####(944386,)####(944387,)####(944388,)####(944389,)####(944390,)####(944391,)####(944392,)####(944393,)####(944394,)####(950103,)####(950110,)####(950111,)####(950112,)####(950113,)####(950114,)####(950115,)####(950116,)####(950117,)####(950118,)####(950119,)####(950120,)####(950121,)####(950122,)####(950123,)####(950124,)####(950125,)####(950126,)####(950127,)####(950128,)####(950129,)####(950130,)####(950131,)####(950132,)####(950133,)####(950134,)####(950135,)####(950136,)####(950137,)####(950138,)####(950139,)####(950150,)####(950218,)####(950151,)####(950219,)####(950220,)####(950152,)####(950221,)####(950222,)####(950153,)####(950223,)####(950154,)####(950224,)####(950155,)####(950225,)####(950226,)####(950227,)####(950156,)####(950228,)####(950229,)####(950157,)####(950230,)####(950231,)####(950158,)####(950232,)####(950159,)####(950233,)####(950234,)####(950160,)####(950235,)####(950161,)####(950236,)####(950237,)####(950238,)####(950162,)####(950239,)####(950163,)####(950240,)####(950241,)####(950242,)####(950164,)####(950243,)####(950244,)####(950165,)####(950245,)####(950166,)####(950246,)####(950167,)####(950247,)####(950168,)####(950248,)####(950249,)####(950169,)####(950250,)####(950170,)####(950251,)####(950252,)####(950171,)####(950253,)####(950254,)####(950255,)####(950172,)####(950256,)####(950257,)####(950173,)####(950258,)####(950259,)####(950174,)####(950260,)####(950175,)####(950261,)####(950179,)####(950180,)####(950262,)####(950263,)####(950181,)####(950183,)####(950186,)####(950187,)####(950188,)####(950189,)####(950190,)####(950191,)####(950192,)####(950193,)####(950194,)####(950195,)####(950196,)####(950197,)####(950209,)####(950210,)####(950211,)####(950212,)####(950213,)####(950214,)####(950215,)####(950216,)####(950217,)####(975914,)####(975923,)####(975924,)####(976045,)####(976046,)####(976047,)####(976048,)####(975925,)####(976049,)####(976050,)####(976051,)####(976052,)####(976053,)####(976054,)####(976080,)####(976081,)####(976082,)####(976083,)####(976084,)####(976085,)####(976086,)####(976087,)####(976088,)####(976089,)####(976090,)####(976091,)####(976092,)####(976093,)####(976094,)####(976095,)####(976096,)####(976097,)####(976098,)####(976099,)####(976100,)####(976101,)####(976102,)####(976103,)####(976104,)####(976105,)####(976106,)####(976107,)####(976108,)####(976110,)####(976111,)####(976112,)####(976114,)####(976115,)####(976116,)####(976117,)####(976118,)####(976119,)####(976120,)####(976121,)####(976122,)####(976135,)####(976136,)####(976137,)####(976138,)####(976139,)####(976140,)####(976141,)####(976142,)####(976143,)####(976144,)####(976145,)####(976146,)####(976147,)####(976148,)####(976149,)####(976150,)####(976151,)####(976152,)####(976153,)####(976154,)####(976155,)####(976156,)####(976157,)####(976158,)####(976159,)####(976160,)####(976161,)####(976162,)####(976163,)####(976164,)####(976165,)####(976166,)####(976167,)####(976168,)####(976169,)####(976170,)####(976171,)####(976172,)####(976173,)####(976174,)####(976175,)####(976176,)####(976177,)####(976178,)####(976179,)####(976180,)####(976181,)####(976182,)####(976183,)####(976184,)####(976185,)####(976186,)####(976187,)####(976188,)####(976189,)####(976190,)####(976191,)####(976192,)####(976193,)####(976194,)####(976195,)####(976196,)####(976197,)####(976198,)####(976199,)####(976200,)####(976201,)####(976202,)####(976203,)####(975926,)####(975927,)####(975928,)####(975929,)####(975930,)####(975931,)####(975932,)####(975933,)####(975934,)####(975935,)####(975936,)####(975937,)####(975938,)####(975939,)####(975940,)####(975941,)####(975942,)####(975943,)####(975944,)####(975945,)####(975946,)####(975947,)####(975948,)####(975949,)####(975950,)####(975951,)####(975952,)####(975953,)####(975954,)####(975955,)####(975956,)####(975957,)####(975958,)####(975959,)####(975960,)####(975961,)####(975962,)####(975963,)####(975964,)####(975965,)####(975966,)####(975967,)####(975968,)####(975969,)####(975970,)####(975971,)####(975972,)####(975973,)####(975974,)####(975975,)####(975976,)####(975977,)####(975978,)####(975979,)####(975980,)####(975981,)####(975982,)####(975983,)####(975984,)####(975991,)####(975994,)####(975996,)####(975998,)####(975999,)####(976001,)####(976002,)####(976003,)####(976005,)####(976006,)####(976007,)####(976009,)####(976011,)####(976012,)####(976013,)####(976015,)####(976017,)####(976018,)####(976019,)####(976020,)####(976032,)####(976033,)####(976034,)####(976035,)####(976041,)####(976042,)####(976043,)####(976044,)####(1006334,)####(1006348,)####(1006341,)####(1006349,)####(1006350,)####(1006351,)####(1006342,)####(1006352,)####(1006432,)####(1006433,)####(1006434,)####(1006435,)####(1006436,)####(1006437,)####(1006438,)####(1006440,)####(1006445,)####(1006448,)####(1006449,)####(1006450,)####(1006353,)####(1006354,)####(1006355,)####(1006356,)####(1006357,)####(1006358,)####(1006359,)####(1006360,)####(1006361,)####(1006362,)####(1006363,)####(1006364,)####(1006365,)####(1006366,)####(1006367,)####(1006368,)####(1006369,)####(1006451,)####(1006452,)####(1006453,)####(1006454,)####(1006455,)####(1006456,)####(1006457,)####(1006458,)####(1006459,)####(1006460,)####(1006461,)####(1006462,)####(1006474,)####(1006475,)####(1006476,)####(1006477,)####(1006478,)####(1006479,)####(1006480,)####(1006481,)####(1006482,)####(1006483,)####(1006484,)####(1006485,)####(1006486,)####(1006487,)####(1006488,)####(1006489,)####(1006490,)####(1006491,)####(1006492,)####(1006493,)####(1006494,)####(1006495,)####(1006496,)####(1006497,)####(1006498,)####(1006499,)####(1006500,)####(1006501,)####(1006502,)####(1006503,)####(1006504,)####(1006505,)####(1006506,)####(1006507,)####(1006508,)####(1006509,)####(1006510,)####(1006370,)####(1006371,)####(1006372,)####(1006373,)####(1006374,)####(1006375,)####(1006376,)####(1006377,)####(1006378,)####(1006379,)####(1006380,)####(1006381,)####(1006382,)####(1006393,)####(1006394,)####(1006395,)####(1006396,)####(1006397,)####(1006398,)####(1006399,)####(1006400,)####(1006401,)####(1006402,)####(1006403,)####(1006404,)####(1006405,)####(1006406,)####(1006407,)####(1006408,)####(1006409,)####(1006410,)####(1006411,)####(1006412,)####(1006413,)####(1006414,)####(1006415,)####(1006416,)####(1006417,)####(1006418,)####(1006419,)####(1006420,)####(1006421,)####(1006422,)####(1006423,)####(1006424,)####(1006425,)####(1006426,)####(1006427,)####(1006343,)####(1006428,)####(1006429,)####(1006344,)####(1006430,)####(1006345,)####(1006346,)####(1006347,)####(1042911,)####(1042954,)####(1042955,)####(1042956,)####(1042913,)####(1042957,)####(1042958,)####(1043126,)####(1043127,)####(1043128,)####(1042959,)####(1042960,)####(1042961,)####(1042962,)####(1042963,)####(1043129,)####(1043130,)####(1043131,)####(1043132,)####(1043133,)####(1043134,)####(1043135,)####(1043136,)####(1043137,)####(1043138,)####(1043139,)####(1043140,)####(1043141,)####(1043142,)####(1043143,)####(1043144,)####(1043145,)####(1043146,)####(1043147,)####(1043148,)####(1043149,)####(1043150,)####(1042964,)####(1042965,)####(1042966,)####(1042967,)####(1042968,)####(1042969,)####(1042970,)####(1043048,)####(1043049,)####(1043050,)####(1043051,)####(1043052,)####(1043053,)####(1043054,)####(1043055,)####(1043056,)####(1043057,)####(1043058,)####(1043059,)####(1043060,)####(1043061,)####(1043062,)####(1043063,)####(1043064,)####(1043065,)####(1043066,)####(1043067,)####(1043068,)####(1043069,)####(1043070,)####(1043071,)####(1043072,)####(1043073,)####(1043074,)####(1043153,)####(1043154,)####(1043157,)####(1043158,)####(1043159,)####(1043160,)####(1043162,)####(1043164,)####(1043165,)####(1043166,)####(1043167,)####(1043168,)####(1043169,)####(1043171,)####(1043172,)####(1043174,)####(1043175,)####(1043177,)####(1043180,)####(1043183,)####(1043184,)####(1043186,)####(1043189,)####(1043190,)####(1043192,)####(1043193,)####(1043196,)####(1043203,)####(1043206,)####(1043207,)####(1043208,)####(1043219,)####(1043220,)####(1043221,)####(1043222,)####(1043223,)####(1043224,)####(1043226,)####(1043228,)####(1043229,)####(1043231,)####(1043233,)####(1043252,)####(1043253,)####(1043254,)####(1043255,)####(1043256,)####(1043257,)####(1043258,)####(1043259,)####(1043260,)####(1043261,)####(1043262,)####(1043263,)####(1043264,)####(1043265,)####(1043266,)####(1043267,)####(1043268,)####(1043269,)####(1043270,)####(1043271,)####(1043272,)####(1043273,)####(1043274,)####(1043275,)####(1043276,)####(1043277,)####(1043278,)####(1043279,)####(1043280,)####(1043281,)####(1043282,)####(1043283,)####(1043284,)####(1043285,)####(1043286,)####(1043287,)####(1043288,)####(1043289,)####(1043290,)####(1043291,)####(1043292,)####(1043293,)####(1043294,)####(1043295,)####(1043296,)####(1043297,)####(1043298,)####(1043299,)####(1043300,)####(1043301,)####(1043302,)####(1043303,)####(1043304,)####(1043305,)####(1043306,)####(1043307,)####(1043308,)####(1043309,)####(1043310,)####(1043311,)####(1043312,)####(1043313,)####(1043314,)####(1043315,)####(1043316,)####(1043317,)####(1043318,)####(1043319,)####(1043320,)####(1043321,)####(1043322,)####(1043323,)####(1043324,)####(1043325,)####(1043326,)####(1043327,)####(1043328,)####(1043329,)####(1043330,)####(1043331,)####(1043332,)####(1043333,)####(1043334,)####(1043335,)####(1043336,)####(1043337,)####(1043338,)####(1043339,)####(1043340,)####(1043341,)####(1043342,)####(1043343,)####(1043344,)####(1043345,)####(1043075,)####(1043076,)####(1043077,)####(1043078,)####(1043079,)####(1043080,)####(1043081,)####(1043082,)####(1043083,)####(1043084,)####(1043085,)####(1043086,)####(1043087,)####(1043088,)####(1043089,)####(1043090,)####(1043108,)####(1043109,)####(1043110,)####(1043111,)####(1043112,)####(1043113,)####(1043114,)####(1043115,)####(1043116,)####(1043117,)####(1043118,)####(1043119,)####(1043120,)####(1043121,)####(1043122,)####(1043123,)####(1043124,)####(1043125,)####(1042919,)####(1042920,)####(1042921,)####(1042922,)####(1042923,)####(1042924,)####(1042925,)####(1042926,)####(1042927,)####(1042928,)####(1042929,)####(1042930,)####(1042931,)####(1042932,)####(1042933,)####(1042934,)####(1042935,)####(1042936,)####(1042937,)####(1042938,)####(1042939,)####(1042940,)####(1042941,)####(1042942,)####(1042943,)####(1042944,)####(1042945,)####(1042946,)####(1042947,)####(1042948,)####(1042949,)####(1042950,)####(1042951,)####(1042952,)####(1042953,)####(1049219,)####(1049285,)####(1049286,)####(1049287,)####(1049288,)####(1049289,)####(1049290,)####(1049291,)####(1049300,)####(1049301,)####(1049303,)####(1049304,)####(1049305,)####(1049308,)####(1049310,)####(1049220,)####(1049311,)####(1049320,)####(1049333,)####(1049334,)####(1049335,)####(1049224,)####(1049336,)####(1049337,)####(1049338,)####(1049344,)####(1049345,)####(1049347,)####(1049349,)####(1049225,)####(1049350,)####(1049351,)####(1049352,)####(1049353,)####(1049354,)####(1049371,)####(1049372,)####(1049373,)####(1049374,)####(1049226,)####(1049375,)####(1049376,)####(1049377,)####(1049378,)####(1049379,)####(1049227,)####(1049380,)####(1049381,)####(1049382,)####(1049383,)####(1049384,)####(1049385,)####(1049386,)####(1049228,)####(1049387,)####(1049388,)####(1049389,)####(1049390,)####(1049391,)####(1049229,)####(1049392,)####(1049393,)####(1049394,)####(1049395,)####(1049396,)####(1049230,)####(1049397,)####(1049398,)####(1049399,)####(1049400,)####(1049401,)####(1049231,)####(1049402,)####(1049403,)####(1049404,)####(1049405,)####(1049406,)####(1049232,)####(1049407,)####(1049408,)####(1049409,)####(1049410,)####(1049411,)####(1049412,)####(1049233,)####(1049413,)####(1049414,)####(1049415,)####(1049416,)####(1049417,)####(1049418,)####(1049234,)####(1049419,)####(1049420,)####(1049421,)####(1049235,)####(1049236,)####(1049237,)####(1049238,)####(1049239,)####(1049240,)####(1049241,)####(1049242,)####(1049243,)####(1049244,)####(1049245,)####(1049246,)####(1049247,)####(1049248,)####(1049249,)####(1049250,)####(1049251,)####(1049252,)####(1049253,)####(1049254,)####(1049255,)####(1049272,)####(1049273,)####(1049274,)####(1049275,)####(1049276,)####(1049277,)####(1049278,)####(1049279,)####(1049280,)####(1049281,)####(1049282,)####(1049283,)####(1049284,)####(1095523,)####(1095426,)####(1095438,)####(1095439,)####(1095440,)####(1095441,)####(1095442,)####(1095443,)####(1095444,)####(1095445,)####(1095446,)####(1095447,)####(1095448,)####(1095449,)####(1095450,)####(1095451,)####(1095452,)####(1095453,)####(1095454,)####(1095455,)####(1095456,)####(1095457,)####(1095458,)####(1095459,)####(1095460,)####(1095461,)####(1095462,)####(1095463,)####(1095464,)####(1095465,)####(1095466,)####(1095354,)####(1095362,)####(1095363,)####(1095364,)####(1095365,)####(1095366,)####(1095367,)####(1095368,)####(1095369,)####(1095370,)####(1095371,)####(1095372,)####(1095373,)####(1095374,)####(1095375,)####(1095376,)####(1095377,)####(1095378,)####(1095379,)####(1095380,)####(1095381,)####(1095382,)####(1095383,)####(1095384,)####(1095385,)####(1095386,)####(1095387,)####(1095388,)####(1095389,)####(1095390,)####(1095391,)####(1095392,)####(1095393,)####(1095394,)####(1095395,)####(1095396,)####(1095397,)####(1095398,)####(1095399,)####(1095400,)####(1095401,)####(1095402,)####(1095403,)####(1095467,)####(1095468,)####(1095469,)####(1095470,)####(1095471,)####(1095472,)####(1095473,)####(1095474,)####(1095475,)####(1095476,)####(1095477,)####(1095478,)####(1095479,)####(1095480,)####(1095481,)####(1095482,)####(1095483,)####(1095484,)####(1095485,)####(1095486,)####(1095487,)####(1095488,)####(1095489,)####(1095490,)####(1095491,)####(1095492,)####(1095493,)####(1095494,)####(1095495,)####(1095496,)####(1095497,)####(1095498,)####(1095510,)####(1095511,)####(1095512,)####(1095513,)####(1095514,)####(1095515,)####(1095516,)####(1095517,)####(1095518,)####(1095519,)####(1095520,)####(1095521,)####(1095522,)####(1095626,)####(1095632,)####(1095633,)####(1095634,)####(1095635,)####(1095636,)####(1095637,)####(1095638,)####(1095639,)####(1095646,)####(1095647,)####(1095648,)####(1095649,)####(1095650,)####(1095651,)####(1095653,)####(1095655,)####(1095656,)####(1095657,)####(1095658,)####(1095659,)####(1095660,)####(1095661,)####(1095662,)####(1095663,)####(1095664,)####(1095666,)####(1095667,)####(1095668,)####(1095669,)####(1095670,)####(1095671,)####(1095672,)####(1095673,)####(1095674,)####(1095675,)####(1095678,)####(1095679,)####(1095680,)####(1095681,)####(1095683,)####(1095684,)####(1095685,)####(1095686,)####(1095687,)####(1095688,)####(1095689,)####(1095690,)####(1095691,)####(1095692,)####(1095693,)####(1095694,)####(1095695,)####(1095696,)####(1095697,)####(1095698,)####(1095699,)####(1095700,)####(1095701,)####(1095714,)####(1095715,)####(1095716,)####(1095717,)####(1095718,)####(1095719,)####(1095720,)####(1095721,)####(1095722,)####(1095723,)####(1095724,)####(1095725,)####(1095726,)####(1095727,)####(1095728,)####(1095729,)####(1095730,)####(1095731,)####(1095732,)####(1095733,)####(1095734,)####(1095735,)####(1095736,)####(1095737,)####(1095738,)####(1095739,)####(1095740,)####(1095741,)####(1095742,)####(1095743,)####(1095744,)####(1095745,)####(1095746,)####(1095838,)####(1095839,)####(1095840,)####(1095841,)####(1095842,)####(1095404,)####(1095405,)####(1095406,)####(1095407,)####(1095408,)####(1095409,)####(1095410,)####(1095411,)####(1095412,)####(1095413,)####(1095414,)####(1095415,)####(1095416,)####(1095417,)####(1095418,)####(1095419,)####(1095420,)####(1095421,)####(1095422,)####(1095423,)####(1095424,)####(1095425,)####(1095747,)####(1095748,)####(1095749,)####(1095750,)####(1095751,)####(1095752,)####(1095753,)####(1095754,)####(1095755,)####(1095756,)####(1095757,)####(1095758,)####(1095759,)####(1095760,)####(1095761,)####(1095762,)####(1095763,)####(1095764,)####(1095765,)####(1095766,)####(1095767,)####(1095768,)####(1095769,)####(1095770,)####(1095771,)####(1095772,)####(1095773,)####(1095774,)####(1095775,)####(1095776,)####(1095777,)####(1095778,)####(1095779,)####(1095780,)####(1095781,)####(1095782,)####(1095783,)####(1095784,)####(1095785,)####(1095786,)####(1095787,)####(1095788,)####(1095789,)####(1095790,)####(1095791,)####(1095792,)####(1095793,)####(1095794,)####(1095795,)####(1095796,)####(1095797,)####(1095798,)####(1095799,)####(1095800,)####(1095801,)####(1095802,)####(1095803,)####(1095804,)####(1095805,)####(1095806,)####(1095807,)####(1095808,)####(1095809,)####(1095810,)####(1095811,)####(1095812,)####(1095813,)####(1095814,)####(1095815,)####(1095816,)####(1095817,)####(1095818,)####(1095819,)####(1095820,)####(1095821,)####(1095822,)####(1095823,)####(1095824,)####(1095825,)####(1095826,)####(1095827,)####(1095828,)####(1095829,)####(1095830,)####(1095831,)####(1095832,)####(1095833,)####(1095834,)####(1095835,)####(1095836,)####(1095837,)####(1095524,)####(1095525,)####(1095526,)####(1095527,)####(1095528,)####(1095529,)####(1095530,)####(1095531,)####(1095532,)####(1095533,)####(1095534,)####(1095535,)####(1095536,)####(1095537,)####(1095538,)####(1095539,)####(1095540,)####(1095541,)####(1095542,)####(1095543,)####(1095544,)####(1095545,)####(1095546,)####(1095547,)####(1095548,)####(1095549,)####(1095550,)####(1095551,)####(1095552,)####(1095553,)####(1095554,)####(1095555,)####(1095556,)####(1095557,)####(1095558,)####(1095559,)####(1095560,)####(1095561,)####(1095562,)####(1095563,)####(1095564,)####(1095565,)####(1095566,)####(1095567,)####(1095568,)####(1095569,)####(1095570,)####(1095586,)####(1095587,)####(1095588,)####(1095589,)####(1095590,)####(1095592,)####(1095593,)####(1095594,)####(1095595,)####(1095596,)####(1095597,)####(1095598,)####(1095599,)####(1095600,)####(1095601,)####(1095602,)####(1095603,)####(1095604,)####(1095605,)####(1095606,)####(1095607,)####(1095608,)####(1095609,)####(1095610,)####(1095611,)####(1095612,)####(1095613,)####(1095614,)####(1095615,)####(1095616,)####(1095617,)####(1095618,)####(1095619,)####(1095620,)####(1095621,)####(1110936,)####(1110943,)####(1111013,)####(1110944,)####(1111014,)####(1111015,)####(1110945,)####(1111016,)####(1111024,)####(1110946,)####(1111025,)####(1110947,)####(1111026,)####(1111027,)####(1110948,)####(1111028,)####(1111029,)####(1110949,)####(1111030,)####(1111031,)####(1111032,)####(1110950,)####(1111033,)####(1111034,)####(1111035,)####(1110951,)####(1111037,)####(1110952,)####(1111045,)####(1111046,)####(1111047,)####(1110953,)####(1111048,)####(1111049,)####(1110954,)####(1111050,)####(1110955,)####(1111051,)####(1111052,)####(1110956,)####(1111053,)####(1111054,)####(1111055,)####(1110957,)####(1111056,)####(1110958,)####(1111057,)####(1110959,)####(1111069,)####(1110960,)####(1111070,)####(1111071,)####(1110961,)####(1111072,)####(1110962,)####(1111073,)####(1111074,)####(1111075,)####(1110963,)####(1111076,)####(1111077,)####(1111078,)####(1110964,)####(1111079,)####(1111080,)####(1110965,)####(1111081,)####(1110966,)####(1111082,)####(1111083,)####(1111084,)####(1110967,)####(1111085,)####(1110968,)####(1111086,)####(1111087,)####(1111088,)####(1110969,)####(1111089,)####(1110970,)####(1111090,)####(1111091,)####(1110971,)####(1111092,)####(1111093,)####(1110972,)####(1111094,)####(1111095,)####(1110983,)####(1111096,)####(1110984,)####(1111097,)####(1110985,)####(1111098,)####(1111099,)####(1111100,)####(1110986,)####(1111101,)####(1111102,)####(1110987,)####(1111103,)####(1110988,)####(1111104,)####(1110989,)####(1111105,)####(1111106,)####(1110990,)####(1111107,)####(1111108,)####(1110991,)####(1111109,)####(1111110,)####(1110992,)####(1111111,)####(1111112,)####(1110993,)####(1111113,)####(1110994,)####(1111114,)####(1110995,)####(1111115,)####(1110996,)####(1111116,)####(1110997,)####(1111117,)####(1111118,)####(1111119,)####(1110998,)####(1111120,)####(1111121,)####(1110999,)####(1111000,)####(1111001,)####(1111002,)####(1111003,)####(1111004,)####(1111005,)####(1111006,)####(1111007,)####(1111008,)####(1111010,)####(1207328,)####(1207329,)####(1207330,)####(1207331,)####(1207332,)####(1207333,)####(1207334,)####(1207335,)####(1207336,)####(1207337,)####(1207290,)####(1207291,)####(1207292,)####(1207293,)####(1207295,)####(1207296,)####(1207297,)####(1207483,)####(1207484,)####(1207485,)####(1207486,)####(1207487,)####(1207488,)####(1207489,)####(1207490,)####(1207299,)####(1207300,)####(1207306,)####(1207307,)####(1207310,)####(1207311,)####(1207312,)####(1207313,)####(1207314,)####(1207315,)####(1207316,)####(1207317,)####(1207318,)####(1207319,)####(1207320,)####(1207321,)####(1207322,)####(1207323,)####(1207324,)####(1207325,)####(1207326,)####(1207327,)####(1207491,)####(1207492,)####(1207493,)####(1207494,)####(1207495,)####(1207496,)####(1207497,)####(1207498,)####(1207499,)####(1207500,)####(1207501,)####(1207502,)####(1207503,)####(1207504,)####(1207505,)####(1207506,)####(1207507,)####(1207508,)####(1207509,)####(1207568,)####(1207569,)####(1207570,)####(1207571,)####(1207572,)####(1207573,)####(1207584,)####(1207588,)####(1207589,)####(1207590,)####(1207591,)####(1207592,)####(1207594,)####(1207595,)####(1207596,)####(1207597,)####(1207598,)####(1207599,)####(1207602,)####(1207604,)####(1207608,)####(1207609,)####(1207610,)####(1207611,)####(1207612,)####(1207613,)####(1207614,)####(1207615,)####(1207616,)####(1207617,)####(1207618,)####(1207619,)####(1207620,)####(1207621,)####(1207622,)####(1207623,)####(1207630,)####(1207631,)####(1207632,)####(1207633,)####(1207634,)####(1207510,)####(1207511,)####(1207512,)####(1207513,)####(1207524,)####(1207525,)####(1207526,)####(1207527,)####(1207528,)####(1207529,)####(1207530,)####(1207531,)####(1207532,)####(1207533,)####(1207534,)####(1207535,)####(1207536,)####(1207537,)####(1207538,)####(1207539,)####(1207540,)####(1207541,)####(1207542,)####(1207543,)####(1207544,)####(1207545,)####(1207546,)####(1207547,)####(1207548,)####(1207549,)####(1207550,)####(1207551,)####(1207552,)####(1207553,)####(1207554,)####(1207555,)####(1207556,)####(1207557,)####(1207558,)####(1207559,)####(1207560,)####(1207561,)####(1207562,)####(1207563,)####(1207564,)####(1207635,)####(1207636,)####(1207637,)####(1207639,)####(1207640,)####(1207641,)####(1207646,)####(1207648,)####(1207649,)####(1207650,)####(1207651,)####(1207652,)####(1207653,)####(1207661,)####(1207662,)####(1207663,)####(1207664,)####(1207665,)####(1207666,)####(1207667,)####(1207668,)####(1207669,)####(1207670,)####(1207671,)####(1207672,)####(1207675,)####(1207676,)####(1207677,)####(1207678,)####(1207689,)####(1207690,)####(1207691,)####(1207692,)####(1207693,)####(1207694,)####(1207695,)####(1207696,)####(1207697,)####(1207698,)####(1207699,)####(1207700,)####(1207701,)####(1207702,)####(1207703,)####(1207704,)####(1207705,)####(1207706,)####(1207707,)####(1207708,)####(1207709,)####(1207710,)####(1207711,)####(1207806,)####(1207807,)####(1207808,)####(1207809,)####(1207810,)####(1207811,)####(1207812,)####(1207813,)####(1207565,)####(1207566,)####(1207567,)####(1207712,)####(1207713,)####(1207714,)####(1207715,)####(1207716,)####(1207717,)####(1207718,)####(1207719,)####(1207720,)####(1207721,)####(1207722,)####(1207723,)####(1207724,)####(1207725,)####(1207726,)####(1207727,)####(1207728,)####(1207729,)####(1207730,)####(1207731,)####(1207732,)####(1207733,)####(1207734,)####(1207735,)####(1207736,)####(1207338,)####(1207737,)####(1207738,)####(1207339,)####(1207739,)####(1207740,)####(1207340,)####(1207741,)####(1207742,)####(1207341,)####(1207743,)####(1207744,)####(1207342,)####(1207745,)####(1207746,)####(1207747,)####(1207343,)####(1207748,)####(1207749,)####(1207344,)####(1207750,)####(1207345,)####(1207751,)####(1207752,)####(1207346,)####(1207753,)####(1207754,)####(1207347,)####(1207755,)####(1207756,)####(1207348,)####(1207757,)####(1207349,)####(1207758,)####(1207759,)####(1207760,)####(1207350,)####(1207761,)####(1207351,)####(1207762,)####(1207763,)####(1207352,)####(1207764,)####(1207765,)####(1207353,)####(1207766,)####(1207354,)####(1207767,)####(1207355,)####(1207768,)####(1207769,)####(1207770,)####(1207356,)####(1207771,)####(1207357,)####(1207772,)####(1207773,)####(1207358,)####(1207774,)####(1207359,)####(1207775,)####(1207776,)####(1207360,)####(1207777,)####(1207361,)####(1207778,)####(1207779,)####(1207362,)####(1207780,)####(1207781,)####(1207363,)####(1207782,)####(1207364,)####(1207783,)####(1207784,)####(1207365,)####(1207785,)####(1207786,)####(1207787,)####(1207366,)####(1207788,)####(1207367,)####(1207789,)####(1207790,)####(1207368,)####(1207791,)####(1207369,)####(1207792,)####(1207793,)####(1207370,)####(1207794,)####(1207371,)####(1207795,)####(1207796,)####(1207797,)####(1207372,)####(1207798,)####(1207373,)####(1207799,)####(1207800,)####(1207374,)####(1207801,)####(1207802,)####(1207803,)####(1207375,)####(1207804,)####(1207376,)####(1207805,)####(1207377,)####(1207378,)####(1207379,)####(1207380,)####(1207381,)####(1207382,)####(1207383,)####(1207384,)####(1207385,)####(1207386,)####(1207387,)####(1207388,)####(1207389,)####(1207390,)####(1207391,)####(1207392,)####(1207393,)####(1207464,)####(1207465,)####(1207466,)####(1207467,)####(1207468,)####(1207469,)####(1207470,)####(1207471,)####(1207472,)####(1207473,)####(1207474,)####(1207475,)####(1207476,)####(1207477,)####(1207478,)####(1207479,)####(1207480,)####(1207481,)####(1207482,)####(1274497,)####(1246246,)####(1246248,)####(1246249,)####(1246250,)####(1246251,)####(1246252,)####(1246253,)####(1246254,)####(1246257,)####(1246261,)####(1246262,)####(1246269,)####(1246270,)####(1246289,)####(1246290,)####(1246291,)####(1246292,)####(1246293,)####(1246294,)####(1246307,)####(1246308,)####(1246309,)####(1246310,)####(1246311,)####(1246313,)####(1246314,)####(1246315,)####(1246316,)####(1246347,)####(1246348,)####(1246349,)####(1246350,)####(1246351,)####(1246352,)####(1246353,)####(1246354,)####(1246355,)####(1246356,)####(1246357,)####(1246358,)####(1246359,)####(1246360,)####(1246361,)####(1246362,)####(1246363,)####(1246364,)####(1246365,)####(1246366,)####(1246367,)####(1246368,)####(1246369,)####(1246370,)####(1246371,)####(1246372,)####(1246373,)####(1246374,)####(1246375,)####(1246376,)####(1246377,)####(1246378,)####(1246379,)####(1246380,)####(1246381,)####(1246382,)####(1246383,)####(1246384,)####(1246385,)####(1246386,)####(1246387,)####(1246388,)####(1246389,)####(1246390,)####(1246391,)####(1246392,)####(1246393,)####(1246394,)####(1246395,)####(1246396,)####(1246397,)####(1246398,)####(1246399,)####(1246400,)####(1246401,)####(1246402,)####(1246403,)####(1246404,)####(1246405,)####(1274523,)####(1274524,)####(1274525,)####(1274526,)####(1274527,)####(1274528,)####(1274529,)####(1274530,)####(1274555,)####(1274557,)####(1274559,)####(1274560,)####(1274561,)####(1274562,)####(1274563,)####(1274564,)####(1274565,)####(1274567,)####(1274568,)####(1274569,)####(1274576,)####(1274577,)####(1274578,)####(1274579,)####(1274580,)####(1274581,)####(1274583,)####(1274585,)####(1274586,)####(1274587,)####(1274588,)####(1274592,)####(1274593,)####(1274594,)####(1274595,)####(1274596,)####(1274597,)####(1274598,)####(1274599,)####(1274600,)####(1274601,)####(1274602,)####(1274603,)####(1274604,)####(1274605,)####(1274606,)####(1274607,)####(1274608,)####(1274609,)####(1274610,)####(1274611,)####(1274612,)####(1274613,)####(1274614,)####(1274615,)####(1274616,)####(1274617,)####(1274618,)####(1274619,)####(1274620,)####(1274621,)####(1274622,)####(1274623,)####(1274624,)####(1274625,)####(1274626,)####(1274627,)####(1274628,)####(1274629,)####(1274630,)####(1274631,)####(1274632,)####(1274633,)####(1274634,)####(1274635,)####(1274636,)####(1274637,)####(1274638,)####(1274639,)####(1274640,)####(1274641,)####(1274642,)####(1274643,)####(1274644,)####(1274645,)####(1274498,)####(1274499,)####(1274500,)####(1274501,)####(1274502,)####(1274503,)####(1274504,)####(1274505,)####(1274506,)####(1274507,)####(1274508,)####(1274509,)####(1274510,)####(1274511,)####(1274512,)####(1274513,)####(1274514,)####(1274515,)####(1274516,)####(1274517,)####(1274519,)####(1274520,)####(1274521,)####(1274522,)####(1427261,)####(1427118,)####(1427122,)####(1427123,)####(1427124,)####(1427125,)####(1427126,)####(1427127,)####(1427128,)####(1427129,)####(1427130,)####(1427131,)####(1427132,)####(1427133,)####(1427134,)####(1427135,)####(1427136,)####(1427137,)####(1427138,)####(1427139,)####(1427140,)####(1427141,)####(1427142,)####(1427143,)####(1427144,)####(1427145,)####(1427146,)####(1427147,)####(1427148,)####(1427149,)####(1427150,)####(1427151,)####(1427152,)####(1427153,)####(1427154,)####(1427162,)####(1427163,)####(1427164,)####(1427165,)####(1427166,)####(1427167,)####(1427168,)####(1427169,)####(1427170,)####(1427171,)####(1427172,)####(1427173,)####(1427174,)####(1427175,)####(1427176,)####(1427177,)####(1427178,)####(1427179,)####(1427180,)####(1427181,)####(1427182,)####(1427183,)####(1427184,)####(1427185,)####(1427186,)####(1427187,)####(1427188,)####(1427189,)####(1427190,)####(1427198,)####(1427199,)####(1427200,)####(1427201,)####(1427202,)####(1427203,)####(1427204,)####(1427205,)####(1427206,)####(1427207,)####(1427208,)####(1427209,)####(1427210,)####(1427211,)####(1427212,)####(1427213,)####(1427214,)####(1427215,)####(1427216,)####(1427217,)####(1427218,)####(1427219,)####(1427220,)####(1427221,)####(1427222,)####(1427223,)####(1427224,)####(1427225,)####(1427226,)####(1427234,)####(1427235,)####(1427236,)####(1427237,)####(1427238,)####(1427239,)####(1427240,)####(1427241,)####(1427242,)####(1427243,)####(1427244,)####(1427245,)####(1427246,)####(1427247,)####(1427248,)####(1427249,)####(1427250,)####(1427251,)####(1427252,)####(1427253,)####(1427383,)####(1427384,)####(1427385,)####(1427386,)####(1427387,)####(1427388,)####(1427389,)####(1427390,)####(1427391,)####(1427392,)####(1427393,)####(1427394,)####(1427395,)####(1427396,)####(1427397,)####(1427398,)####(1427254,)####(1427255,)####(1427256,)####(1427257,)####(1427258,)####(1427259,)####(1427260,)####(1427382,)####(1427262,)####(1427263,)####(1427265,)####(1427267,)####(1427269,)####(1427272,)####(1427273,)####(1427274,)####(1427275,)####(1427278,)####(1427279,)####(1427280,)####(1427281,)####(1427282,)####(1427283,)####(1427284,)####(1427285,)####(1427286,)####(1427287,)####(1427293,)####(1427294,)####(1427295,)####(1427296,)####(1427303,)####(1427304,)####(1427305,)####(1427306,)####(1427307,)####(1427308,)####(1427309,)####(1427310,)####(1427311,)####(1427312,)####(1427313,)####(1427314,)####(1427315,)####(1427317,)####(1427319,)####(1427320,)####(1427321,)####(1427322,)####(1427324,)####(1427325,)####(1427326,)####(1427327,)####(1427328,)####(1427337,)####(1427338,)####(1427339,)####(1427340,)####(1427341,)####(1427342,)####(1427343,)####(1427344,)####(1427345,)####(1427346,)####(1427347,)####(1427348,)####(1427349,)####(1427350,)####(1427351,)####(1427352,)####(1427353,)####(1427354,)####(1427355,)####(1427356,)####(1427357,)####(1427358,)####(1427359,)####(1427360,)####(1427361,)####(1427362,)####(1427363,)####(1427364,)####(1427365,)####(1427366,)####(1427367,)####(1427368,)####(1427369,)####(1427370,)####(1427371,)####(1427372,)####(1427373,)####(1427374,)####(1427375,)####(1427376,)####(1427377,)####(1427378,)####(1427379,)####(1427380,)####(1427381,)####(1515026,)####(1514990,)####(1515027,)####(1514995,)####(1514996,)####(1515028,)####(1514997,)####(1514998,)####(1514999,)####(1515029,)####(1515000,)####(1515001,)####(1515002,)####(1515030,)####(1515003,)####(1515031,)####(1515004,)####(1515005,)####(1515032,)####(1515006,)####(1515007,)####(1515033,)####(1515008,)####(1515009,)####(1515010,)####(1515034,)####(1515011,)####(1515012,)####(1515035,)####(1515013,)####(1515014,)####(1515036,)####(1515023,)####(1515037,)####(1515024,)####(1515038,)####(1515025,)####(1515048,)####(1515049,)####(1515050,)####(1515051,)####(1515052,)####(1515053,)####(1515054,)####(1515055,)####(1515056,)####(1515057,)####(1515058,)####(1515059,)####(1515060,)####(1515061,)####(1515062,)####(1515065,)####(1515068,)####(1515075,)####(1515076,)####(1515077,)####(1515087,)####(1515088,)####(1515089,)####(1515090,)####(1515091,)####(1515092,)####(1515093,)####(1515094,)####(1515095,)####(1515096,)####(1515097,)####(1515098,)####(1515099,)####(1515100,)####(1515101,)####(1515102,)####(1515103,)####(1515104,)####(1515105,)####(1515106,)####(1515107,)####(1515108,)####(1515109,)####(1515110,)####(1515111,)####(1515112,)####(1515113,)####(1660452,)####(1660453,)####(1660454,)####(1660455,)####(1660456,)####(1660457,)####(1660458,)####(1660375,)####(1660459,)####(1660384,)####(1660386,)####(1660460,)####(1660387,)####(1660474,)####(1660388,)####(1660389,)####(1660575,)####(1660584,)####(1660585,)####(1660586,)####(1660587,)####(1660588,)####(1660590,)####(1660591,)####(1660592,)####(1660593,)####(1660594,)####(1660596,)####(1660390,)####(1660391,)####(1660392,)####(1660393,)####(1660394,)####(1660395,)####(1660396,)####(1660397,)####(1660398,)####(1660399,)####(1660400,)####(1660401,)####(1660402,)####(1660597,)####(1660611,)####(1660612,)####(1660613,)####(1660614,)####(1660615,)####(1660616,)####(1660617,)####(1660618,)####(1660619,)####(1660620,)####(1660621,)####(1660622,)####(1660623,)####(1660624,)####(1660625,)####(1660626,)####(1660627,)####(1660628,)####(1660629,)####(1660630,)####(1660631,)####(1660632,)####(1660633,)####(1660634,)####(1660635,)####(1660636,)####(1660637,)####(1660638,)####(1660639,)####(1660640,)####(1660641,)####(1660642,)####(1660643,)####(1660644,)####(1660645,)####(1660646,)####(1660647,)####(1660648,)####(1660649,)####(1660650,)####(1660651,)####(1660652,)####(1660653,)####(1660654,)####(1660655,)####(1660656,)####(1660403,)####(1660404,)####(1660405,)####(1660406,)####(1660407,)####(1660408,)####(1660409,)####(1660410,)####(1660411,)####(1660412,)####(1660413,)####(1660414,)####(1660415,)####(1660416,)####(1660417,)####(1660418,)####(1660419,)####(1660420,)####(1660421,)####(1660422,)####(1660423,)####(1660424,)####(1660425,)####(1660426,)####(1660427,)####(1660428,)####(1660429,)####(1660430,)####(1660431,)####(1660432,)####(1660433,)####(1660434,)####(1660435,)####(1660436,)####(1660437,)####(1660438,)####(1660439,)####(1660440,)####(1660441,)####(1660442,)####(1660443,)####(1660444,)####(1660445,)####(1660446,)####(1660447,)####(1660448,)####(1660449,)####(1660450,)####(1660451,)####(1660657,)####(1660658,)####(1660659,)####(1660660,)####(1660661,)####(1660662,)####(1660663,)####(1660664,)####(1660665,)####(1660666,)####(1660667,)####(1660668,)####(1660669,)####(1660670,)####(1660671,)####(1660672,)####(1660673,)####(1660674,)####(1660675,)####(1660676,)####(1660677,)####(1660678,)####(1660679,)####(1660680,)####(1660681,)####(1660682,)####(1660683,)####(1660684,)####(1660685,)####(1660686,)####(1660687,)####(1660688,)####(1660689,)####(1660690,)####(1660691,)####(1660692,)####(1660693,)####(1660694,)####(1660695,)####(1660696,)####(1660697,)####(1660698,)####(1660699,)####(1660700,)####(1660701,)####(1660702,)####(1660703,)####(1660704,)####(1660705,)####(1660706,)####(1660707,)####(1660708,)####(1660709,)####(1660710,)####(1660711,)####(1660712,)####(1660713,)####(1660714,)####(1660715,)####(1660716,)####(1660717,)####(1660718,)####(1660719,)####(1660720,)####(1660721,)####(1660722,)####(1660475,)####(1660476,)####(1660477,)####(1660478,)####(1660479,)####(1660480,)####(1660481,)####(1660482,)####(1660483,)####(1660484,)####(1660485,)####(1660486,)####(1660487,)####(1660488,)####(1660489,)####(1660490,)####(1660491,)####(1660492,)####(1660493,)####(1660494,)####(1660495,)####(1660496,)####(1660497,)####(1660498,)####(1660499,)####(1660500,)####(1660501,)####(1660502,)####(1660503,)####(1660504,)####(1660505,)####(1660506,)####(1660507,)####(1660508,)####(1660509,)####(1660510,)####(1660511,)####(1660512,)####(1660513,)####(1660514,)####(1660515,)####(1660516,)####(1660517,)####(1660518,)####(1660519,)####(1660520,)####(1660534,)####(1660536,)####(1660538,)####(1660539,)####(1660540,)####(1660541,)####(1660542,)####(1660544,)####(1660545,)####(1660548,)####(1660549,)####(1660550,)####(1660551,)####(1660552,)####(1660553,)####(1660554,)####(1660556,)####(1660558,)####(1660560,)####(1660561,)####(1660562,)####(1660563,)####(1660564,)####(1660565,)####(1660567,)####(1660568,)####(1660573,)####(1660574,)####(1721839,)####(1721961,)####(1721962,)####(1721963,)####(1721964,)####(1721965,)####(1721840,)####(1721966,)####(1721841,)####(1721772,)####(1721842,)####(1721773,)####(1721774,)####(1721843,)####(1721775,)####(1721777,)####(1721778,)####(1721844,)####(1721779,)####(1721845,)####(1721780,)####(1721783,)####(1721846,)####(1721788,)####(1721789,)####(1721847,)####(1721790,)####(1721850,)####(1721792,)####(1721793,)####(1721856,)####(1721796,)####(1721797,)####(1721798,)####(1721860,)####(1721799,)####(1721800,)####(1721861,)####(1721801,)####(1721862,)####(1721802,)####(1721803,)####(1721804,)####(1721863,)####(1721805,)####(1721864,)####(1721806,)####(1721807,)####(1721808,)####(1721865,)####(1721809,)####(1721810,)####(1721866,)####(1721811,)####(1721822,)####(1721867,)####(1721823,)####(1721872,)####(1721824,)####(1721825,)####(1721826,)####(1721874,)####(1721827,)####(1721828,)####(1721876,)####(1721829,)####(1721878,)####(1721830,)####(1721879,)####(1721880,)####(1721831,)####(1721883,)####(1721832,)####(1721885,)####(1721833,)####(1721886,)####(1721834,)####(1721887,)####(1721835,)####(1721888,)####(1721836,)####(1721837,)####(1721838,)####(1721890,)####(1721891,)####(1721892,)####(1721893,)####(1721894,)####(1721895,)####(1721906,)####(1721907,)####(1721908,)####(1721909,)####(1721910,)####(1721911,)####(1721912,)####(1721913,)####(1721914,)####(1721915,)####(1721916,)####(1721917,)####(1721918,)####(1721919,)####(1721920,)####(1721921,)####(1721922,)####(1721923,)####(1721924,)####(1721925,)####(1721926,)####(1721927,)####(1721928,)####(1721929,)####(1721930,)####(1721931,)####(1721932,)####(1721933,)####(1721934,)####(1721935,)####(1721936,)####(1721937,)####(1721938,)####(1721939,)####(1721940,)####(1721941,)####(1721942,)####(1721943,)####(1721944,)####(1721945,)####(1721946,)####(1721947,)####(1721948,)####(1721949,)####(1721950,)####(1721951,)####(1721952,)####(1721953,)####(1721954,)####(1721955,)####(1721956,)####(1721957,)####(1721958,)####(1721959,)####(1721960,)####(1850343,)####(1850350,)####(1850353,)####(1850357,)####(1850358,)####(1850359,)####(1850364,)####(1850365,)####(1850366,)####(1850367,)####(1850368,)####(1850369,)####(1850370,)####(1850371,)####(1850372,)####(1850373,)####(1850374,)####(1850375,)####(1850376,)####(1850409,)####(1850410,)####(1850411,)####(1850554,)####(1850555,)####(1850556,)####(1850557,)####(1850558,)####(1850559,)####(1850560,)####(1850561,)####(1850562,)####(1850563,)####(1850564,)####(1850565,)####(1850566,)####(1850567,)####(1850568,)####(1850569,)####(1850570,)####(1850571,)####(1850572,)####(1850412,)####(1850413,)####(1850414,)####(1850415,)####(1850416,)####(1850417,)####(1850418,)####(1850419,)####(1850420,)####(1850421,)####(1850422,)####(1850423,)####(1850424,)####(1850425,)####(1850426,)####(1850427,)####(1850428,)####(1850429,)####(1850430,)####(1850431,)####(1850444,)####(1850573,)####(1850574,)####(1850575,)####(1850576,)####(1850577,)####(1850578,)####(1850579,)####(1850580,)####(1850581,)####(1850582,)####(1850583,)####(1850584,)####(1850585,)####(1850586,)####(1850587,)####(1850588,)####(1850589,)####(1850590,)####(1850591,)####(1850592,)####(1850593,)####(1850594,)####(1850595,)####(1850596,)####(1850597,)####(1850598,)####(1850599,)####(1850600,)####(1850601,)####(1850602,)####(1850603,)####(1850604,)####(1850445,)####(1850446,)####(1850447,)####(1850448,)####(1850449,)####(1850450,)####(1850451,)####(1850452,)####(1850453,)####(1850454,)####(1850455,)####(1850456,)####(1850457,)####(1850458,)####(1850459,)####(1850460,)####(1850461,)####(1850462,)####(1850463,)####(1850464,)####(1850465,)####(1850466,)####(1850467,)####(1850472,)####(1850377,)####(1850473,)####(1850378,)####(1850475,)####(1850476,)####(1850477,)####(1850379,)####(1850478,)####(1850479,)####(1850480,)####(1850380,)####(1850482,)####(1850483,)####(1850381,)####(1850484,)####(1850382,)####(1850487,)####(1850488,)####(1850491,)####(1850383,)####(1850492,)####(1850494,)####(1850384,)####(1850497,)####(1850385,)####(1850501,)####(1850508,)####(1850386,)####(1850509,)####(1850387,)####(1850510,)####(1850511,)####(1850512,)####(1850388,)####(1850513,)####(1850389,)####(1850390,)####(1850516,)####(1850528,)####(1850391,)####(1850529,)####(1850392,)####(1850530,)####(1850393,)####(1850531,)####(1850532,)####(1850394,)####(1850533,)####(1850395,)####(1850549,)####(1850550,)####(1850551,)####(1850408,)####(1850552,)####(1850553,)####(2121509,)####(2121516,)####(2121683,)####(2121684,)####(2121685,)####(2121686,)####(2121687,)####(2121589,)####(2121590,)####(2121591,)####(2121688,)####(2121689,)####(2121690,)####(2121691,)####(2121692,)####(2121693,)####(2121694,)####(2121695,)####(2121696,)####(2121697,)####(2121698,)####(2121699,)####(2121700,)####(2121701,)####(2121712,)####(2121713,)####(2121714,)####(2121715,)####(2121716,)####(2121717,)####(2121592,)####(2121593,)####(2121594,)####(2121595,)####(2121596,)####(2121597,)####(2121598,)####(2121599,)####(2121600,)####(2121601,)####(2121602,)####(2121603,)####(2121604,)####(2121605,)####(2121616,)####(2121617,)####(2121618,)####(2121718,)####(2121719,)####(2121720,)####(2121721,)####(2121722,)####(2121723,)####(2121724,)####(2121725,)####(2121726,)####(2121727,)####(2121728,)####(2121729,)####(2121730,)####(2121731,)####(2121732,)####(2121733,)####(2121734,)####(2121735,)####(2121736,)####(2121737,)####(2121738,)####(2121739,)####(2121740,)####(2121741,)####(2121742,)####(2121743,)####(2121744,)####(2121745,)####(2121746,)####(2121747,)####(2121748,)####(2121749,)####(2121760,)####(2121761,)####(2121762,)####(2121763,)####(2121764,)####(2121765,)####(2121766,)####(2121767,)####(2121768,)####(2121769,)####(2121770,)####(2121771,)####(2121772,)####(2121773,)####(2121774,)####(2121775,)####(2121776,)####(2121777,)####(2121619,)####(2121620,)####(2121621,)####(2121622,)####(2121623,)####(2121624,)####(2121625,)####(2121626,)####(2121627,)####(2121628,)####(2121629,)####(2121630,)####(2121631,)####(2121632,)####(2121633,)####(2121634,)####(2121635,)####(2121636,)####(2121637,)####(2121638,)####(2121639,)####(2121640,)####(2121641,)####(2121642,)####(2121643,)####(2121644,)####(2121645,)####(2121646,)####(2121647,)####(2121648,)####(2121649,)####(2121650,)####(2121651,)####(2121652,)####(2121653,)####(2121664,)####(2121665,)####(2121666,)####(2121667,)####(2121668,)####(2121669,)####(2121670,)####(2121671,)####(2121672,)####(2121673,)####(2121674,)####(2121675,)####(2121676,)####(2121677,)####(2121678,)####(2121679,)####(2121680,)####(2121681,)####(2121682,)####(2121778,)####(2121779,)####(2121780,)####(2121781,)####(2121782,)####(2121783,)####(2121784,)####(2121785,)####(2121786,)####(2121787,)####(2121788,)####(2121789,)####(2121790,)####(2121791,)####(2121792,)####(2121793,)####(2121794,)####(2121795,)####(2121796,)####(2121797,)####(2121801,)####(2121803,)####(2121806,)####(2121807,)####(2121809,)####(2121816,)####(2121817,)####(2121818,)####(2121819,)####(2121820,)####(2121821,)####(2121822,)####(2121823,)####(2121824,)####(2121825,)####(2121828,)####(2121829,)####(2121833,)####(2121839,)####(2121840,)####(2121841,)####(2121842,)####(2121843,)####(2121844,)####(2121852,)####(2121853,)####(2121865,)####(2121866,)####(2121867,)####(2121868,)####(2121869,)####(2121870,)####(2121871,)####(2121872,)####(2121873,)####(2121874,)####(2121875,)####(2121876,)####(2121877,)####(2121878,)####(2121879,)####(2121880,)####(2121881,)####(2121882,)####(2121883,)####(2121884,)####(2121885,)####(2121886,)####(2121887,)####(2121888,)####(2121889,)####(2121890,)####(2121891,)####(2121892,)####(2121893,)####(2121894,)####(2121895,)####(2121896,)####(2121897,)####(2121898,)####(2121899,)####(2121900,)####(2121901,)####(2121902,)####(2121903,)####(2121904,)####(2121905,)####(2121906,)####(2121907,)####(2121908,)####(2121909,)####(2121910,)####(2121911,)####(2121912,)####(2121913,)####(2121914,)####(2121915,)####(2121916,)####(2121917,)####(2121918,)####(2121919,)####(2121920,)####(2121921,)####(2121922,)####(2121923,)####(2121924,)####(2121925,)####(2121926,)####(2121927,)####(2121928,)####(2121929,)####(2121930,)####(2121931,)####(2121932,)####(2121933,)####(2121517,)####(2121518,)####(2121519,)####(2121520,)####(2121521,)####(2121522,)####(2121523,)####(2121524,)####(2121525,)####(2121526,)####(2121527,)####(2121528,)####(2121529,)####(2121530,)####(2121531,)####(2121532,)####(2121533,)####(2121534,)####(2121535,)####(2121536,)####(2121537,)####(2121538,)####(2121539,)####(2121540,)####(2121541,)####(2121542,)####(2121543,)####(2121544,)####(2121545,)####(2121546,)####(2121547,)####(2121548,)####(2121549,)####(2121550,)####(2121551,)####(2121552,)####(2121553,)####(2121554,)####(2121555,)####(2121556,)####(2121557,)####(2121568,)####(2121569,)####(2121570,)####(2121571,)####(2121572,)####(2121573,)####(2121574,)####(2121575,)####(2121576,)####(2121577,)####(2121578,)####(2121579,)####(2121580,)####(2121581,)####(2121582,)####(2121583,)####(2121584,)####(2121585,)####(2121586,)####(2121587,)####(2121588,)####(2465866,)####(2465765,)####(2465716,)####(2465721,)####(2465722,)####(2465723,)####(2465724,)####(2465725,)####(2465726,)####(2465727,)####(2465728,)####(2465729,)####(2465730,)####(2465731,)####(2465732,)####(2465733,)####(2465734,)####(2465735,)####(2465736,)####(2465737,)####(2465738,)####(2465739,)####(2465740,)####(2465741,)####(2465742,)####(2465743,)####(2465744,)####(2465745,)####(2465746,)####(2465747,)####(2465748,)####(2465749,)####(2465750,)####(2465751,)####(2465752,)####(2465753,)####(2465754,)####(2465755,)####(2465756,)####(2465757,)####(2465758,)####(2465759,)####(2465760,)####(2465761,)####(2465762,)####(2465763,)####(2465764,)####(2465967,)####(2465968,)####(2465969,)####(2465766,)####(2465767,)####(2465768,)####(2465769,)####(2465770,)####(2465771,)####(2465772,)####(2465773,)####(2465774,)####(2465775,)####(2465776,)####(2465777,)####(2465778,)####(2465779,)####(2465780,)####(2465781,)####(2465782,)####(2465783,)####(2465784,)####(2465785,)####(2465786,)####(2465787,)####(2465788,)####(2465789,)####(2465791,)####(2465794,)####(2465795,)####(2465796,)####(2465797,)####(2465798,)####(2465799,)####(2465800,)####(2465801,)####(2465802,)####(2465803,)####(2465804,)####(2465805,)####(2465806,)####(2465807,)####(2465808,)####(2465809,)####(2465810,)####(2465811,)####(2465812,)####(2465813,)####(2465814,)####(2465815,)####(2465816,)####(2465819,)####(2465821,)####(2465823,)####(2465824,)####(2465825,)####(2465970,)####(2465971,)####(2465972,)####(2465973,)####(2465974,)####(2465975,)####(2465976,)####(2465977,)####(2465978,)####(2465979,)####(2465980,)####(2465981,)####(2465982,)####(2465983,)####(2465984,)####(2465985,)####(2465986,)####(2465987,)####(2465988,)####(2465989,)####(2465990,)####(2465991,)####(2465992,)####(2465993,)####(2465994,)####(2465995,)####(2465996,)####(2465997,)####(2465998,)####(2465999,)####(2466000,)####(2466001,)####(2466002,)####(2466003,)####(2466004,)####(2466005,)####(2466006,)####(2466007,)####(2466008,)####(2466009,)####(2466010,)####(2466011,)####(2466012,)####(2466013,)####(2466014,)####(2466015,)####(2466016,)####(2466017,)####(2466018,)####(2466019,)####(2466020,)####(2466021,)####(2466022,)####(2466023,)####(2466024,)####(2466025,)####(2466026,)####(2466027,)####(2466028,)####(2466029,)####(2466030,)####(2466031,)####(2466032,)####(2466033,)####(2466034,)####(2466035,)####(2466036,)####(2465826,)####(2465827,)####(2465828,)####(2465835,)####(2465836,)####(2465837,)####(2465838,)####(2465839,)####(2465840,)####(2465841,)####(2465842,)####(2465843,)####(2465844,)####(2465845,)####(2465846,)####(2465847,)####(2465848,)####(2465849,)####(2465850,)####(2465851,)####(2465852,)####(2465853,)####(2465854,)####(2465855,)####(2465856,)####(2465857,)####(2465858,)####(2465859,)####(2465860,)####(2465861,)####(2465862,)####(2465863,)####(2465864,)####(2465865,)####(2465867,)####(2465868,)####(2465869,)####(2465870,)####(2465871,)####(2465872,)####(2465873,)####(2465874,)####(2465876,)####(2465877,)####(2465878,)####(2465879,)####(2465880,)####(2465881,)####(2465882,)####(2465883,)####(2465884,)####(2465885,)####(2465886,)####(2465887,)####(2465888,)####(2465889,)####(2465890,)####(2465891,)####(2465892,)####(2465893,)####(2465903,)####(2465904,)####(2465905,)####(2465906,)####(2465907,)####(2465908,)####(2465909,)####(2465910,)####(2465911,)####(2465912,)####(2465913,)####(2465914,)####(2465915,)####(2465916,)####(2465917,)####(2465918,)####(2465919,)####(2465920,)####(2465921,)####(2465922,)####(2465923,)####(2465924,)####(2465925,)####(2465926,)####(2465927,)####(2465928,)####(2465929,)####(2465930,)####(2465931,)####(2465932,)####(2465933,)####(2465934,)####(2465935,)####(2465936,)####(2465937,)####(2465938,)####(2465939,)####(2465940,)####(2465941,)####(2465942,)####(2465943,)####(2465944,)####(2465945,)####(2465946,)####(2465947,)####(2465948,)####(2465949,)####(2465950,)####(2465951,)####(2465952,)####(2465953,)####(2465954,)####(2465955,)####(2465956,)####(2465957,)####(2465958,)####(2465959,)####(2465960,)####(2465961,)####(2465962,)####(2465963,)####(2465964,)####(2465965,)####(2465966,)####(2866358,)####(2735846,)####(2735847,)####(2735848,)####(2735849,)####(2735850,)####(2735851,)####(2735852,)####(2735853,)####(2735854,)####(2735855,)####(2735856,)####(2735858,)####(2735863,)####(2735865,)####(2735868,)####(2735870,)####(2735871,)####(2735873,)####(2735875,)####(2735877,)####(2735878,)####(2735879,)####(2735887,)####(2735888,)####(2735902,)####(2735903,)####(2735905,)####(2735914,)####(2735915,)####(2735917,)####(2735918,)####(2735919,)####(2735920,)####(2735939,)####(2735940,)####(2735941,)####(2735942,)####(2735943,)####(2735944,)####(2735945,)####(2735946,)####(2735947,)####(2735948,)####(2735949,)####(2735950,)####(2735951,)####(2735952,)####(2735953,)####(2735954,)####(2735955,)####(2735956,)####(2735957,)####(2735958,)####(2735959,)####(2735960,)####(2735961,)####(2735962,)####(2735963,)####(2735964,)####(2735965,)####(2735966,)####(2735967,)####(2735968,)####(2735969,)####(2735970,)####(2735971,)####(2735972,)####(2735973,)####(2735974,)####(2735975,)####(2735976,)####(2735977,)####(2735978,)####(2735979,)####(2735980,)####(2735981,)####(2735982,)####(2735983,)####(2735984,)####(2735985,)####(2735986,)####(2735987,)####(2735988,)####(2735989,)####(2735990,)####(2735991,)####(2735992,)####(2735993,)####(2735994,)####(2866445,)####(2866446,)####(2866447,)####(2866448,)####(2866449,)####(2866450,)####(2866451,)####(2866452,)####(2866453,)####(2866454,)####(2866455,)####(2866363,)####(2866364,)####(2866365,)####(2866366,)####(2866367,)####(2866368,)####(2866369,)####(2866370,)####(2866371,)####(2866372,)####(2866373,)####(2866374,)####(2866375,)####(2866376,)####(2866377,)####(2866378,)####(2866379,)####(2866380,)####(2866381,)####(2866382,)####(2866384,)####(2866386,)####(2866387,)####(2866388,)####(2866395,)####(2866396,)####(2866397,)####(2866398,)####(2866399,)####(2866400,)####(2866402,)####(2866403,)####(2866404,)####(2866407,)####(2866408,)####(2866409,)####(2866410,)####(2866411,)####(2866412,)####(2866422,)####(2866423,)####(2866424,)####(2866425,)####(2866426,)####(2866427,)####(2866428,)####(2866429,)####(2866430,)####(2866431,)####(2866432,)####(2866433,)####(2866434,)####(2866435,)####(2866436,)####(2866437,)####(2866438,)####(2866439,)####(2866440,)####(2866441,)####(2866442,)####(2866443,)####(2866444,)####(3395266,)####(3395166,)####(3395167,)####(3395168,)####(3395169,)####(3395170,)####(3395171,)####(3395172,)####(3395173,)####(3395174,)####(3395175,)####(3395176,)####(3395177,)####(3395178,)####(3395179,)####(3395180,)####(3395181,)####(3395182,)####(3395183,)####(3395184,)####(3395154,)####(3395164,)####(3395165,)####(3395539,)####(3395185,)####(3395186,)####(3395187,)####(3395188,)####(3395189,)####(3395190,)####(3395191,)####(3395192,)####(3395193,)####(3395194,)####(3395195,)####(3395196,)####(3395197,)####(3395198,)####(3395367,)####(3395368,)####(3395369,)####(3395370,)####(3395371,)####(3395372,)####(3395373,)####(3395374,)####(3395375,)####(3395199,)####(3395200,)####(3395201,)####(3395202,)####(3395216,)####(3395217,)####(3395218,)####(3395219,)####(3395220,)####(3395221,)####(3395222,)####(3395223,)####(3395224,)####(3395225,)####(3395226,)####(3395227,)####(3395228,)####(3395229,)####(3395230,)####(3395231,)####(3395232,)####(3395233,)####(3395234,)####(3395235,)####(3395236,)####(3395237,)####(3395238,)####(3395239,)####(3395240,)####(3395241,)####(3395242,)####(3395243,)####(3395244,)####(3395245,)####(3395246,)####(3395247,)####(3395248,)####(3395249,)####(3395250,)####(3395264,)####(3395265,)####(3395376,)####(3395377,)####(3395378,)####(3395379,)####(3395380,)####(3395381,)####(3395382,)####(3395383,)####(3395384,)####(3395385,)####(3395386,)####(3395387,)####(3395388,)####(3395389,)####(3395390,)####(3395391,)####(3395392,)####(3395393,)####(3395394,)####(3395412,)####(3395427,)####(3395437,)####(3395438,)####(3395439,)####(3395452,)####(3395455,)####(3395463,)####(3395473,)####(3395476,)####(3395479,)####(3395495,)####(3395496,)####(3395497,)####(3395498,)####(3395499,)####(3395500,)####(3395501,)####(3395502,)####(3395503,)####(3395504,)####(3395505,)####(3395506,)####(3395507,)####(3395508,)####(3395509,)####(3395510,)####(3395511,)####(3395540,)####(3395541,)####(3395542,)####(3395543,)####(3395544,)####(3395545,)####(3395546,)####(3395547,)####(3395548,)####(3395549,)####(3395550,)####(3395551,)####(3395552,)####(3395553,)####(3395554,)####(3395555,)####(3395556,)####(3395557,)####(3395558,)####(3395559,)####(3395560,)####(3395561,)####(3395562,)####(3395563,)####(3395564,)####(3395565,)####(3395566,)####(3395567,)####(3395512,)####(3395513,)####(3395514,)####(3395515,)####(3395516,)####(3395517,)####(3395518,)####(3395519,)####(3395520,)####(3395521,)####(3395522,)####(3395523,)####(3395524,)####(3395525,)####(3395526,)####(3395527,)####(3395528,)####(3395529,)####(3395530,)####(3395267,)####(3395531,)####(3395532,)####(3395268,)####(3395533,)####(3395534,)####(3395269,)####(3395535,)####(3395270,)####(3395536,)####(3395537,)####(3395538,)####(3395271,)####(3395272,)####(3395273,)####(3395274,)####(3395275,)####(3395276,)####(3395277,)####(3395278,)####(3395279,)####(3395280,)####(3395281,)####(3395282,)####(3395283,)####(3395284,)####(3395285,)####(3395286,)####(3395287,)####(3395288,)####(3395289,)####(3395290,)####(3395291,)####(3395292,)####(3395293,)####(3395294,)####(3395295,)####(3395296,)####(3395297,)####(3395298,)####(3395312,)####(3395313,)####(3395314,)####(3395315,)####(3395316,)####(3395317,)####(3395318,)####(3395319,)####(3395320,)####(3395321,)####(3395322,)####(3395323,)####(3395324,)####(3395325,)####(3395326,)####(3395327,)####(3395328,)####(3395329,)####(3395330,)####(3395331,)####(3395332,)####(3395333,)####(3395334,)####(3395335,)####(3395336,)####(3395337,)####(3395338,)####(3395339,)####(3395340,)####(3395341,)####(3395342,)####(3395343,)####(3395344,)####(3395345,)####(3395346,)####(3395360,)####(3395361,)####(3395362,)####(3395363,)####(3395364,)####(3395365,)####(3395366,)####(3456205,)####(3445544,)####(3445545,)####(3445546,)####(3445547,)####(3445548,)####(3445549,)####(3445550,)####(3445551,)####(3445552,)####(3445553,)####(3445554,)####(3445555,)####(3445556,)####(3445557,)####(3445558,)####(3445559,)####(3445560,)####(3445561,)####(3445562,)####(3445563,)####(3445564,)####(3445565,)####(3445566,)####(3445567,)####(3445568,)####(3445569,)####(3445570,)####(3445571,)####(3445572,)####(3445573,)####(3445574,)####(3445575,)####(3445576,)####(3445577,)####(3445578,)####(3445579,)####(3445580,)####(3445581,)####(3445582,)####(3445583,)####(3445584,)####(3445585,)####(3445586,)####(3445587,)####(3445588,)####(3448970,)####(3448971,)####(3448972,)####(3448973,)####(3448974,)####(3448975,)####(3448976,)####(3448977,)####(3448978,)####(3448979,)####(3448980,)####(3448981,)####(3448982,)####(3448983,)####(3448984,)####(3448985,)####(3448986,)####(3448987,)####(3448988,)####(3448989,)####(3448990,)####(3448991,)####(3448992,)####(3448993,)####(3448994,)####(3448995,)####(3448996,)####(3448997,)####(3448998,)####(3455270,)####(3455271,)####(3455272,)####(3455273,)####(3455274,)####(3455275,)####(3455276,)####(3455277,)####(3455278,)####(3455279,)####(3455280,)####(3455281,)####(3455282,)####(3455283,)####(3455284,)####(3455285,)####(3455286,)####(3455287,)####(3455288,)####(3455289,)####(3455290,)####(3455291,)####(3455292,)####(3455293,)####(3455294,)####(3455295,)####(3455296,)####(3455297,)####(3455298,)####(3455299,)####(3455300,)####(3455301,)####(3455302,)####(3455303,)####(3455304,)####(3455305,)####(3455306,)####(3455307,)####(3455308,)####(3455309,)####(3455310,)####(3455311,)####(3455312,)####(3455313,)####(3455314,)####(3455315,)####(3455316,)####(3455317,)####(3455318,)####(3455319,)####(3455320,)####(3455321,)####(3455322,)####(3455323,)####(3455324,)####(3455325,)####(3455326,)####(3455327,)####(3455328,)####(3455329,)####(3455330,)####(3455331,)####(3455332,)####(3455333,)####(3456728,)####(3456729,)####(3456730,)####(3456731,)####(3456732,)####(3456733,)####(3456734,)####(3456735,)####(3456736,)####(3456737,)####(3456738,)####(3456739,)####(3456740,)####(3456741,)####(3456742,)####(3456743,)####(3456744,)####(3456745,)####(3456746,)####(3456747,)####(3456748,)####(3445589,)####(3445590,)####(3445591,)####(3445592,)####(3445593,)####(3445594,)####(3445595,)####(3445596,)####(3445597,)####(3445598,)####(3445599,)####(3445600,)####(3445601,)####(3445602,)####(3445603,)####(3445604,)####(3445605,)####(3445606,)####(3448999,)####(3449000,)####(3449001,)####(3449002,)####(3449003,)####(3449004,)####(3449005,)####(3449006,)####(3449007,)####(3449008,)####(3449009,)####(3449010,)####(3449011,)####(3449012,)####(3449013,)####(3449014,)####(3449015,)####(3449016,)####(3449017,)####(3449018,)####(3449019,)####(3449020,)####(3449021,)####(3449022,)####(3449023,)####(3449024,)####(3449025,)####(3449026,)####(3449027,)####(3449028,)####(3449029,)####(3449030,)####(3455334,)####(3455335,)####(3455336,)####(3455337,)####(3455338,)####(3456206,)####(3456207,)####(3456208,)####(3456209,)####(3456210,)####(3456211,)####(3456212,)####(3456213,)####(3456214,)####(3456215,)####(3456216,)####(3456217,)####(3456218,)####(3456219,)####(3456220,)####(3456221,)####(3456222,)####(3456223,)####(3456224,)####(3456225,)####(3456226,)####(3456227,)####(3456228,)####(3456229,)####(3456230,)####(3456231,)####(3456232,)####(3456233,)####(3456234,)####(3459253,)####(3459254,)####(3459255,)####(3459256,)####(3459257,)####(3459258,)####(3459259,)####(3459260,)####(3459261,)####(3459262,)####(3459263,)####(3459264,)####(3459265,)####(3469391,)####(3459266,)####(3459267,)####(3459268,)####(3459269,)####(3459270,)####(3459271,)####(3459272,)####(3459273,)####(3459274,)####(3459275,)####(3459276,)####(3459277,)####(3459278,)####(3459279,)####(3459280,)####(3459281,)####(3459282,)####(3459283,)####(3459284,)####(3459285,)####(3459286,)####(3459287,)####(3459288,)####(3459289,)####(3459290,)####(3459291,)####(3459292,)####(3469392,)####(3469393,)####(3469394,)####(3469395,)####(3469396,)####(3469397,)####(3469398,)####(3469399,)####(3469400,)####(3469401,)####(3469402,)####(3469403,)####(3469404,)####(3469405,)####(3469406,)####(3469407,)####(3469408,)####(3469409,)####(3469410,)####(3469411,)####(3469412,)####(3469413,)####(3469414,)####(3469415,)####(3469416,)####(3469417,)####(3469418,)####(3469419,)####(3469420,)####(3469421,)####(3469422,)####(3469423,)####(3469424,)####(3469425,)####(3469426,)####(3469427,)####(3469428,)####(3469429,)####(3528844,)####(3496895,)####(3496896,)####(3496897,)####(3496898,)####(3496899,)####(3496900,)####(3496901,)####(3496902,)####(3496903,)####(3496904,)####(3496905,)####(3496906,)####(3496907,)####(3496908,)####(3496909,)####(3496910,)####(3496911,)####(3496912,)####(3496913,)####(3496914,)####(3496915,)####(3496916,)####(3496917,)####(3496918,)####(3496919,)####(3496920,)####(3496921,)####(3496922,)####(3496923,)####(3496924,)####(3496925,)####(3496926,)####(3496927,)####(3496928,)####(3496929,)####(3496930,)####(3496931,)####(3496932,)####(3496933,)####(3496934,)####(3496935,)####(3496936,)####(3496937,)####(3496938,)####(3496939,)####(3496940,)####(3496941,)####(3496942,)####(3496943,)####(3507073,)####(3507074,)####(3507075,)####(3507076,)####(3507077,)####(3507078,)####(3507079,)####(3507080,)####(3507081,)####(3507082,)####(3507083,)####(3507084,)####(3507085,)####(3507086,)####(3507087,)####(3507088,)####(3507089,)####(3507090,)####(3507091,)####(3507092,)####(3507093,)####(3507094,)####(3507095,)####(3507096,)####(3507097,)####(3511627,)####(3511628,)####(3511629,)####(3511630,)####(3511631,)####(3511632,)####(3511633,)####(3511634,)####(3528854,)####(3528855,)####(3528856,)####(3528857,)####(3528858,)####(3528859,)####(3528860,)####(3528861,)####(3528862,)####(3528863,)####(3528864,)####(3528865,)####(3528866,)####(3528867,)####(3528868,)####(3528869,)####(3528870,)####(3528871,)####(3528872,)####(3528873,)####(3528874,)####(3528875,)####(3528876,)####(3528877,)####(3528878,)####(3528879,)####(3528880,)####(3528881,)####(3528882,)####(3496875,)####(3496876,)####(3496877,)####(3496878,)####(3496879,)####(3496880,)####(3496881,)####(3496882,)####(3496883,)####(3496884,)####(3496885,)####(3496886,)####(3496887,)####(3496888,)####(3496889,)####(3496890,)####(3496891,)####(3496892,)####(3496893,)####(3496894,)####(3511635,)####(3511636,)####(3511637,)####(3511638,)####(3511639,)####(3511640,)####(3511641,)####(3511642,)####(3511643,)####(3511644,)####(3511645,)####(3511646,)####(3511647,)####(3528845,)####(3528846,)####(3528847,)####(3528848,)####(3528849,)####(3528850,)####(3528851,)####(3530553,)####(3530554,)####(3530555,)####(3530556,)####(3530557,)####(3530558,)####(3530559,)####(3530560,)####(3530561,)####(3530562,)####(3530563,)####(3530564,)####(3530565,)####(3530566,)####(3530567,)####(3530568,)####(3530569,)####(3530570,)####(3528852,)####(3528853,)####(3534764,)####(3534765,)####(3534766,)####(3534767,)####(3534768,)####(3534769,)####(3534770,)####(3534771,)####(3534772,)####(3534725,)####(3534773,)####(3534726,)####(3534727,)####(3534728,)####(3534729,)####(3534774,)####(3534730,)####(3534731,)####(3534732,)####(3534733,)####(3534734,)####(3534735,)####(3534736,)####(3534737,)####(3534775,)####(3534738,)####(3534739,)####(3534740,)####(3534741,)####(3534742,)####(3534743,)####(3534744,)####(3534776,)####(3534745,)####(3534746,)####(3534747,)####(3534748,)####(3534749,)####(3534750,)####(3534777,)####(3534751,)####(3534752,)####(3534753,)####(3534754,)####(3534755,)####(3534756,)####(3534778,)####(3534757,)####(3534758,)####(3534759,)####(3534760,)####(3534761,)####(3534762,)####(3534779,)####(3534763,)####(3534780,)####(3534781,)####(3534782,)####(3534783,)####(3534784,)####(3534785,)####(3534786,)####(3541978,)####(3536547,)####(3536548,)####(3536549,)####(3536550,)####(3536551,)####(3536552,)####(3536553,)####(3536554,)####(3536555,)####(3536556,)####(3536557,)####(3536558,)####(3536559,)####(3536560,)####(3536561,)####(3536562,)####(3536563,)####(3536564,)####(3536565,)####(3536566,)####(3536567,)####(3536568,)####(3536569,)####(3536570,)####(3536571,)####(3536572,)####(3536573,)####(3536574,)####(3536575,)####(3536576,)####(3536577,)####(3536578,)####(3536579,)####(3536580,)####(3536581,)####(3536582,)####(3536583,)####(3536584,)####(3536585,)####(3536586,)####(3536587,)####(3536588,)####(3536589,)####(3536590,)####(3536591,)####(3536592,)####(3536593,)####(3536594,)####(3536595,)####(3536596,)####(3536597,)####(3536598,)####(3536599,)####(3536600,)####(3536601,)####(3536602,)####(3536603,)####(3536604,)####(3536605,)####(3536606,)####(3536607,)####(3536608,)####(3536609,)####(3536610,)####(3536611,)####(3538077,)####(3538078,)####(3538079,)####(3538080,)####(3538081,)####(3538082,)####(3538083,)####(3538084,)####(3538085,)####(3538086,)####(3538087,)####(3538088,)####(3538089,)####(3538090,)####(3538091,)####(3538092,)####(3538093,)####(3538094,)####(3538095,)####(3538096,)####(3538097,)####(3538098,)####(3538099,)####(3538100,)####(3538101,)####(3538102,)####(3538103,)####(3538104,)####(3538105,)####(3538106,)####(3538107,)####(3538108,)####(3538109,)####(3538110,)####(3538111,)####(3538112,)####(3538113,)####(3538114,)####(3538115,)####(3538116,)####(3538117,)####(3538118,)####(3538119,)####(3538120,)####(3538121,)####(3538122,)####(3538123,)####(3538124,)####(3538125,)####(3538126,)####(3538127,)####(3538128,)####(3538129,)####(3538130,)####(3538131,)####(3538132,)####(3538133,)####(3538134,)####(3538135,)####(3538136,)####(3538137,)####(3538138,)####(3538139,)####(3541957,)####(3541958,)####(3541959,)####(3541960,)####(3541961,)####(3541962,)####(3541963,)####(3541964,)####(3541965,)####(3541966,)####(3541967,)####(3541968,)####(3541969,)####(3541970,)####(3541971,)####(3541972,)####(3541973,)####(3541974,)####(3541975,)####(3541976,)####(3541977,)####(3541979,)####(3541980,)####(3541981,)####(3541982,)####(3541983,)####(3541984,)####(3541985,)####(3541986,)####(3541987,)####(3541988,)####(3541989,)####(3541990,)####(3541991,)####(3541992,)####(3541993,)####(3541994,)####(3541995,)####(3541996,)####(3541997,)####(3541998,)####(3541999,)####(3542000,)####(3542001,)####(3542002,)####(3542003,)####(3542004,)####(3542005,)####(3542006,)####(3542007,)####(3542008,)####(3542009,)####(3542010,)####(3542011,)####(3542012,)####(3542013,)####(3542014,)####(3542015,)####(3542016,)####(3542017,)####(3543134,)####(3543135,)####(3543136,)####(3543137,)####(3543138,)####(3543139,)####(3543140,)####(3543141,)####(3543142,)####(3543143,)####(3543144,)####(3543145,)####(3543146,)####(3543147,)####(3543148,)####(3543149,)####(3543150,)####(3543151,)####(3543152,)####(3543153,)####(3543154,)####(3543155,)####(3543156,)####(3543157,)####(3543158,)####(3543159,)####(3543160,)####(3543161,)####(3543162,)####(3543163,)####(3543164,)####(3543165,)####(3543166,)####(3543167,)####(3543168,)####(3543169,)####(3543170,)####(3543171,)####(3543172,)####(3543173,)####(3543174,)####(3543175,)####(3543176,)####(3543177,)####(3543178,)####(3543179,)####(3543180,)####(3543181,)####(3543182,)####(3543183,)####(3543184,)####(3543185,)####(3543186,)####(3543187,)####(3543188,)####(3543189,)####(3543190,)####(3543191,)####(3543192,)####(3543193,)####(3543194,)####(3543195,)####(3543196,)####(3543197,)####(3543198,)####(3543199,)####(3543200,)####(3543201,)####(3543202,)####(3545410,)####(3545396,)####(3545397,)####(3545398,)####(3545399,)####(3545400,)####(3545401,)####(3545402,)####(3545403,)####(3545404,)####(3545405,)####(3545406,)####(3545407,)####(3545408,)####(3545409,)####(3545411,)####(3545412,)####(3545413,)####(3545414,)####(3545415,)####(3545416,)####(3545417,)####(3545418,)####(3545419,)####(3545420,)####(3545421,)####(3545422,)####(3545423,)####(3553392,)####(3548874,)####(3548875,)####(3548876,)####(3548877,)####(3548878,)####(3548879,)####(3548880,)####(3548881,)####(3548882,)####(3548883,)####(3548884,)####(3548885,)####(3548886,)####(3548887,)####(3548888,)####(3548889,)####(3548890,)####(3548891,)####(3548892,)####(3548893,)####(3548894,)####(3548895,)####(3548896,)####(3548897,)####(3548898,)####(3549295,)####(3549296,)####(3549297,)####(3549298,)####(3549299,)####(3549300,)####(3549301,)####(3549302,)####(3549303,)####(3549304,)####(3549305,)####(3553393,)####(3553394,)####(3553395,)####(3553396,)####(3553397,)####(3553398,)####(3553399,)####(3553400,)####(3553401,)####(3553402,)####(3553403,)####(3553404,)####(3553405,)####(3553406,)####(3553407,)####(3553408,)####(3553409,)####(3553410,)####(3553411,)####(3553412,)####(3553413,)####(3553414,)####(3553415,)####(3553416,)####(3553417,)####(3553418,)####(3553419,)####(3553420,)####(3553421,)####(3553422,)####(3553423,)####(3554498,)####(3554499,)####(3554500,)####(3554501,)####(3554502,)####(3554503,)####(3554504,)####(3554505,)####(3554506,)####(3554507,)####(3554508,)####(3554509,)####(3554510,)####(3554511,)####(3554512,)####(3554513,)####(3554514,)####(3554515,)####(3554516,)####(3554517,)####(3554518,)####(3554519,)####(3554520,)####(3554521,)####(3554522,)####(3554523,)####(3554524,)####(3554525,)####(3554526,)####(3554527,)####(3554528,)####(3554529,)####(3554530,)####(3554531,)####(3554532,)####(3554533,)####(3554534,)####(3554571,)####(3554572,)####(3554573,)####(3554574,)####(3554535,)####(3554575,)####(3554576,)####(3554577,)####(3554536,)####(3554578,)####(3554579,)####(3554580,)####(3554581,)####(3554582,)####(3554583,)####(3554537,)####(3554584,)####(3554585,)####(3554586,)####(3554587,)####(3554588,)####(3554589,)####(3554590,)####(3554538,)####(3554591,)####(3554592,)####(3554593,)####(3554594,)####(3554595,)####(3554539,)####(3554596,)####(3554597,)####(3554598,)####(3554599,)####(3554600,)####(3554601,)####(3557118,)####(3554602,)####(3554603,)####(3557119,)####(3557120,)####(3557121,)####(3557122,)####(3557123,)####(3557124,)####(3557085,)####(3557125,)####(3557086,)####(3557126,)####(3557087,)####(3557088,)####(3557127,)####(3557089,)####(3557128,)####(3557090,)####(3557129,)####(3557091,)####(3557092,)####(3557093,)####(3557130,)####(3557094,)####(3557095,)####(3557131,)####(3557096,)####(3557132,)####(3557097,)####(3557098,)####(3557133,)####(3557099,)####(3557100,)####(3557134,)####(3557101,)####(3557102,)####(3557135,)####(3557103,)####(3557104,)####(3557136,)####(3557105,)####(3557137,)####(3557106,)####(3557107,)####(3557108,)####(3557138,)####(3557109,)####(3557110,)####(3557139,)####(3557111,)####(3557112,)####(3557140,)####(3557113,)####(3557141,)####(3557114,)####(3557115,)####(3557116,)####(3566999,)####(3557117,)####(3567000,)####(3567001,)####(3567002,)####(3567003,)####(3567004,)####(3567005,)####(3567006,)####(3567007,)####(3567008,)####(3567009,)####(3567010,)####(3567011,)####(3567012,)####(3567013,)####(3567014,)####(3567015,)####(3567016,)####(3567017,)####(3567018,)####(3567019,)####(3567020,)####(3567021,)####(3567022,)####(3567023,)####(3567024,)####(3567025,)####(3567026,)####(3567027,)####(3567028,)####(3567029,)####(3567030,)####(3567031,)####(3567032,)####(3571757,)####(3568764,)####(3568765,)####(3568766,)####(3568767,)####(3568768,)####(3568769,)####(3568770,)####(3568771,)####(3568772,)####(3568773,)####(3568774,)####(3568775,)####(3568776,)####(3568777,)####(3568778,)####(3568779,)####(3571758,)####(3571759,)####(3571760,)####(3571761,)####(3571762,)####(3571763,)####(3571764,)####(3571765,)####(3571766,)####(3571767,)####(3571768,)####(3571769,)####(3571770,)####(3571771,)####(3571772,)####(3571773,)####(3571774,)####(3571775,)####(3571776,)####(3579043,)####(3571920,)####(3571921,)####(3571922,)####(3571923,)####(3571924,)####(3571925,)####(3571926,)####(3571927,)####(3571928,)####(3571929,)####(3571930,)####(3571931,)####(3571932,)####(3571933,)####(3571934,)####(3571935,)####(3571936,)####(3571937,)####(3571938,)####(3571939,)####(3571940,)####(3571941,)####(3571942,)####(3571943,)####(3571944,)####(3574481,)####(3574482,)####(3574483,)####(3574484,)####(3574485,)####(3574486,)####(3574487,)####(3574488,)####(3574489,)####(3574490,)####(3574491,)####(3574492,)####(3574493,)####(3574494,)####(3574495,)####(3574496,)####(3574497,)####(3574498,)####(3574499,)####(3574500,)####(3574501,)####(3574502,)####(3579044,)####(3579045,)####(3579046,)####(3579047,)####(3579048,)####(3579049,)####(3579050,)####(3579051,)####(3579052,)####(3579053,)####(3579054,)####(3579055,)####(3579056,)####(3579057,)####(3579058,)####(3579059,)####(3579060,)####(3579061,)####(3579062,)####(3579063,)####(3579064,)####(3579065,)####(3579066,)####(3579067,)####(3579068,)####(3579472,)####(3579473,)####(3579474,)####(3579475,)####(3579476,)####(3579477,)####(3579478,)####(3579479,)####(3579480,)####(3579481,)####(3579482,)####(3579483,)####(3579484,)####(3579485,)####(3579486,)####(3579487,)####(3579488,)####(3579489,)####(3579490,)####(3579491,)####(3579492,)####(3579493,)####(3579494,)####(3579495,)####(3579496,)####(3579497,)####(3579498,)####(3579499,)####(3579500,)####(3579501,)####(3580444,)####(3580445,)####(3580446,)####(3580447,)####(3580448,)####(3580449,)####(3580450,)####(3580451,)####(3580452,)####(3580453,)####(3580454,)####(3580455,)####(3580456,)####(3580457,)####(3580458,)####(3580459,)####(3580460,)####(3580461,)####(3580462,)####(3580463,)####(3580464,)####(3580465,)####(3580466,)####(3580467,)####(3585326,)####(3585327,)####(3585328,)####(3585329,)####(3585330,)####(3585331,)####(3585332,)####(3585333,)####(3585334,)####(3585335,)####(3585336,)####(3585337,)####(3585338,)####(3585339,)####(3585340,)####(3585341,)####(3585342,)####(3585343,)####(3585344,)####(3585345,)####(3585346,)####(3585347,)####(3585348,)####(3585349,)####(3585350,)####(3585351,)####(3585352,)####(3585353,)####(3585354,)####(3585355,)####(3585356,)####(3585357,)####(3590646,)####(3590647,)####(3590648,)####(3590649,)####(3590650,)####(3590651,)####(3590652,)####(3590653,)####(3590654,)####(3590655,)####(3590656,)####(3590657,)####(3590658,)####(3590659,)####(3590660,)####(3590661,)####(3590662,)####(3590663,)####(3590664,)####(3590665,)####(3590666,)####(3590667,)####(3590668,)####(3590669,)####(3590670,)####(3590671,)####(3590672,)####(3590673,)####(3590674,)####(3590675,)####(3590676,)####(3590677,)####(3590678,)####(3590679,)####(3590680,)####(3590681,)####(3590682,)####(3590683,)####(3590684,)####(3590685,)####(3590686,)####(3590687,)####(3590688,)####(3590689,)####(3590690,)####(3590691,)####(3590692,)####(3590693,)####(3590694,)####(3590695,)####(3590696,)####(3590697,)####(3590698,)####(3590699,)####(3590700,)####(3595624,)####(3595648,)####(3595649,)####(3595650,)####(3595651,)####(3595652,)####(3595653,)####(3595654,)####(3595655,)####(3595656,)####(3595657,)####(3595658,)####(3595659,)####(3595660,)####(3595661,)####(3595662,)####(3595663,)####(3595664,)####(3595665,)####(3595666,)####(3595667,)####(3595668,)####(3595669,)####(3595670,)####(3595671,)####(3595672,)####(3595673,)####(3595674,)####(3595675,)####(3595676,)####(3595677,)####(3595678,)####(3595679,)####(3595680,)####(3591315,)####(3591316,)####(3591317,)####(3591318,)####(3591319,)####(3591320,)####(3591321,)####(3591322,)####(3591323,)####(3591324,)####(3591325,)####(3591326,)####(3591327,)####(3591328,)####(3591329,)####(3591330,)####(3591331,)####(3591332,)####(3595625,)####(3595626,)####(3595627,)####(3595628,)####(3595629,)####(3595630,)####(3595631,)####(3595632,)####(3595633,)####(3595634,)####(3595635,)####(3595636,)####(3595637,)####(3595638,)####(3595639,)####(3595640,)####(3595641,)####(3595642,)####(3595643,)####(3595644,)####(3595645,)####(3595646,)####(3595647,)####(3596723,)####(3596724,)####(3596725,)####(3596726,)####(3596727,)####(3596728,)####(3596729,)####(3596730,)####(3596731,)####(3596732,)####(3596733,)####(3596734,)####(3596735,)####(3596736,)####(3596737,)####(3596738,)####(3596739,)####(3607500,)####(3598484,)####(3598485,)####(3598486,)####(3598487,)####(3598488,)####(3598489,)####(3598490,)####(3598491,)####(3598492,)####(3598493,)####(3598494,)####(3598495,)####(3598496,)####(3598497,)####(3607501,)####(3607502,)####(3607503,)####(3607504,)####(3607505,)####(3607506,)####(3607507,)####(3607508,)####(3607509,)####(3607510,)####(3607511,)####(3607512,)####(3607513,)####(3607514,)####(3607515,)####(3607516,)####(3607517,)####(3607518,)####(3607519,)####(3607520,)####(3607521,)####(3607522,)####(3607523,)####(3607524,)####(3607525,)####(3607526,)####(3607527,)####(3607528,)####(3607529,)####(3607530,)####(3607531,)####(3607532,)####(3607533,)####(3607534,)####(3607535,)####(3607536,)####(3607537,)####(3607538,)####(3607539,)####(3607540,)####(3607541,)####(3607542,)####(3607543,)####(3607544,)####(3607545,)####(3607546,)####(3607547,)####(3607548,)####(3607549,)####(3607550,)####(3607551,)####(3607552,)####(3607553,)####(3607554,)####(3607555,)####(3607556,)####(3607557,)####(3607558,)####(3607559,)####(3607560,)####(3607561,)####(3609505,)####(3609506,)####(3609507,)####(3609508,)####(3609509,)####(3609510,)####(3609511,)####(3609512,)####(3609513,)####(3609514,)####(3609515,)####(3609516,)####(3609517,)####(3609518,)####(3609519,)####(3609520,)####(3609521,)####(3609522,)####(3609523,)####(3609524,)####(3609525,)####(3609526,)####(3609527,)####(3609528,)####(3609529,)####(3609530,)####(3609531,)####(3609532,)####(3609533,)####(3616310,)####(3612375,)####(3612376,)####(3612377,)####(3612378,)####(3612379,)####(3612380,)####(3612381,)####(3612382,)####(3612383,)####(3612384,)####(3612385,)####(3612386,)####(3612387,)####(3612388,)####(3612389,)####(3612390,)####(3612391,)####(3612392,)####(3612393,)####(3612394,)####(3612395,)####(3612396,)####(3612397,)####(3612398,)####(3612399,)####(3616103,)####(3616104,)####(3616105,)####(3616106,)####(3616107,)####(3616108,)####(3616109,)####(3616110,)####(3616111,)####(3616112,)####(3616113,)####(3616114,)####(3616115,)####(3616116,)####(3616117,)####(3616118,)####(3616119,)####(3616120,)####(3616121,)####(3616122,)####(3616123,)####(3616124,)####(3616125,)####(3616126,)####(3616127,)####(3616128,)####(3616129,)####(3616130,)####(3616131,)####(3616132,)####(3616133,)####(3616134,)####(3616135,)####(3616136,)####(3616137,)####(3616138,)####(3616139,)####(3616140,)####(3616141,)####(3616142,)####(3616143,)####(3616144,)####(3616145,)####(3616146,)####(3616147,)####(3609763,)####(3609764,)####(3609765,)####(3609766,)####(3609767,)####(3609768,)####(3609769,)####(3609770,)####(3609771,)####(3609772,)####(3609773,)####(3609774,)####(3609775,)####(3609776,)####(3609777,)####(3609778,)####(3609779,)####(3609780,)####(3609781,)####(3609782,)####(3609783,)####(3609784,)####(3609785,)####(3609786,)####(3609787,)####(3609788,)####(3609789,)####(3609790,)####(3609791,)####(3609792,)####(3609793,)####(3609794,)####(3609795,)####(3609796,)####(3609797,)####(3612331,)####(3612332,)####(3612333,)####(3612334,)####(3612335,)####(3612336,)####(3612337,)####(3612338,)####(3612339,)####(3612340,)####(3612341,)####(3612342,)####(3612343,)####(3612344,)####(3612345,)####(3612346,)####(3612347,)####(3612348,)####(3612349,)####(3612350,)####(3612351,)####(3612352,)####(3612353,)####(3612354,)####(3612355,)####(3612356,)####(3612357,)####(3612358,)####(3612359,)####(3612360,)####(3612361,)####(3612362,)####(3612363,)####(3612364,)####(3612365,)####(3612366,)####(3612367,)####(3612368,)####(3612369,)####(3612370,)####(3612371,)####(3612372,)####(3612373,)####(3612374,)####(3614835,)####(3614836,)####(3614837,)####(3614838,)####(3614839,)####(3614840,)####(3614841,)####(3614842,)####(3614843,)####(3614844,)####(3614845,)####(3614846,)####(3614847,)####(3614848,)####(3614849,)####(3614850,)####(3614851,)####(3614852,)####(3614853,)####(3614854,)####(3614855,)####(3614856,)####(3614857,)####(3614858,)####(3614859,)####(3614860,)####(3614861,)####(3614862,)####(3614863,)####(3614864,)####(3614865,)####(3614866,)####(3614867,)####(3614868,)####(3614869,)####(3616148,)####(3616149,)####(3616150,)####(3616151,)####(3616152,)####(3616153,)####(3616154,)####(3616155,)####(3616156,)####(3616157,)####(3616158,)####(3616159,)####(3616160,)####(3616161,)####(3616162,)####(3616163,)####(3616324,)####(3616325,)####(3616326,)####(3616327,)####(3616328,)####(3616329,)####(3616330,)####(3616331,)####(3616332,)####(3616333,)####(3616334,)####(3616335,)####(3616336,)####(3616337,)####(3616338,)####(3616339,)####(3616311,)####(3616312,)####(3616313,)####(3616314,)####(3616315,)####(3616316,)####(3616317,)####(3616318,)####(3616319,)####(3616320,)####(3616321,)####(3616322,)####(3616323,)####(3620714,)####(3617621,)####(3617622,)####(3617623,)####(3617624,)####(3617625,)####(3617626,)####(3617627,)####(3617628,)####(3617629,)####(3617630,)####(3617631,)####(3617632,)####(3617633,)####(3617634,)####(3617635,)####(3617636,)####(3617637,)####(3617638,)####(3617639,)####(3617640,)####(3617641,)####(3617642,)####(3617643,)####(3617644,)####(3617645,)####(3617646,)####(3617647,)####(3617648,)####(3617649,)####(3617650,)####(3617651,)####(3617652,)####(3617653,)####(3617654,)####(3617655,)####(3617656,)####(3617657,)####(3617658,)####(3617659,)####(3617660,)####(3617661,)####(3617662,)####(3617663,)####(3617664,)####(3617665,)####(3617666,)####(3617667,)####(3617668,)####(3617669,)####(3617670,)####(3617671,)####(3617672,)####(3617673,)####(3617674,)####(3617675,)####(3619101,)####(3619102,)####(3619103,)####(3619104,)####(3619105,)####(3619106,)####(3619107,)####(3619108,)####(3619109,)####(3619110,)####(3619111,)####(3619112,)####(3619113,)####(3619114,)####(3619115,)####(3619116,)####(3619117,)####(3619118,)####(3619119,)####(3619120,)####(3619121,)####(3619122,)####(3619123,)####(3619124,)####(3619125,)####(3619126,)####(3619127,)####(3619128,)####(3620759,)####(3620760,)####(3620761,)####(3620762,)####(3620763,)####(3620764,)####(3620765,)####(3620766,)####(3620767,)####(3620768,)####(3620769,)####(3620770,)####(3620771,)####(3620772,)####(3620773,)####(3620774,)####(3620775,)####(3620776,)####(3620777,)####(3620715,)####(3620716,)####(3620717,)####(3620718,)####(3620719,)####(3620720,)####(3620721,)####(3620722,)####(3620723,)####(3620724,)####(3620725,)####(3620726,)####(3620727,)####(3620728,)####(3620729,)####(3620730,)####(3620731,)####(3620732,)####(3620733,)####(3620734,)####(3620735,)####(3620736,)####(3620737,)####(3620738,)####(3620739,)####(3620740,)####(3620741,)####(3620742,)####(3620743,)####(3620744,)####(3620745,)####(3620746,)####(3620747,)####(3620748,)####(3620749,)####(3620750,)####(3620751,)####(3620752,)####(3620753,)####(3620754,)####(3620755,)####(3620756,)####(3620757,)####(3620758,)####(3630680,)####(3627733,)####(3627734,)####(3627735,)####(3627736,)####(3627737,)####(3627738,)####(3627739,)####(3627740,)####(3627741,)####(3627742,)####(3627743,)####(3627744,)####(3627745,)####(3627746,)####(3622944,)####(3622945,)####(3622946,)####(3622947,)####(3622948,)####(3622949,)####(3622950,)####(3622951,)####(3622952,)####(3622953,)####(3622954,)####(3622955,)####(3622956,)####(3630681,)####(3630682,)####(3630683,)####(3630684,)####(3630685,)####(3630686,)####(3630687,)####(3630688,)####(3630689,)####(3630690,)####(3630673,)####(3630691,)####(3630674,)####(3630675,)####(3630692,)####(3630676,)####(3630693,)####(3630677,)####(3630678,)####(3630694,)####(3630679,)####(3630695,)####(3630696,)####(3630697,)####(3630698,)####(3630699,)####(3630700,)####(3630701,)####(3630702,)####(3630703,)####(3630704,)####(3630705,)####(3630706,)####(3630707,)####(3630708,)####(3630709,)####(3630710,)####(3630711,)####(3630712,)####(3630713,)####(3630714,)####(3630715,)####(3630716,)####(3630717,)####(3630718,)####(3630719,)####(3630720,)####(3630721,)####(3630722,)####(3630723,)####(3630724,)####(3630725,)####(3630726,)####(3630727,)####(3630728,)####(3630729,)####(3630730,)####(3630731,)####(3630732,)####(3630733,)####(3630734,)####(3630735,)####(3636845,)####(3633043,)####(3633044,)####(3633045,)####(3633046,)####(3633047,)####(3633048,)####(3633049,)####(3633050,)####(3633051,)####(3633052,)####(3633053,)####(3633054,)####(3633055,)####(3633056,)####(3633057,)####(3633058,)####(3633059,)####(3633060,)####(3633061,)####(3633062,)####(3633063,)####(3633064,)####(3633065,)####(3633066,)####(3633067,)####(3633068,)####(3633069,)####(3633070,)####(3633071,)####(3633072,)####(3633073,)####(3633074,)####(3633075,)####(3633076,)####(3633077,)####(3633078,)####(3633079,)####(3633080,)####(3633081,)####(3633082,)####(3633083,)####(3633084,)####(3633085,)####(3633086,)####(3633087,)####(3633088,)####(3633089,)####(3633090,)####(3633091,)####(3633092,)####(3633093,)####(3633094,)####(3633095,)####(3633096,)####(3633097,)####(3633098,)####(3633099,)####(3633821,)####(3633822,)####(3633823,)####(3633824,)####(3633825,)####(3633826,)####(3633827,)####(3633828,)####(3633829,)####(3633830,)####(3633831,)####(3633832,)####(3633833,)####(3633834,)####(3633835,)####(3633836,)####(3633837,)####(3633838,)####(3633839,)####(3633840,)####(3633841,)####(3633842,)####(3633843,)####(3633844,)####(3633845,)####(3633846,)####(3633847,)####(3633848,)####(3633849,)####(3633850,)####(3633851,)####(3633852,)####(3633853,)####(3633854,)####(3633794,)####(3633795,)####(3633796,)####(3633797,)####(3633798,)####(3633799,)####(3633800,)####(3633801,)####(3633802,)####(3633803,)####(3633804,)####(3633805,)####(3633806,)####(3633807,)####(3633808,)####(3633809,)####(3633810,)####(3633811,)####(3633812,)####(3633813,)####(3633814,)####(3633815,)####(3633816,)####(3633817,)####(3633818,)####(3633819,)####(3633820,)####(3636846,)####(3636847,)####(3636848,)####(3636849,)####(3636850,)####(3636851,)####(3636852,)####(3636853,)####(3636854,)####(3636855,)####(3636856,)####(3636857,)####(3636858,)####(3636859,)####(3636860,)####(3636861,)####(3636862,)####(3636863,)####(3636864,)####(3636865,)####(3636866,)####(3636867,)####(3636868,)####(3636869,)####(3636870,)####(3636871,)####(3636872,)####(3636873,)####(3636874,)####(3636875,)####(3636876,)####(3636877,)####(3636878,)####(3636879,)####(3636880,)####(3636881,)####(3636882,)####(3636883,)####(3636884,)####(3636885,)####(3636886,)####(3636887,)####(3636888,)####(3636889,)####(3636890,)####(3636891,)####(3636892,)####(3636893,)####(3636894,)####(3636895,)####(3636896,)####(3636897,)####(3636898,)####(3636899,)####(3636900,)####(3636901,)####(3636902,)####(3636903,)####(3636904,)####(3636905,)####(3648564,)####(3637557,)####(3637558,)####(3637559,)####(3637560,)####(3637561,)####(3637562,)####(3637563,)####(3637564,)####(3637565,)####(3637566,)####(3637567,)####(3637568,)####(3637569,)####(3637570,)####(3637571,)####(3637572,)####(3637573,)####(3637574,)####(3637575,)####(3637576,)####(3637577,)####(3637578,)####(3637579,)####(3637580,)####(3637581,)####(3637582,)####(3637583,)####(3637584,)####(3637585,)####(3637586,)####(3643797,)####(3643798,)####(3643799,)####(3643800,)####(3643801,)####(3643802,)####(3643803,)####(3643804,)####(3643805,)####(3643806,)####(3643807,)####(3643808,)####(3643809,)####(3643810,)####(3643811,)####(3643812,)####(3643813,)####(3643814,)####(3643815,)####(3643816,)####(3643817,)####(3643818,)####(3643819,)####(3643820,)####(3643821,)####(3643822,)####(3643823,)####(3643824,)####(3643825,)####(3643826,)####(3643827,)####(3643828,)####(3643829,)####(3643830,)####(3643831,)####(3643832,)####(3643833,)####(3643834,)####(3648565,)####(3648566,)####(3648567,)####(3648568,)####(3648569,)####(3648570,)####(3648571,)####(3648572,)####(3648573,)####(3648574,)####(3648575,)####(3648576,)####(3648577,)####(3648578,)####(3648579,)####(3648580,)####(3648581,)####(3648582,)####(3648583,)####(3648584,)####(3648585,)####(3648586,)####(3648587,)####(3648588,)####(3647799,)####(3647800,)####(3647801,)####(3647802,)####(3647803,)####(3647804,)####(3647805,)####(3647806,)####(3647807,)####(3647808,)####(3647809,)####(3647810,)####(3647811,)####(3647812,)####(3647813,)####(3647814,)####(3647815,)####(3647816,)####(3647817,)####(3647818,)####(3647819,)####(3647820,)####(3647821,)####(3647822,)####(3647823,)####(3647824,)####(3647825,)####(3647826,)####(3647827,)####(3647828,)####(3647829,)####(3647830,)####(3647831,)####(3647832,)####(3647833,)####(3647834,)####(3647835,)####(3647836,)####(3647837,)####(3647838,)####(3647839,)####(3647840,)####(3647841,)####(3647842,)####(3647843,)####(3647844,)####(3647845,)####(3647846,)####(3647847,)####(3653996,)####(3654015,)####(3654016,)####(3654017,)####(3654018,)####(3654019,)####(3654020,)####(3653997,)####(3654021,)####(3654022,)####(3654023,)####(3654024,)####(3654025,)####(3653998,)####(3654026,)####(3654027,)####(3654028,)####(3654029,)####(3654030,)####(3654031,)####(3654032,)####(3654033,)####(3654034,)####(3654035,)####(3653999,)####(3654036,)####(3654037,)####(3654038,)####(3654039,)####(3654040,)####(3654041,)####(3654000,)####(3654042,)####(3654043,)####(3654044,)####(3654001,)####(3654045,)####(3654046,)####(3654047,)####(3654048,)####(3654049,)####(3654050,)####(3654002,)####(3654051,)####(3654052,)####(3654053,)####(3654054,)####(3654055,)####(3654003,)####(3654056,)####(3654057,)####(3654058,)####(3654059,)####(3654060,)####(3654004,)####(3654005,)####(3654006,)####(3654007,)####(3654008,)####(3654009,)####(3654010,)####(3654011,)####(3654012,)####(3654013,)####(3654014,)####(3662953,)####(3655819,)####(3655820,)####(3655821,)####(3655822,)####(3655823,)####(3655824,)####(3655825,)####(3655826,)####(3655827,)####(3655828,)####(3655829,)####(3655830,)####(3655831,)####(3655832,)####(3655833,)####(3655834,)####(3655835,)####(3655836,)####(3655837,)####(3655838,)####(3655839,)####(3655840,)####(3655841,)####(3655842,)####(3655843,)####(3655844,)####(3655845,)####(3655846,)####(3655847,)####(3655848,)####(3662954,)####(3662955,)####(3662986,)####(3662956,)####(3662987,)####(3662988,)####(3662989,)####(3662957,)####(3662990,)####(3662991,)####(3662992,)####(3662993,)####(3662994,)####(3662995,)####(3662996,)####(3662997,)####(3662958,)####(3662998,)####(3662999,)####(3663000,)####(3662959,)####(3663001,)####(3663002,)####(3663003,)####(3663004,)####(3662960,)####(3662961,)####(3662962,)####(3662963,)####(3662964,)####(3662965,)####(3662966,)####(3662967,)####(3662968,)####(3662969,)####(3662970,)####(3662971,)####(3662972,)####(3662973,)####(3662974,)####(3662975,)####(3662976,)####(3662977,)####(3662978,)####(3662979,)####(3662980,)####(3662981,)####(3662982,)####(3662983,)####(3662984,)####(3662985,)####(3669229,)####(3664077,)####(3664078,)####(3664079,)####(3664080,)####(3664081,)####(3664082,)####(3664083,)####(3664084,)####(3664085,)####(3664086,)####(3664087,)####(3664088,)####(3664089,)####(3664090,)####(3664091,)####(3664092,)####(3664093,)####(3664094,)####(3664095,)####(3664096,)####(3664097,)####(3664098,)####(3664099,)####(3667460,)####(3667461,)####(3667462,)####(3667463,)####(3667464,)####(3667465,)####(3667466,)####(3667467,)####(3667468,)####(3667469,)####(3667470,)####(3667471,)####(3667472,)####(3667473,)####(3667474,)####(3667475,)####(3667476,)####(3667477,)####(3667478,)####(3667479,)####(3667480,)####(3667481,)####(3667482,)####(3667483,)####(3667484,)####(3667485,)####(3667486,)####(3667487,)####(3667488,)####(3667489,)####(3667490,)####(3667491,)####(3667492,)####(3669230,)####(3669231,)####(3669232,)####(3669233,)####(3669234,)####(3669235,)####(3669236,)####(3669237,)####(3669238,)####(3669239,)####(3669240,)####(3669241,)####(3669242,)####(3669243,)####(3669244,)####(3669245,)####(3669246,)####(3669247,)####(3669248,)####(3672093,)####(3672094,)####(3672095,)####(3672096,)####(3672097,)####(3672098,)####(3672099,)####(3672100,)####(3672082,)####(3672083,)####(3672084,)####(3672101,)####(3672085,)####(3672086,)####(3672087,)####(3672088,)####(3672102,)####(3672089,)####(3672090,)####(3672091,)####(3672092,)####(3672103,)####(3672104,)####(3672105,)####(3672106,)####(3672107,)####(3672108,)####(3672109,)####(3672110,)####(3672111,)####(3672112,)####(3672113,)####(3672114,)####(3672115,)####(3672116,)####(3672117,)####(3672118,)####(3672119,)####(3672120,)####(3672121,)####(3672122,)####(3672123,)####(3672124,)####(3672125,)####(3672126,)####(3672127,)####(3672128,)####(3672129,)####(3672130,)####(3675326,)####(3673177,)####(3673178,)####(3673179,)####(3673180,)####(3673181,)####(3673182,)####(3673183,)####(3673184,)####(3673185,)####(3673186,)####(3673187,)####(3673188,)####(3673189,)####(3673190,)####(3673191,)####(3673192,)####(3673193,)####(3673194,)####(3673195,)####(3673196,)####(3673197,)####(3673198,)####(3673199,)####(3673200,)####(3673201,)####(3673202,)####(3673203,)####(3673204,)####(3673205,)####(3675327,)####(3675317,)####(3675328,)####(3675318,)####(3675319,)####(3675329,)####(3675320,)####(3675321,)####(3675330,)####(3675322,)####(3675323,)####(3675331,)####(3675324,)####(3675325,)####(3675332,)####(3675333,)####(3675334,)####(3675335,)####(3675336,)####(3675337,)####(3675338,)####(3675339,)####(3675340,)####(3675341,)####(3680712,)####(3680713,)####(3680714,)####(3680715,)####(3680716,)####(3680717,)####(3680718,)####(3680719,)####(3680720,)####(3680721,)####(3680722,)####(3680723,)####(3680724,)####(3680683,)####(3680684,)####(3680685,)####(3680686,)####(3680687,)####(3680688,)####(3680689,)####(3680690,)####(3680691,)####(3680692,)####(3680693,)####(3680694,)####(3680695,)####(3680696,)####(3680697,)####(3680698,)####(3680699,)####(3680700,)####(3680701,)####(3680702,)####(3680703,)####(3680704,)####(3680705,)####(3680706,)####(3680707,)####(3680708,)####(3680709,)####(3680710,)####(3680711,)",
        "base_pg_sql": "SELECT T3.trans_id FROM district AS T1 INNER JOIN account AS T2 ON T1.district_id = T2.district_id INNER JOIN trans AS T3 ON T2.account_id = T3.account_id WHERE T1.district_id = 5",
        "base_question": "Please list all the transactions made by accounts from district 5.",
        "base_evidence": "",
        "gt": {
            "type": "delete",
            "table": "trans",
            "condition": "SELECT trans.trans_id FROM trans, district AS T1, account AS T2 WHERE T1.district_id = T2.district_id AND T2.account_id = trans.account_id AND T1.district_id = 5 "
        }
    },
    {
        "question_id": 704,
        "db_id": "codebase_community",
        "question": "Delete the tag named 'sample' from the tags table.",
        "evidence": "",
        "SQL": "SELECT ExcerptPostId, WikiPostId FROM tags WHERE TagName = 'sample'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM tags WHERE TagName = 'sample' ",
        "result_size": 1,
        "result": "(28276, 28275)",
        "base_pg_sql": "SELECT ExcerptPostId, WikiPostId FROM tags WHERE TagName = 'sample'",
        "base_question": "What is the excerpt post ID and wiki post ID of the tag named sample?",
        "base_evidence": "tag named sample refers to TagName = 'sample';",
        "gt": {
            "type": "delete",
            "table": "tags",
            "condition": "SELECT tags.id FROM tags WHERE TagName = 'sample' "
        }
    },
    {
        "question_id": 1252,
        "db_id": "thrombosis_prediction",
        "question": "Delete all patients who have both a normal Ig G level (between 900 and 2000) and exhibit symptoms.",
        "evidence": "normal Ig G level refers to IGG > 900 and IGG < 2000; have symptoms refers to Symptoms IS NOT NULL;",
        "SQL": "SELECT COUNT(T1.ID) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID INNER JOIN Examination AS T3 ON T3.ID = T2.ID WHERE T2.IGG BETWEEN 900 AND 2000 AND T3.Symptoms IS NOT NULL",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM Patient USING Laboratory AS T2, Examination AS T3 WHERE Patient.ID = T2.ID AND T3.ID = T2.ID AND T2.IGG BETWEEN 900 AND 2000 AND NOT T3.Symptoms IS NULL ",
        "result_size": 1,
        "result": "(4,)",
        "base_pg_sql": "SELECT COUNT(T1.ID) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID INNER JOIN Examination AS T3 ON T3.ID = T2.ID WHERE T2.IGG BETWEEN 900 AND 2000 AND NOT T3.Symptoms IS NULL",
        "base_question": "Among the patients with a normal Ig G level, how many of them have symptoms?",
        "base_evidence": "normal Ig G level refers to IGG > 900 and IGG < 2000; have symptoms refers to Symptoms IS NOT NULL;",
        "gt": {
            "type": "delete",
            "table": "Patient",
            "condition": "SELECT Patient.id FROM Patient, Laboratory AS T2, Examination AS T3 WHERE Patient.ID = T2.ID AND T3.ID = T2.ID AND T2.IGG BETWEEN 900 AND 2000 AND NOT T3.Symptoms IS NULL "
        }
    },
    {
        "question_id": 207,
        "db_id": "toxicology",
        "question": "Delete all atoms that are part of a double type bond.",
        "evidence": "double type bond refers to bond_type = '=';",
        "SQL": "SELECT DISTINCT T1.element FROM atom AS T1 INNER JOIN bond AS T2 ON T1.molecule_id = T2.molecule_id INNER JOIN connected AS T3 ON T1.atom_id = T3.atom_id WHERE T2.bond_type = '='",
        "difficulty": "challenging",
        "pg_sql": "DELETE FROM atom USING bond AS T2, connected AS T3 WHERE atom.molecule_id = T2.molecule_id AND atom.atom_id = T3.atom_id AND T2.bond_type = '=' ",
        "result_size": 14,
        "result": "('sn',)####('ca',)####('cu',)####('p',)####('h',)####('cl',)####('o',)####('s',)####('c',)####('br',)####('te',)####('n',)####('f',)####('pb',)",
        "base_pg_sql": "SELECT DISTINCT T1.element FROM atom AS T1 INNER JOIN bond AS T2 ON T1.molecule_id = T2.molecule_id INNER JOIN connected AS T3 ON T1.atom_id = T3.atom_id WHERE T2.bond_type = '='",
        "base_question": "What elements are in a double type bond?",
        "base_evidence": "double type bond refers to bond_type = '=';",
        "gt": {
            "type": "delete",
            "table": "atom",
            "condition": "SELECT atom.atom_id FROM atom, bond AS T2, connected AS T3 WHERE atom.molecule_id = T2.molecule_id AND atom.atom_id = T3.atom_id AND T2.bond_type = '=' "
        }
    },
    {
        "question_id": 471,
        "db_id": "card_games",
        "question": "Delete the card set whose translated name is \"Hauptset Zehnte Edition\" from the sets table.",
        "evidence": "card set \"Hauptset Zehnte Edition\" refers to translation = ' Hauptset Zehnte Edition'; expansion type refers to type",
        "SQL": "SELECT T1.type FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T2.translation = 'Hauptset Zehnte Edition'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM sets USING set_translations AS T2 WHERE T2.setCode = sets.code AND T2.translation = 'Hauptset Zehnte Edition' ",
        "result_size": 1,
        "result": "('core',)",
        "base_pg_sql": "SELECT T1.type FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T2.translation = 'Hauptset Zehnte Edition'",
        "base_question": "What is the expansion type of the set \"Hauptset Zehnte Edition\"?",
        "base_evidence": "card set \"Hauptset Zehnte Edition\" refers to translation = ' Hauptset Zehnte Edition'; expansion type refers to type",
        "gt": {
            "type": "delete",
            "table": "sets",
            "condition": "SELECT sets.id FROM sets, set_translations AS T2 WHERE T2.setCode = sets.code AND T2.translation = 'Hauptset Zehnte Edition' "
        }
    },
    {
        "question_id": 653,
        "db_id": "codebase_community",
        "question": "Delete the user who owns the most popular post, based on the highest view count.",
        "evidence": "Higher view count means the post has higher popularity; the most popular post refers to MAX(ViewCount);",
        "SQL": "SELECT DisplayName FROM users WHERE Id = ( SELECT OwnerUserId FROM posts ORDER BY ViewCount DESC LIMIT 1 )",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM users WHERE Id = (SELECT OwnerUserId FROM posts ORDER BY ViewCount DESC NULLS LAST LIMIT 1) ",
        "result_size": 1,
        "result": "('Sharpie',)",
        "base_pg_sql": "SELECT DisplayName FROM users WHERE Id = (SELECT OwnerUserId FROM posts ORDER BY ViewCount DESC NULLS LAST LIMIT 1)",
        "base_question": "What is the owner's display name of the most popular post?",
        "base_evidence": "Higher view count means the post has higher popularity; the most popular post refers to MAX(ViewCount);",
        "gt": {
            "type": "delete",
            "table": "users",
            "condition": "SELECT users.id FROM users WHERE Id = (SELECT OwnerUserId FROM posts ORDER BY ViewCount DESC NULLS LAST LIMIT 1) "
        }
    },
    {
        "question_id": 1434,
        "db_id": "student_club",
        "question": "Delete all zip codes that have post office boxes in San Juan Municipio, Puerto Rico.",
        "evidence": "zip codes that have post office boxes refers to type = 'PO Box'",
        "SQL": "SELECT zip_code FROM zip_code WHERE type = 'PO Box' AND county = 'San Juan Municipio' AND state = 'Puerto Rico'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM zip_code WHERE type = 'PO Box' AND county = 'San Juan Municipio' AND state = 'Puerto Rico' ",
        "result_size": 2,
        "result": "(906,)####(936,)",
        "base_pg_sql": "SELECT zip_code FROM zip_code WHERE type = 'PO Box' AND county = 'San Juan Municipio' AND state = 'Puerto Rico'",
        "base_question": "What are the zip codes that have post office boxes in the country of the country of San Juan Municipio whose state is Puerto Rico?",
        "base_evidence": "zip codes that have post office boxes refers to type = 'PO Box'",
        "gt": {
            "type": "delete",
            "table": "zip_code",
            "condition": "SELECT zip_code.zip_code FROM zip_code WHERE type = 'PO Box' AND county = 'San Juan Municipio' AND state = 'Puerto Rico' "
        }
    },
    {
        "question_id": 487,
        "db_id": "card_games",
        "question": "\"Delete all cards from the set named 'Coldsnap'.\"",
        "evidence": "",
        "SQL": "SELECT CAST(SUM(CASE WHEN T1.cardKingdomFoilId IS NOT NULL AND T1.cardKingdomId IS NOT NULL THEN 1 ELSE 0 END) AS REAL) * 100 / COUNT(T1.id) FROM cards AS T1 INNER JOIN sets AS T2 ON T2.code = T1.setCode WHERE T2.name = 'Coldsnap'",
        "difficulty": "challenging",
        "pg_sql": "DELETE FROM cards USING sets AS T2 WHERE T2.code = cards.setCode AND T2.name = 'Coldsnap' ",
        "result_size": 1,
        "result": "(100.0,)",
        "base_pg_sql": "SELECT CAST(SUM(CASE WHEN NOT T1.cardKingdomFoilId IS NULL AND NOT T1.cardKingdomId IS NULL THEN 1 ELSE 0 END) AS REAL) * 100 / NULLIF(COUNT(T1.id), 0) FROM cards AS T1 INNER JOIN sets AS T2 ON T2.code = T1.setCode WHERE T2.name = 'Coldsnap'",
        "base_question": "What is the percentage of incredibly powerful cards in the set Coldsnap?",
        "base_evidence": "card set Coldsnap refers to name = 'Coldsnap'; foil is incredibly powerful refers to cardKingdomFoilId is not null AND cardKingdomId is not null; the percentage of incredibly powerful cards in the set refers to DIVIDE(SUM(incredibly powerful), SUM(name = 'Coldsnap'))*100",
        "gt": {
            "type": "delete",
            "table": "cards",
            "condition": "SELECT cards.id FROM cards, sets AS T2 WHERE T2.code = cards.setCode AND T2.name = 'Coldsnap' "
        }
    },
    {
        "question_id": 706,
        "db_id": "codebase_community",
        "question": "Delete all comments that are associated with posts having titles containing 'linear regression'.",
        "evidence": "about linear regression refers to Title contains 'linear regression'",
        "SQL": "SELECT T1.Text FROM comments AS T1 INNER JOIN posts AS T2 ON T1.PostId = T2.Id WHERE T2.Title LIKE '%linear regression%'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM comments USING posts AS T2 WHERE comments.PostId = T2.Id AND T2.Title LIKE '%linear regression%' ",
        "result_size": 1530,
        "result": "('Are you asking about how to reduce the effect of outliers or when to use the log of some variable?',)####('I think that the OP is saying \"I\\'ve heard of people using the log on input variables: why do they do that?\"',)####('Are you refering to the following paper?\\nKuk, A.Y.C. (1984) All subsets regression in a proportional hazards model. Biometrika, 71, 587-592',)####('If I am not mistaken, \\n\\nlinear regression is the estimation of coefficients that define a good linear map from X to Y.\\n\\n ANOVA is a test to know if there is significant differences in X when Y take two different values.  \\n\\nCan you explain us why you think they are the same?',)####('Really great question.  The best way to understand something is from multiple direction of explanation.',)####(\"yes indeed. I guess I'll have to dig up that paper somehow. It seems old, however.\",)####('What R package did you use for HC3 estimation? `sandwich`, `contrast`?',)####('I have no idea what this question is about.',)####('It sounds like it is about the difference between correlation and simple linear regression.',)####('Find this article in the meantime, The lasso method for variable selection in the cox model, from Tibshirani (Stat. Med. 1997 16: 385-395), http://j.mp/bw0mB9. HTH',)####('and this more recent one (closely linked to the `penalized` R package), http://j.mp/cooIT3. Maybe this one too, http://j.mp/bkDQUj. Cheers',)####('Another question while we are in: What is the design you are considering, I mean is there any clustering or multiple predictors, or is it a simple linear regression? This may help the reader to better understand the context of your study.',)####('Have you tried re-expressing the dependent variable to stabilize variance?',)####('Quick clarification: Do you want to assume that exactly one functional form should fit all of the participants, or do some participants have one form and some another?  If one of the functional form fits them all, do the parameter values get to vary across participants, or not?',)####('I\u00b4m using the sandwich package for the HC3 estimator. I\u00b4m using just simple linear regression. Intuitively I feel more comfortable with the bootstrap version and I guess I\u00b4ll stick with that. //thx for the input',)####('ANOVA can be seen as \"syntactic sugar\" for a special subgroup of linear regression models. ANOVA is regularly used by researchers who are not statisticians by training. They are now \"institutionalized\" and its hard to convert them back to using the more general representation ;-)',)####('Sharpie, I wonder if it\\'s time to accept a \"best answer\"or at least explain why you are not satisfied with the answers on offer so far.',)####('@Conjugate prior: No. It is quite clear from the data that the best functional form varies across participants. I am running separate nonlinear regressions.',)####(\"Yes, I did leave that out, didn't I. The goal is to find a predictive equation for share2 from the variables provided.\",)####(\"Is this what you're looking for? http://www.weibull.com/DOEWeb/confidence_intervals_in_simple_linear_regression.htm\",)####('What is the aim of your analyses?',)####(\"@Can'tTell You can find some help about formatting here: http://stats.stackexchange.com/editing-help\",)####('Could you explain where the 82/15/3 values come from?  You appear to be using numbers that you haven\\'t yet shared with us, such as \"the monthly CDDs\" (whatever those may be) and the \"total predictions for the year\" (whatever those may be).',)####(\"Hopefully it's more clear now, thanks!\",)####('@onestop: I am sorry i missed your question. The comparison is on log odds ratio. Thank you for your interest.',)####('The *Mathematica* help page has a nice illustration with explanations: http://reference.wolfram.com/mathematica/tutorial/UnconstrainedOptimizationNewtonsMethodRoot.html .',)####('Comparison of Region 1 and 2 on what metric? e.g. log odds ratio, log risk ratio, risk difference ...?',)####('\"fails to converge\".. \"do converge eventually\". \"a fit that looks good\"...\"the visual fit is terrible\". Sorry, but I\\'m confused!',)####('By tweaking convergence criteria I can get the model to converge, but when this happens the plotted function appears to be a much worse fit than the unconverged results. Presumably due to very large convergence criteria.',)####(\"If your software is well written it automatically standardizes internally to avoid numerical precision problems.  You shouldn't have to do anything special.\",)####('@Weijlie You can use $\\\\\\\\LaTeX$ code in the usual way, i.e. by enclosing your expression with `$` (inline) or `$$` (display). Also, please make your title shorter (e.g., \"Intuitive illustration of Newton\\'s method in nonlinear regression\").',)####('@chl has nicely answered your question in his reply at http://stats.stackexchange.com/questions/8237/logic-behind-the-anova-f-test-in-simple-linear-regression/8247#8247 : see the second paragraph.',)####(\"Could you please confirm that you're also the asker for [Logic behind the ANOVA F-test in simple linear regression](http://stats.stackexchange.com/questions/8237/logic-behind-the-anova-f-test-in-simple-linear-regression) so that we can merge your two accounts. (You will still need to register, see the [FAQ](http://stats.stackexchange.com/faq).)\",)####('You passed the junk regression (F) test but not t tests -- it is the first sign for multicollinearity problem look here for instance http://stats.stackexchange.com/questions/4099/vif-condition-index-and-eigen-values/8223#8223',)####(\"@David: +1 Thank you for the suggestion. I changed it to a comparison question. Hope this is ok. I should have qualified my question with a simulation tag. I am evaluating my scenarios from a driver's perspective so less crashes is better. My original hypothesis was that the output (time at which 80% of the cars crash) does not depend on the three parameters. But from my results, I am tempted to say that I can reject my hypothesis but how strongly I can reject is still something I am looking into but thank you for the insight.\",)####(\"I recommend you change the title. The answer to the question 'is this result good or bad' is subjective, and would depend on what the predictor variables are - as well as the values of the respondent. Car mechanics / salesmen / manufacturers might prefer more crashes, a car driver might prefer less. The size of the estimate of a standard error is just that - there is no reason to think that smaller is better without context. Perhaps you want to know if your results support your hypothesis or not, or if your results are valid, in which case you would be able to find useful answers here.\",)####(\"I tried to reword a little bit your question. I'm afraid it looks like you're asking two very different questions. For the second one, a lot of resources is available on this site, but also on [CRAN](http://cran.r-project.org/).\",)####('I\\'m surprised that they give you different answers. It might help to clarify what you mean by \"answer\".',)####('@chl, yeap, thanks, I wasnt clear. My questions is really this:  If I write LM in R does R understands it as linear always or triesd to fit any model, not necessarily linear regression but any regression ?',)####(\"No, `lm()` stands for a linear regression. Your model includes three parameters (minus the intercept) for `b`, `c`, and their interaction `b:c`, which stands for `b + c + b:c` or `b*c` for short (R follows Wilkinson's notation for statistical models). Fitting a Generalized Linear Model (i.e., where the link function is not identity, as is the case for the linear model expressed above) is requested through `glm()`.\",)####('@Thomas The different answers are $p=ar+b$ and $p=(r-b\\')/a\\'$ from the two regressions.  The slopes *always* differ whenever the correlation is not 1 or -1: that\\'s just another manifestation of \"regression to the mean.\"',)####('I am tempted to say that this strange peak is the more related to poor binning algorithm of histogram than to the features of data. Besides that since no context is given there is a lot of possible answers, but writing them up would not be interesting.',)####('If X\\'s (independent variables) are measured variables, then it is a \"random variable\" instead of a \"fixed value\". According to Wikipedia (http://en.wikipedia.org/wiki/Random_variable):\\n\"In probability and statistics, a random variable or stochastic variable is, roughly speaking, a variable whose value results from a measurement on some type of random process.\" Am I correct?',)####(\"Please don't use replies to ask questions.  You could continue this conversation in comments.  Another option is to re-post this as a question and link back here.\",)####('I say exponentially because I want to control the weight of old samples just like using weighted moving average. But I want the old samples to be less weighted, exponentially less, than newer samples',)####('@mpiktas: actually, the binning algorithm is not guilty here. Wolfgang was right, there was a strong bias in one category of measurements. Problem solved',)####('cheers, you got lucky :)',)####('Hi,\\nYou might try to fit a \"nonlinear\" model. Unfortunately, I can\\'t see the graphic of your variable but I do believe that nonlinear regression will be the best choice for your question. Sincerely,',)####('Did you not answer your own question?',)####(\"@Andy: How come? I don't think so, I mentioned that all of the linear models I know are a very bad fit, and that I am wondering if there is another transformation that can be performed on variables to find a better linear fit.\",)####('A cubic looking curve would suggest a cubic type transformation! Try fitting an OLS model with higher order polynomial X terms (specifically X^3). There are other potential solutions as well (splines, breaking the X variable into different categories and using dummy variables). A recent post also details exploratory data analysis in examining such relationships, http://stats.stackexchange.com/questions/10363/data-mining-how-should-i-go-about-finding-the-functional-form/10520#10520',)####('What do you mean by \"direction\"? Have you read the answers to similar questions http://stats.stackexchange.com/search?q=causal ? The short answer is no!',)####('Neither of your suggestions imply causation (or direction).',)####('I think the OP meant \"direction\" in the sense of positive vs negative correlation, not the direction of any causal relationship between X and Y.',)####('It might be worth remarking that \"rises to some point and then falls to a steady level without a consequent rise\" is distinctly *non-cubic* behavior.  Cubics don\\'t have horizontal asymptotes.  More (quantitative) details would be helpful.',)####(\"I think it's reasonable - I do it fairly frequently. Don't know of a name for it\",)####('In the search for outliers, you should first regress your dependent variable against both $x_{1}$ and $x_{2}$ and look for outliers.',)####('What exactly are you trying to accomplish? This strikes me as a little odd and, in particular, (highly) **prone to misinterpretation**. How would you appropriately label your axes of the last plot to make it easier to interpret?',)####('@cardinal, what sort of misinterpretation do you mean? As for y-axis labels, the 3rd graph to me is \"log(price) detrended\" while the 2nd graph is simply \"residuals\".',)####('@mbq, yes, fair point.',)####('Thanks. I would like to see how this can be reframed as a Kalman filter.',)####(\"I doubt there is a formal derivation, hence the quotes around poor man's version of adaptive parameters.\",)####(\"@TMOD, let's say, on your last plot, there is a point corresponding to $(-0.4, 3.3)$. Using just the plot, how would one interpret the meaning of that point? (The question is a bit rhetorical, with the point being to think about the information that the plot is, and is not, conveying.)\",)####('A side note -- if you plot a log-log graph, it is better to use [log axes](http://had.co.nz/ggplot2/coord_trans.html). Otherwise you suggest a linear relation between price and weight, which is in fact exponential.',)####('Just the other day someone somewhere on one of the related StackExchange sites was commenting on this scheme as \"poor man\\'s Kalman filter\".  If I manage to unearth the link I will add it here.',)####(\"It isn't clear to me what you mean by outliers. If there are outliers in your data then they will affect the calculation of the regression line.  Why are you looking for outliers in both $x_1$ and $x_2$ simultaneously?\",)####('@schenectady Use $$ for LaTeX in comments, please.',)####('Is finding outliers the pupose of your investigation? If so, then you should first regress your dependent variable against both $x_{1}$ and $x_{2}$ and then perform outlier tests. If finding possible causation, then you should consider performing a designed experiment. If the purpose of your experiment is to find a relationship between your two independent variables, looking at a happenstance of collected data will not do the trick.',)####('It sounds like you want a \"prediction interval\".  The answer (with formulas) is available on this site [in several places](http://stats.stackexchange.com/search?q=%22prediction+interval%22).',)####(\"Please post a link to the math.SE question. Usually it's not good to cross-post unless significant time has elapsed.\",)####('Can the same number be selected twice? Is n smaller than or greater than U?',)####('Here is the previous question on math.SE: http://math.stackexchange.com/questions/32569/a-question-about-linear-regression',)####('@Nick Sabbe The selection is without replacement. n is always smaller than U.',)####(\"The problem can be stated more simply: let $\\\\\\\\{(x_i,y_i)\\\\\\\\} = (1,y_1), \\\\\\\\ldots, (n,y_n)$ be the data with the $y_i$ integers, $1 \\\\\\\\le y_1 \\\\\\\\lt y_2 \\\\\\\\cdots \\\\\\\\lt y_n \\\\\\\\le U$.  Finding $\\\\\\\\mathbb{E}[r_{xy}]$ is hopeless.  There's some chance of attacking $\\\\\\\\mathbb{E}[r^2_{xy}]$ but it's not by any means easy (it's a ratio of two quadratic forms in the $y_i$).\",)####('@whuber Thank you for your comments. If $E[r_{xy}]$ cannot be calculated, an estimation or lower bound is ok.',)####(\"@Fan That's a good comment: by allowing approximation or bounding, you are opening up the question to many more solution methods.  Consider making this point explicit in the question itself.  If you can, please indicate the ranges of values of $U$ and $n$ you're interested in.  For instance, would you be interested in asymptotics as $U \\\\\\\\to \\\\\\\\infty$?\",)####('@whuber I think if $U\\\\\\\\rightarrow\\\\\\\\inf$, the problem can be seen as selecting with replacement? This would be also my interest.',)####('@Fan for $U \\\\\\\\sim \\\\\\\\infty$, then after rescaling by $1/U$ you can view the $y_i$ as the order statistics for a sample of $n$ iid variables from a uniform distribution on $[0,1]$, thereby replacing the discrete sums in the original problem with integrals.',)####('@whuber I see. Do you have some relevant materials about your approach?',)####('@Fan Applicable techniques would include quadratic forms in random variables (http://stats.stackexchange.com/questions/9220), the \"delta method\" for estimating moments of functions of random variables; distributions of order statistics for uniform variables; the relationship between gaps between uniform variables and the exponential distribution, and possibly even saddlepoint methods, normal approximations, Central Limit Theorem, etc.',)####('@whuber Thank you very much, the information helps me much!!',)####('Another option that might keep the client happy would be to use the R plugin in SPSS.',)####('thx Dmitrij, just looked briefly into BIC but none of the others. Any good pointers?',)####('I know it is not a real answer, but I would strongly recommend to use `R` for this purpose. To my knowledge SPSS is just not made for \"reusing\" results. However, you could just get the regression coefficients and simply use the `COMPUTE` command to compute the predicted values using the general forumal `y = a + bx`',)####(\"If I could use R for this I would, however the client runs an SPSS house and is not keen to use R. The COMPUTE option would work if we were just after the mean estimate, but I'd like to get the prediction intervals out as well.\",)####('how you determine best lags? From my personal practice omitting some *bad periods of time* by lags my produce better fits, but the model is not passing the _laugh test_. So what is the origin of your data? And yes you can do subset searches, a lot of different data mining techniques are available.',)####(\"That's exactly my question to more experienced users: Are there better / other determinants than just an improved fit? Obviously I should use a serious model, but still I wonder whether there is some good strategy to follow in order to get a good first guess up to which order lags should be relevant. Data is e.g. quarterly trend deviation from some stock index.\",)####('@CantTell Please register your account and accept some answers.',)####('have you tried any information criteria like AC/BIC/HQ to optimize the lag choices? Another option could be cross-validation like out-of-sample and jack-knife performance.',)####('I think @Andy understood your problem. What he was getting at was that if two covariates are highly collinear, then one will tend to \"compensate\" for the other. So you can get a negative value for one and positive for another. This makes interpretation of the sign of a particular coefficient estimate a very slippery matter.',)####('Are \"top floor\" and \"corner unit\" correlated with each other?',)####(\"Sorry, I should've said those coefficients have a negative relationship with the dependent variable (price), but a top floor unit is not necessarily a corner unit and vice versa.\",)####('Does model 2 also have the main effects from model 1 in it?',)####('It sounds like you have a sample size of 40 and at least 10 total predictors. Your current model can be interpreted as taking the square footage and multiplying it by a constant to get a \"base\" price. Then adding/subtracting constant dollar amounts based on additional \"features\" of the unit. Increasing your sample size could help matters.',)####(\"Dropping exposure sounds like a questionable thing to do, though it may need to be recoded according to the building the unit is in, perhaps on a subjective basis. For example, in North America, southern exposure is probably generally desirable due to increased sunlight. However, if you're in a large urban area and there's a big park just to the west of your condo, then western exposure may be highly desirable.\",)####(\"@santiagozky I've made some edits to your question to make it clear that you're trying to predict values from a linear regression model. However, you'll need to clarify some issues: (a) are you trying to predict new (unobserved) values or get the fitted values for your actual observations? (b) can you give more information on model fit (R `summary()` output) or a scatterplot of your data (as a rough check for outliers, etc.)?\",)####('`var(fitted(lm_model))` do you need this one?',)####('You should mention what kind of data it is and how it was obtained. That would help us answer your question. Statistical methods like lm depend on certain assumptions, and they simply crunch your input through a formula and spit out \"answers\". If the data does not satisfy lm\\'s assumptions, the answer will be random garbage. So you need to think about the data and the system you are trying to model with a straight line, to see if 55.17 and 19.31 make sense. What would they mean in the real world, and is that plausible? And how did your two columns of data arise?',)####('The answer is yes and the details are provided at http://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model',)####('thanks for suggestions, I added more info and corrected the regression, which was from another case I have. @whuber thanks. I thought this was asked before but I couldnt find it',)####('@santiagozky: The description and graph are very helpful because they show this is a time-to-failure problem, not a regression problem.',)####(\"Excellent update! (Unfortunately, your clarification puts the problem into an area that I'm not familiar with.) Two suggestions, though: 1) Look at the time between errors and read up on the Poisson distribution (http://en.wikipedia.org/wiki/Poisson_distribution), and 2) is anything being done to decrease failures or is the software and process for using the software in a steady state? (Most of the software reliability methods I could find assume that bugs are found and fixed, so the curve you've drawn would become less and less steep over time.)\",)####('What does this expression $\\\\\\\\widehat{var(y)}$ mean?  Suppose the data are $(x_i,y_i)$ and the $y_i$ are random variables with expectations $\\\\\\\\beta x_i$ for some unknown coefficient(s) $\\\\\\\\beta$.  Do you want the variance of the set of $\\\\\\\\beta x_i$ (either assuming the $x_i$ are fixed or under some distributional assumptions for them) or do you want to estimate the variance of the $y_i$?  The latter is larger than the former.',)####('@Dmitrij Post an answer!',)####(\"@cardinal.  Right on the money.  Please post this as an answer -- let's minimize the number of unanswered questions.  :O)  Ps. Up-votes headed your way.\",)####('@cardinal Good answer!  (Maybe you could post it as a reply?)',)####('Thank you cardinal. By the way, is there a way to force the regression line to have a negative slope?',)####('I have deleted my comment and expanded it slightly into a full answer.',)####('If the fitted line does not have a negative slope, the best you can do is a zero slope, which will pass through the point $(x,y)$, thereby uniquely determining it.',)####('Well, any linear-programming package would work. That leaves you with a lot of options. :)',)####(\"@Cardinal How would you recast this as a linear program?  It's not evident how to do it even in trivial cases (such as two data points and one parameter): there are no constraints and the objective function is nonlinear.\",)####('**Key phrase**: Chebyshev approximation. (More to follow. The idea is to *introduce* an extra variable then turn objective into the constraints.)',)####('@cardinal You mean this one :http://mathworld.wolfram.com/ChebyshevApproximationFormula.html It seems quite complicated.',)####(\"Well, it's a bit related, but not germane to this problem. Your problem can be solved with a simple LP. As soon as I can get to a computer, I'll post an answer.\",)####('@cardinal Thank you very much.',)####(\"@whuber : On second look, this isn't really a duplicate actually. The solution for this problem is not evident for the other. And the solution of the other question is far too complicated for this one, as shown in my edited answer.\",)####('@Joris The functional form `S ~ b0 + (A > T) * b1 * (A - T)` is equivalent to `S ~ b0 + b1*max(0,A-T)`.  Changing notation to $x$ = A, $\\\\\\\\delta$ = T, $\\\\\\\\beta_0$ = b0, $\\\\\\\\beta_2$ = b2, and setting $\\\\\\\\beta_1=0$ gives a special case of the problem solved in the duplicate post.  If you have improvements to suggest for the solution there, then it would be best also to post your reply there so we can keep this common thread together.',)####(\"I would just like to add a slope can be very significant (p-value<.001) even though the relationship is weak, especially with a large sample size.  This was hinted at in most of the answers as that the slope (even if it's significant) says nothing about the strength of the relationship.\",)####('A derivation is in the Wikipedia article: http://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares',)####('The [Durbin-Watson statistic](http://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) is a test for serial correlation: that is, to see whether *adjacent error terms* are mutually correlated.  It says nothing about the correlation between your X and your Y!  Failing the test is an indication that the slope and p-value should be interpreted with caution.',)####(\"@whuber : off course, my mistake. I'll add an answer there.\",)####(\"Ah, ok. That makes a little more sense than whether the two variables themselves are correlated...after all, I thought that's what I was trying to find using the regression. And that failing the test indicates I should be cautious interpreting the slope and p-value makes even more sense in this case! Thanks @whuber!\",)####('dollar signs make you enter an equation environment and is why things are randomly getting italicized.',)####('formatting: to get `code`, indent by 4 spaces.',)####(\"if you use the 4 spaces indenting, you can put stars and dollar signs in there and they will show up as such. if you use them outside the code formatting, they will be treated like markup. If you don't want a complete `code` line, use backticks: `this is code with a $ and *`\",)####('It is good that you have used a reproducible example. You could make the example even better by including `set.seed(1)` (or whatever number you like) before running random number generation, so that everybody gets exactly the same results as you (not that it matters much in this case, though).',)####('Thanks for your suggestions, wolf.rauch.',)####(\"Wolfgang: I am aware of these points, but it never hurts to point them out anyway, as they are important! :) [also in case others who read this thread weren't aware]\",)####('Just two small comments. You wrote \"In order to find the estimate of \\'lifespan\\' when the value of \\'weight\\' is 1, I add (Intercept)+height=63.64319\". Note that this is the estimated average lifespan when weight is = 1 **and height = 0**. That is probably not very meaningful. Also, predictions outside of the range of the observed values of the independent variables need to be treated with caution (weight is between 50 and 120, so weight = 1 is also not very meaningful). Just some side-notes and probably things you already knew. But just in case ...',)####('How do you measure accuracy of your current model? In general this question is unanswerable without knowing details about what you are trying to model. If the true model is linear, then linear regression will be the most accurate, for appropriate definition of what is accurate.',)####(\"@cardinal, say I was looking for the most affordable diamonds regardless of size (number of carats). i.e. trying to find the best deal for cut and clarity. You're right that the last plot doesn't help much, but it's just confirming that the data is detrended for carats. I'd then do `d$lprice_detrended_carat <- mean(d$lprice) + resid(detrend)` and start looking at that. Maybe this doesn't make the most sense with logs, but the data I'm _actually_ applying this to is not log transformed.\",)####('@mpiktas Those vectors (x, y, z, v) are stocks. I mean... historical prices of 4 differents stocks. Example 4 stocks on nasdaq, 4 stocks on nyse etc etc.',)####('I\\'m a bit skeptical when I hear \"stock prices\" and \"accuracy\" in the same sentence. Stock price prediction is the deep, dark morass of statistics, and I\\'m not sure that anything in the market can be considered linear.',)####(\"@Dail Your accounts were merged once again (Thanks @Gavin!). Please don't create new accounts each time you ask a question, but use this registered one instead.\",)####('Note that if you have `n` distinct points, you can fit a polynomial of degree at most `n-1` to fit the data exactly.  However, in practice this is a *terrible* idea that will lead to overfitting.  You are probably concerned with accuracy on out of sample (unseen) data.',)####('@Dail Please focus more at reading answers than asking the same question again and again. Total least squares is not a magic wand that will make linear regression a universal, robust model -- you should first try to identify *why* lm is not satisfying (outliers? nonlinear dependence? leverage?) and then try to select appropriate solution.',)####('Yes, it has the two main effects + the two interactions.',)####('because of your variable type (0,1), you should use sth like ridge regression. plenty of text are around ;-)',)####('Binary variables are neither necessary nor sufficient to suggest ridge regression.',)####(\"If the variables aren't all on the same scale, then the $\\\\\\\\beta$'s will be difficult to represent all on one plot. If the variables are all standardized, for example, you could make a plot with side-by-side dots for the point estimate and error bars around the bars - the numerical values on the $y$-axis and the variable names on the $x$-axis\",)####('(1) You might find [R-squared: useful or dangerous?](http://stats.stackexchange.com/q/13314) informative. (2) Concerning the phrase \"significantly influence,\" you should also read some of the threads found by searching this site for \"causality,\" including [Statistics and causal inference?](http://stats.stackexchange.com/q/2245).',)####(\"It's possible, also are you sure the coefficient means a decrease of 230% or 2.3%?\",)####('im quite sure that the coefficient means a decrease of 230%, as I am using a log transformed dependent variable and there is no continuous independent variable - but of course, I am not certain.',)####('@Andy W I thought if I switch the reference with the included binary it will probably turn around the result, to a positive number, showing the increase between the \"before reference\" to the \"before independent\". however I am not quite sure if its not too easy just to interpret the coefficient as an percental change, I should probably look into the formulas again - any ideas, whether this interpreation makes sense?',)####('This is not an answer, but do note that the $t$-distribution approaches the normal distribution as the degrees-of-freedom parameter $\\\\\\\\nu$ grows larger. Past $\\\\\\\\nu \\\\\\\\geq 30$, there is no appreciable difference, particularly in most hypothesis-testing frameworks. The limiting behavior is \"from above\" in the sense that the if $T \\\\\\\\sim t_{\\\\\\\\nu}$ and $Z \\\\\\\\sim \\\\\\\\mathcal{N}(0,1)$, then $|T|$ is [stochastically larger](http://en.wikipedia.org/wiki/Stochastic_ordering) than $|Z|$.',)####('@whuber: (+1) this question originated on math.SE, and I pointed the OP to the same question you have linked to. I think that prompted the deletion of the question over there and the move here.',)####(\"@Andy W I wasn't trying to suggest your interpretation was incorrect.  Your comment suggested a way of thinking about the question that goes beyond technical assumptions, perhaps pointing towards what may be needed for valid interpretation of regression results.  It wouldn't be necessary to write a treatise in response, but even a list of some of those broader issues could be illuminating and might expand the scope and interest of this thread.\",)####('[Here](http://stats.stackexchange.com/questions/12546/software-package-to-solve-l-infinity-norm-linear-regression) is a similar question and answer. (Forgive the self-citation, please.)',)####(\"I'm confused by your edit. Did you mean to put the $\\\\\\\\max$ function inside the sum? Also, in the original equation, are you wanting to allow for an intercept or not? Otherwise, by forcing the line through the origin, the answer is trivial.\",)####('No, the max function is supposed to be outside the sum. $P$ is the dependent variable, $h_i$ are the independent variables. My combined (hedging + original) portfolio starts with value $ \\\\\\\\sum_{i=1}^n \\\\\\\\alpha_i h_i(0) + P(0)$ and I want to choose $\\\\\\\\alpha_i$ so that at the end of the period $[0,t]$ the negative part of the change in value of the portfolio is minimised.\\n\\nI have just realised that this is trivial, however, if there are no constraints on $\\\\\\\\alpha_i$ which, realistically, there would be.',)####('In fact, I would also be interested in minimising the \"end-of-day\" value of that quantity over a period of time i.e. minimising.\\n\\n$ \\\\\\\\sum_{t=0}^T max\\\\\\\\{\\\\\\\\sum_{i=1}^n \\\\\\\\alpha_i [h_i(0)-h_i(t)] + [P(0)-P(t)], 0\\\\\\\\}$',)####('Thanks for the clarification. That helps.',)####('Normally one uses $t$ for time, which would be the \"x-variable,\" so I\\'m wondering about the connection between your question and the equation you have provided.  Where in that equation is the one month return?',)####('@whuber Oops!  Corrected.',)####(\"It is a standard result of M-estimation theory. The basic idea comes from taylor approximation of the minimizing function. You can look at the derivation in [Wooldridge's book](http://books.google.com/books?id=cdBPOJUP4VsC&lpg=PP1&dq=cross%20sectional%20and%20panel%20data%20wooldridge&hl=fr&pg=PA350#v=onepage&q&f=false), or check out van der Vaart's [Asymptotic Statistics](http://books.google.com/books?id=UEuQEM5RjWgC&lpg=PR1&dq=van%20der%20vaart%20asymptotic%20statistics&hl=fr&pg=PA51#v=onepage&q&f=false).\",)####('Some ideas can be gleaned from this [question](http://stats.stackexchange.com/questions/7308/can-the-empirical-hessian-of-an-m-estimator-be-indefinite) too.',)####('Do you mean how to check if the residuals of a linear regression have a constant variance? The residuals from a regression will of course per definition be random...',)####(\"The residuals can't be constant.  The math of a linear regression is such that some must be positive and some negative.  Perhaps you mean something else.\",)####('To learn about tests for constant variance, you can look up the terms homo- and hetero-skedasticity on this site.  Maybe you had in mind a constant mean.  As in, residuals should not be generally high for some values of predicted Y but generally low for others.  You could test this by dividing your predicted Y into discrete groups and running an anova using the residuals as the dependent variable.',)####('Closing, because this question is either empty (interpreted as written) or it covers previous ground.',)####('Regarding degrees of freedom, look at [this CV Q&A](http://stats.stackexchange.com/questions/884/what-are-degrees-of-freedom) or [this nice explanation](http://www.jerrydallal.com/LHSP/dof.htm) referred to in one of the answers.',)####('Re-write your variance as $\\\\\\\\hat{\\\\\\\\text{var}}[\\\\\\\\hat{\\\\\\\\beta}]=[(X^\u2032X)^{\u22121}/(N-p-1)]\\\\\\\\sum_i(e_i^2)=\\\\\\\\hat{\\\\\\\\sigma}^2(X^\u2032X)^{\u22121}$.  So it seems like your question is more related to why do we estimate $\\\\\\\\sigma^2$ as $\\\\\\\\frac{1}{N-p-1}\\\\\\\\sum_ie_i^2$?',)####('You can find a rather complete list in William Berry\\'s little book on \"Understanding Regression Assumptions\": http://books.google.com/books/about/Understanding_regression_assumptions.html?id=4_Aeo9JdzCMC',)####('@whuber, if $EY_i=X_i\\\\\\\\beta$ this means that the means are different for different $i$, hence $Y_i$ cannot be iid :)',)####('What is the motivation for this question?',)####('Off the cuff, it seems the solution must be a line corresponding to one of the segments of the *least concave majorant* of the data.',)####(\"Thanks.  I'm still puzzled, because now the notation suggests you have a sequence $(r_t)$ = $r_1, r_2, \\\\\\\\ldots, r_n$.  With such a situation it's simply impossible that every pair $(r_t, r_{t+1})$ consists of a (one day, one month) return (except in the trivial case $n=2$), because the case $t=1$, where $r_2$ is the y-variable, implies $r_2$ is a one-day return, but then $(r_2, r_3)$ has a *one-day* return for its x-variable, not a one-month return as stated.\",)####('In particular, I read that the degrees of freedom are N-p-1 where p is the number of parameters, how do we get this?',)####('While respondents have listed some good resources, it is a difficult question to answer in this format, and (many) books have been devoted solely to this topic. There is no cook book, nor should there be given the potential variety of situations that linear regression could encompass.',)####('Technically, (ordinary) linear regression is a model of the form $\\\\\\\\mathbb{E}[Y_i] = \\\\\\\\mathbf{X}_i \\\\\\\\beta$, $Y_i$ iid.  That simple mathematical statement encompasses *all* the assumptions.  This leads me to think, @Andy W, that you may be interpreting the question more broadly, perhaps in the sense of the art and practice of regression.  Your further thoughts about this might be useful here.',)####('I assumed (perhaps wrongly) that the assumptions the OP talks about are in regards to making *valid inferences* based on the $\\\\\\\\beta$ estimates, which require greater constraints than those that simply allow the $\\\\\\\\beta$ to be identifiable (as mentioned by @whuber). It would require clarification from tony though as to whether my assumption is correct (and if it is my first comment still stands, in that it is so broad it would be difficult (but not impossible) to write an answer with a scope that wide).',)####(\"@mp You're right; I should have more carefully stated that $Y_i - \\\\\\\\mathbf{X}_i\\\\\\\\beta$ are iid.\",)####(\"What group to set reference doesn't matter for significance test for the whole model. But it does matter for the tests of individul predictors (the other groups).\",)####(\"Is your interest in the data or in writing the program?  If it's the first, then forget about Python: use a statistics package and learn how to do linear regression, because it's far more than just a formula: you need capabilities of data manipulation, re-expression, and visualization; ways to obtain regression statistics and residuals; computations of regression diagnostics; and more ways to visualize and check the results.\",)####(\"Ultimately it's in the data but my understanding was that for things like linear regression one could either use R or use python with NumPy or SciPy to do similar things.  As a programmer I'd prefer python although if it's considerably harder in python then I'm open to using R (I've already used R some).\",)####(\"R is immediate and interactive: for this kind of work it's really commanding, not programming.  Sure you can do the job in Python (and even Excel for that matter), but the important thing here is for you to learn about regression: the program/command problem is trivial in comparison, but no amount of programming will overcome lack of understanding of how to use the software properly.\",)####('Fair enough.  I probably will learn to do this in R first and then figure out how to translate that into python.  I did find this R tutorial which seems to cover what I want: http://www.gardenersown.co.uk/Education/Lectures/R/regression.htm',)####(\"With regard to visualization I'm assuming you mean plotting.  In the case of multiple variables, is the idea that you plot each independent variable against the dependent variable one at a time?\",)####('@ttphns: Then why am I getting different p-values  for the F test depending on the reference group?',)####('Do you include any interactions?',)####('@whuber: No I am just testing a dependent variable on 1 independent variable with different levels.',)####('It might help to give details of the \"linear regression.\"  Important aspects include whether you include a constant and how you encode the groups.  For your second question, investigate [power](http://stats.stackexchange.com/questions/tagged/power) related threads.',)####('@whuber: I am using 1 and 0 as the coding variables. The intercept will be the reference group.',)####('Are you saying your model is $y =\\\\\\\\beta x +\\\\\\\\varepsilon$ where $x$ takes on only the values $0$ or $1$ (and $\\\\\\\\varepsilon$ is a random error)?',)####('@whuber: yes. thats right. So the intercept is the reference group. But when I change the reference group the F values changes. So what should I use as the reference group?',)####('Some of the responses to this related question, [What is a complete list of the usual assumptions for linear regression?](http://stats.stackexchange.com/q/16381/930), are not limited to simple linear regression. Does that help?',)####(\"It's number two.\",)####('@Fojtasek haha, yeah. I just figured it out and answered my own question. I think the actual act of writing down step by step all the issues I was having is what allowed me to realize the answer XD',)####(\"Ad 1) In linear regression, I normally understand ANOVA as a measure of goodness of fit of the model, i.e. to decide whether the model (regression line) explains substantial part of total variability. The question, whether it is equivalent to all slopes being zero, is really very interesting. Ad 2) it looks I'm getting almost the same p-values for t-test and regression ANOVA in this case. Really interesting theorem!\",)####('\"Are there hard-and-fast rules for how much information to report\" - it really depends on what you want to do after the regression. One might be happy with just the correlation coefficient; one might need the Durbin-Watson value on top of that, and still another one might want to see the diagonal of the hat matrix... it really depends.',)####(\"That the residuals from the *good* part of the data are approximately symmetric, other than that you're safe! PS: you might wanna link to a more recent article, there as been gigantic leaps in computational tractability in this filed in the last 10 years (specially in higher dimensions)....\",)####('Some organizations do have rules.  See the [APA guidelines](http://my.ilstu.edu/~jhkahn/apastats.html) for instance.',)####('These bands are automatically produced by `LinearModelFit`: see the help page, especially the introductory section titled \"Properties of predicted values include:\".',)####('Thanks. Will look for a more recent paper to update the question.',)####('you can use s+ or R and use predict function after modeling your data with lm',)####(\"@cardinal The book may be freely available but it would be *nice* on the OP's part to take the effort to make the question self-contained to increase the chances of getting an answer. Speaking for myself, I am less inclined to answer a question that requires me to click a link, go to a specific page in that book, understand the context, navigate back to answer the qn etc.\",)####('Please make the question self-contained. I do not have access to the book.',)####('@varty: I agree. My intent was not to be argumentative, but simply to (kindly) point out that you *did* have access to the book, in case you were interested.',)####('@varty: The book is freely available (legally) at the link provided by the OP. That said, I still agree the question should be made self-contained if for no other reason than that future versions, or even printings, of the text may have different equation numbering.)',)####('This appears to be the same question as http://stats.stackexchange.com/questions/5700/finding-the-change-point-in-data-from-a-piecewise-linear-function.  If it differs in any substantial way, please let us know by editing your question to reflect the differences; otherwise, we will close it as a duplicate.',)####('Sorry this is the eqn I am referring to',)####('$$\\\\\\\\beta|(\\\\\\\\hat{\\\\\\\\beta} -\\\\\\\\beta)^TX^TX(\\\\\\\\hat{\\\\\\\\beta} -\\\\\\\\beta)\\\\\\\\leq \\\\\\\\sigma ^2\\\\\\\\chi ^2$$',)####('here beta_hat are the estimates for beta, and sigma is the variance of the error term. On the right of sigma_square is the chi_square distribution. Please let me know if I wasnt clear.',)####('I think you can do this as a non-linear optimisation problem. Just write the equation of the function to be fitted, with the coefficients and the knot locations as parameters.',)####('I have edited the question.',)####('can you give examples how your \"species x year\" matrices look like?',)####(\"... and clarify what 'distance between years' means?\",)####('Even the ordinary regression fit may be invalid because the $(n^2-n)/2$ points reflect at most $n$ independent observations, causing extreme interdependence.  So, testing significance appears to be the least of the concerns here.  It looks like you need a different approach to the analysis altogether.',)####('@psj: My matrices consist of species (several dozens) in rows and years (25) in columns. For the MCPP I shuffle the column positions randomly (10,000 times).',)####(\"@onestop: Dissimilarity between years were measured by Hellinger distance, that is, Euclidean distance of Hellinger transformed data. Hellinger transformation: N'ij = \u221a(Nij/\u2211Nij) where Nij is the population size of species i in year j, and \u2211Nij is the sum of individuals across all species in year j.\",)####(\"@whuber: I'd be eager to get a suggestion...\",)####('I cannot offer an alternative because I do not know what\\'s in the data (what exactly has been recorded about these \"species\"?) and knowing the purpose of the analysis.  Your research might be better served by reformulating the original question to provide this information.',)####(\"@whuber: It's population size (=number of individuals) of species per year (and they are bird species, by the way). The aim is to check species composition change over time. When there is directional change, then years further apart should show larger Hellinger distances than years closer together.\",)####('Thanks: the problem is getting clearer.  I\\'m still not sure what exactly you mean by \"check\" the composition change over time and why that requires making all possible comparisons between species.  Could you perhaps share a high-level description of your scientific objective for this analysis? Also, there is a preliminary issue concerning what a \"population\" is.  I doubt you truly know the populations: these must either be estimates or summaries of observations, but which are they precisely?  That will influence the interpretation of any analyses.',)####('Presumably, in the second formula, $\\\\\\\\le$ is a typo for $\\\\\\\\sim$ and $p+1$ is the number of parameters (including the constant), right?',)####('Yes, and Yes. Can you make sense of it though, how does the second formula come?',)####('The left hand side in (1) is a sum of squares of $p+1$ normal variables.  By definition, a chi-squared distribution describes a sum of squares of $p+1$ standard normal variables.',)####('How do you get that? How can we take the XTX matrix out from the variance term?',)####('Factor it: because it is symmetric positive-definite, you can write it as $X^TX = UU^T$ for an invertible $p+1$ by $p+1$ matrix $U$.',)####('Wait, I am grossly missing something here, the matrix encodes the correlation between the different betas right, now if we take the matrix out we are left with a variance term multilplying the Identity matrix, and a new scaled variable on the left which is $$(X^TX)^(-1/2)(\\\\\\\\betahat-\\\\\\\\beta)$$ this variable has no corrleation structure? Since the variance covariance matrix just becomes sigma^2*I',)####('We are assuming that the variable I wrote above is normaly with mean 0 and a variance conariance matrix of sigma^2*I. My question is  that how can this be done, where do the cross correlation term go?',)####(\"You would benefit greatly from working an actual problem.  Why don't you fit a line through the points $(0,2),(3,4),(4,8),(7,10)$.  (I chose this for the easy arithmetic.)  You should compute $U = \\\\\\\\{\\\\\\\\{2,0\\\\\\\\},\\\\\\\\{7,5\\\\\\\\}\\\\\\\\}$.  Then you might see where the correlation appears.\",)####(\"@whuber, True, if you undo the transformation and try to interpret that result, it may not have a normal distribution; I forgot about that part of the question. But I'd recommend bootstrapping that distribution, rather than forcing yourself to find a model that has errors that are normally distributed.\",)####('You may wish to check out this similar question on CV: http://stats.stackexchange.com/questions/10079/rules-of-thumb-for-minimum-sample-size-for-multiple-regression/12058#12058',)####(\"The coefficients would be easier to interpret if you used the natural log. Also, you should worry about the linearity assumption seeming to make sense. You don't need normality of anything (variables or error terms) so long as you have enough data---the central limit theorem typically comes to your rescue.\",)####('Actually, @Charlie, you do need normality of some statistics if you want the t-tests and confidence intervals to be correct.  The log transformation is strong enough to raise doubts unless the dataset is quite large: the approximate normality of an estimated coefficient of a log response will imply the *non* -normality of the coefficients for the response itself and *vice versa.*',)####(\"I'm no sure that I understand what you mean by refining. I'm also a little confused of how to interpret $\\\\\\\\frac{A-B}{A+B}$. Perhaps having standard deviation at the bottom would be more intuitive (although with only two stores it probably will be the same)...\",)####('I added a tag for splines, in case anyone wants to list rules of thumb for them.',)####('My \"normal interval workbook\" at http://www.quantdec.com/envstats/software contains macros \"Tol2,\" \"NCTDist,\" and \"NCTInv\" to evaluate the non-central T and illustrates how to compute normal-theory tolerance intervals with it.  See the [K Tables] sheet.',)####(\"Do you really want just to *predict* Y, or do you want to explain the mechanism by which one or more variables *explain* Y?  If the former, then there's no reason not to use 4).  If the latter, you'll have some work to do to make it clear to your audience just how 4) is helpful.  It's not always the case that the most useful solution is the one with the highest r-squared.\",)####('I want to get the best r-squared. All four cases are sound, but I want to understand if the highest correlated variables produce the highest r-squared in a multiple regression, or if, more in general, analyzing each independent variable correlation to Y is a good way to prepare the best fit model.',)####('Also I am aware that there are ways to select/discard the independent based on anova and the t-test, after the regression is run. I would like to understand if the general idea of increasing the correlation against Y like in my example is a sound method to increase r-squared.',)####('This is a broad, diffuse set of questions.  You would be able to focus them better, and learn more from us, by first learning a little about multiple regression.  A Web search will turn up plenty of sites where you can start.',)####('I agree that this is too broad to be answered at the moment. Unfortunately, the CPAN page makes it clear that there will be no support and that users are expected to be familiar with OLS estimation. Take a look at the section related to multiple regression in [The Little Handbook of Statistical Practice](http://www.tufts.edu/~gdallal/LHSP.HTM), and if you are still in trouble with a particular aspect try to narrow down the above questions.',)####('Robert, R-squared is meaningless when you are re-expressing variables.  For some discussion of R-squared, see http://stats.stackexchange.com/questions/13314/r-squared-useful-or-dangerous/13317#13317.  You should prefer to obtain a model which has a good fit overall and acceptably small residuals.',)####('thank you - please see my edit above.',)####(\"I don't see a problem with the unlimited support of Poisson distributions: it's similar to using Normal distributions to model, say, values that must be nonnegative.  Provided the chances associated with impossible values are tiny, it can be a good model nevertheless.  Negative binomial is the standard alternative to Poisson used to test goodness of fit and overdispersion; it's a good idea.  If SPSS is too limited, use something else!  (`R` has packages for zero-inflated models; [search this site](http://stats.stackexchange.com/search?q=%2Bzero+%2Binflation+%2Bpoisson).)\",)####(\"Have you thought of using Poisson regression instead?  It's naturally indicated with dependent count data and your success with a log transformation is consistent with Poisson distributions.  The coefficients would be interpreted in terms of proportional increases in expected probability of missing a day of school.  One advantage is that no special treatment of zeros is needed (although it's still a very good idea to look at a zero-inflated alternative model).\",)####('That doesn\\'t seem to me to be a sufficiently well-defined problem; for example, if you add one feature, but it\\'s highly informative w.r.t. the residuals of the \"current\" model, the std. errors on the existing coefficients will go down, perhaps substantially, even with no increase in sample size; if the additional feature is sufficiently close to orthogonal to the residuals, it may increase the std. errors of the existing coefficients.',)####(\"I agree with @whuber  I think you probably want a ZIP or ZINB model. I'd just add that they are also available in SAS via PROC COUNTREG (in ETS) and, starting with SAS 9.2, in PROC GENMOD (in STAT)\",)####(\"Hi Whuber, Yes, I was thinking about Poisson regression but wasn't sure about this or opting for negative binomial regression. I guess negative binomial as the data is over dispersed - i.e. the mean is lower than the variance in the dataset (hence positive skew). Also, strictly, there is an upper limit on the number of school session in the year, whereas Poisson assumes an unlimited denominator? Or do you still think Poisson is more appropriate? Unfortunately SPSS doesn't support zero inflated models as far as I've seen...) Thanks Whuber :)\",)####(\"Hi Whuber, Peter, thanks both for your input. Unfortunately I only have SPSS and can't acquire SAS due to budgets, though obviously I could look at R. In the absence of a zero-inflated model, do you think Poisson/NB regression are suitable for dealing with zeroes - if not, is it reasonable to 'just add 1' as a constant to all values? Whuber, would it be possible to elaborate on 'The coefficients would be interpreted in terms of proportional increases in expected probability of missing a day of school' please? Thanks both :)\",)####(\"That's not true in general. Perhaps you're just seeing that in your data. Paste this code: y = rnorm(10);\\nx = rnorm(10);\\nlm(y~x);\\nlm(x~y); into R several times and you'll find it goes both ways.\",)####('that looks great, I will try it. Thanks, I am not sure how to upvote your answer here?',)####('That\\'s a bit different from what I was describing. In your example y wasn\\'t a function of x at all, so there\\'s not really any \"slope\" (the \\'a\\' in my example).',)####('lm(y~x) fits the model $y = \\\\\\\\beta_{0} + \\\\\\\\beta_{1}x + \\\\\\\\varepsilon$ by least squares (equivalent to ML estimation when the errors are iid normal). There is a slope.',)####('Your question is asked and answered (sort of) at http://stats.stackexchange.com/questions/13126 and http://stats.stackexchange.com/questions/18434.  However, I believe nobody has yet contributed a simple, clear explanation of the relationships between (a) regression of $Y$ vs $X$, (b) regression of $X$ vs $Y$, (c) analysis of the correlation of $X$ and $Y$, (d) errors-in-variables regression of $X$ and $Y$, and (e) fitting a bivariate Normal distribution to $(X,Y)$.  This would be a good place for such an exposition :-).',)####(\"Use this modified example: y = rnorm(10);\\nx = .5 + .1*y + rnorm(10);\\nlm(y~x);\\nlm(x~y); so there is a relationship. The phenomena you're describing still doesn't happen every time.\",)####('Of course Macro is correct: because x and y play equivalent roles in the question, which slope is more extreme is a matter of chance.  However, geometry suggests (incorrectly) that when we reverse x and y in the regression, we should get the *recipocal* of the original slope.  That never happens except when x and y are linearly dependent.  This question can be interpreted as asking why.',)####(\"In the modified example, it's not really clear to me why y is sampled from rnorm; for simplicity let's just say it's 1:10 (or better yet, 1:100 so that there are more data points). In that scenario, when I've used a sufficiently large number of observations, I've gotten a steeper slope each time.\",)####(\"Of course, you're right. The slopes should be at the group level, not at the individual level; fixed.\",)####('Is there a question in here somewhere that I missed?',)####(\"A well-formatted and thoughtful question is more likely to draw the attention of other users. Please, work on the quality of your post (there's some hint on our [FAQ](http://stats.stackexchange.com/faq) and the [editing help guide](http://stats.stackexchange.com/editing-help)), and I'll second @Dason's comment: What's the question? In the absence of action on your part, we'll have to close this post as *non-constructive*.\",)####(\"You're going to have a problem with that particular formulation; by setting `beta1[i] <- y[i] / x1[i]` and `beta0 <- 0`, the errors will all be zero.  You've got $N+1$ parameters and only $N$ observations...\",)####(\"Isn't lwr and upr the confidence interval? For each of your new ind values, 3,5,0.25 respectively?\",)####('What is the distinction between \"beta1[group[i]] is drawn from a distribution with zero mean\" and \"all the beta1[] are drawn from a distribution the mean of which is fixed at zero\"?  If all the beta1[] are drawn from a distribution the mean of which is fixed at zero, then necessarily each beta1[group[i]] is drawn from a distribution the mean of which is fixed at zero...  but the posterior mean for each beta1[group[i]] won\\'t be zero unless a miracle occurs, if that\\'s your concern.',)####(\"In the former case, I'm expressing my prior belief that the beta1[] are drawn from a distribution with zero mean. In the latter case, I want to constrain the posterior means of each beta1[j] such that if I take the mean of these posterior means, I'll get zero.\",)####('That clarifies things, thank you very much.',)####(\"The answer to this depends highly upon your goals and requirements: are you looking for simple association, or are you aiming for prediction; how high are you on interpretability; do you have any information on the variables from other publications that could influence the process; how about interactions or tranformed versions of the variables: can you include those; etc. You need to specify more details on what you're trying to do to get a good answer.\",)####('@Spacedman - well not exactly...I need the CI for ind not for the height ...because I am calculating the ind (the X) given a new value of the height. And I need the CI for the new X=(Y-beta0)/beta1',)####('@Spacedman- is it that value X +/- the standard error at 95 %CI ?',)####('If you have errors in your design variables, ordinary least squares is not very good. The usual LS estimators are actually biased in this setting. A good way out is to use a [errors-in-variables](http://en.wikipedia.org/wiki/Errors-in-variables_models) model, in which case you also should get the answer to your question for free.',)####('Based on what you asked, this will be for prediction. Influence on other variables just offers possible association. There are no interactions between them. Only one value needs to be transformed, and it has been done.',)####(\"@rolando2: Not a mixture model..and p isnt multiplied by 1..it's just px1 in dimensions...no particular significance im just saying some of the coefficients to be estimated must satisfy linear constraints..in the above notation $p$ just happens to be the number of such linear constriants.\",)####('Pretty much any linear regression theory textbook will provide the derivation and (closed-form) solution for finding the least-squares estimates of the coefficients under linear equality constraints. See, for example, G. A. F. Seber and A. J. Lee, *Linear Regression Analysis*, 2nd. ed., **Sec. 3.8**, pp 59-62.',)####('It\\'s not quite clear what your problem is here.  What are these \"other weights z\"?  Also, it sounds like your issue with the constraints on beta is actually the problem rather than anything to do with the weighting.  So perhaps you should be a bit more specific about what those constraints are.',)####(\"I've edited my question...I basically want to know where I can read about/ learn about regression with these types of constraints.\",)####(\"I'm not the swiftest with mathematical notation...what is p? What is the significance of p being multiplied by 1? And is this a mixture design?\",)####('How big were the training and validation sample sizes? Maybe the model without an intercept was better just by chance.',)####('I understand the task as follows: we are given a linear regression model ($y$ on $x_1$, $x_2$) under G-M assumptions. We can estimate it\\'s parameters through ols. After that we consider the regression of $x_2$ on $y$, $x_1$. It is asked if the reciprocal of $\\\\\\\\hat{\\\\\\\\beta_2}$ (which is an estimator of $\\\\\\\\beta_2$) is an unbiased estimate of the parameter $\\\\\\\\gamma_2$ in the new regression model. By \"why you don\\'t just fit your first equation\" you mean to plug $x_2$ from second equation into the first one?',)####(\"Is this homework?  It's hard to see why this would be a necessary thing to do in an applied situation.\",)####(\"yes, it's my HA\",)####('By \"fit your first equation\" I just meant that if you have values of y, x1 and x2 why do you need to muck around with reciprocals of anything, why not just use OLS to fit whichever of the two equations you want estimates for.  But the exercise looks like one to see the relationship between different estimators, not just a practical one of trying to fit a model.',)####('So can I run a regression of $x_2$ on $y, x_1$ and compute ols estimate of $\\\\\\\\gamma_2$ if I am not sure about G-M assumptions?',)####(\"Can you give a fuller example? This seems like a pretty silly thing to do, so I would like to know more information about the particular situation you're thinking of. Can you add a link to your question to illustrate with a specific example?\",)####(\"Not a stupid question.  I'm not sure though whether you have either: a system of two equations (because y is somehow caused by x1 and x2, and x2 is also caused by x1 and y); or just one equation,about which you don't mind the causality, that you have switched around and want to estimate the second for some reason (what?).  If the former, you have a problem, as you won't be able to fit both equations simultaneously (as they both have the same sets of variables - not possible to separate out the two way causality).  If the latter, it's not clear why you don't just fit your first equation.\",)####('Is there a theory that says what predictors you should include? If you have a lot of variables that you have measured, and no theory, I would recommend holding out a set of observations so you can test your model on data that was not used to create it. It is not correct to test and validate a model on the same data.',)####('For example, the dialysis dose (Kt/V) and protein catabolic rate (PCR) are both calculated from pre-dialysis and post-dialysis blood urea nitrogen levels in hemodialysis and are thus mathematically coupled (The formulae are given in the article \"Mathematical Coupling and the Association Between Kt/V and PCRn. Seminars in Dialysis 1999;12:S20-S28\" http://onlinelibrary.wiley.com/doi/10.1046/j.1525-139X.1999.90204.x/abstract My question is: can PCR be used as a covariate to adjust for the effect of the key predictor variable (such as systolic blood pressure level) on Kt/V?',)####('Jinn-Yuh, this may sound philosophical but I think it gets to part of the issue: what\\'s the difference between \"mathematically coupled\" and not independent?  In many cases we use sets of explanatory variables that have clear lack of independence.  We can remove their correlations with linear transformations (to orthogonalize them).  This effectively expresses the original variables as \"mathematically coupled\" versions of the orthogonal variables.  What\\'s any different about this situation and (dialysis dose, PCR) or (BMI, obesity)?',)####(\"I'm confused with your terminology. Could you give a specific example?\",)####('The training sample size was 289 whereas the validation sample size was 406. By the way, how to determine the best training and validation sample sizes?',)####('For example, BMI=BW/BH^2 and obesity=BW^2/BH are two mathematically-coupled variables. Is obesity a spurious predictor for BMI and should never be used?',)####(\"Obesity is defined as above a specific BMI cut-point, and it's categorical rather than continuous, so I am not familiar with your second formula. Given the definitional association, I would be concerned to see either being used in a regression to predict the other.\",)####('\"Obesity\" as defined above is a continuous (instead of a categorical) variable and is only a (in)convenient term used to illustrate my point. You can call it \"new-BMI\" or any other terms.',)####('data, observations, thank you!',)####('What software are you using?',)####('Can you share your purpose in comparing the fit of the different models? To develop a predictive model? To describe a particular data set? To provide evidence for or against a particular theory? You might choose different comparison approaches depending on your purpose.',)####('It would help if you edited your question to show the first couple of rows of your data, and put up a scatterplot.',)####('I\\'m a bit confused by your notation. Is \"x1, x2...x(n\uff0b1)\" referring to random variables or observations? And what is the significance of \"(n+1)\"?',)####('Anne, at this point it is simply for description.  I would like to quantitatively be able to say that a non-linear fit is preferred.',)####('Presumably you mean \"H0: B1 and B2 are the same\" and you are looking for statistically significant evidence to reject H0.',)####('Don\\'t you have access to x1,...,x5? I do not get this \"x1+...+x5\" representation.',)####('Yes, but I just put them there as placeholders as an example. They are just variables. I have thousands of variables in my database, so writing this code by hand is not possible.',)####(\"@Xi'an Alexander has a character string representing the RHS of a model formula in R. The problem then is how to create a valid R formula from this character string representation.\",)####('+1 From me - clear question with simple example and evidence of effort in trying to solve the problem. Not sure why this was down-voted?',)####('Michelle, I use R.',)####(\"Thanks, I did figure it out eventually. I hadn't realized that $e_i$ ~ $N(0,\\\\\\\\sigma^2)$, but once I noticed that then I could divide it out as you said and then the numerator worked as well.\",)####('Hi Michelle, I\\'ve added what I know of how the score is calculated and tried to clarify the question some. I think the main discussion is if it\\'s OK to have a non-normal variable in a regression as the outcome or if this breaks some fundamental assumption. I\\'m not that familiar with multinomial regression so I\\'m also a little curious of how this impacts the interpretation, it\\'s much easier to have a score being \".2 better in females\" than \"20% more improved in females and 30% more mixed results in females\"',)####('Personally I find it rather difficult to assess normality from a histogram (or a kernel density plot). I would never rely on them as an \"ultimate\" evidence. QQ plots are much more powerful for this purpose.',)####('Sorry if that\\'s unclear - I guess it\\'s more commonly used in Swedish where it\\'s like a taxtable - depending on your answers you\\'re taxed differently. What I mean is simply the way the answers are translated into a \"continuous variable\". I\\'ve changed the question to clarify.',)####(\"The generic term for this is [Errors-in-variables models](http://en.wikipedia.org/wiki/Errors-in-variables_models). The particular case you're considering looks like [Deming regression](http://en.wikipedia.org/wiki/Deming_regression).\",)####('Deming regression is very nice, but as far as I understand it assumes that:Uncertainity rates are the same for each point, and I know there will be different in my setting. \\n\\nAlso there is no mention of how to calculate errors of A i B.',)####('Every correlation matrix will be symmetric because $\\\\\\\\mathrm{cov}\\\\\\\\left(x,y\\\\\\\\right)=\\\\\\\\mathrm{cov}\\\\\\\\left(y,x\\\\\\\\right)$. I encourage you to work out the math to see that this is indeed true. If you know the relationship between $x$ and $y$ (or whatever the variables of interest are) is not symmetric _a priori_, it might benefit you to look in to other methods of analysis.',)####('Interesting points were made on a related question, [Effect of switching response and explanatory variable in simple linear regression](http://stats.stackexchange.com/q/20553/930).',)####('You are getting the conclusions that different individuals often describe their speed differently, but that overall they often notice when their speed changes.',)####('Hi, could you clarify a few things.  Which is your response variable (on the vertical axis) - perceived velocity or actual velocity?  What is the scale of actual velocity (is it the same seven point scale as perceived, or is it a four point scale - one interpretation of what you mean by \"I have 4 velocities\"). What *do* you mean by \"I have 4 velocities\" - is this 4 possible velocities?  In the first regression, do you have 60 points or 15? And in the second are there 120 or 30?',)####('Hi, can you show us the scatterplot of your two velocity variables?',)####('You are right Peter. Thank you!',)####('Why do you want to take derivative???',)####(\"Is this a homework question?  If so, please add the homework tag... otherwise let us know it's not.\",)####('As a hint - what is the distribution of $\\\\\\\\hat{\\\\\\\\beta}_1$?  What is the relationship between that distribution and the $\\\\\\\\chi^2$ distribution (if any)?',)####(\"Not homework, it's from the textbook I'm using to try to teach myself linear regression.\",)####('Well, $\\\\\\\\hat\\\\\\\\beta_1$ ~ $N(0, \\\\\\\\sigma^2/S_{xx})$ since $\\\\\\\\beta_1 = 0$, and I know that the $\\\\\\\\chi^2$ distribution with $k$ degrees of freedom is the sum of the squares of $k$ standard normal random variables... I was thinking that $\\\\\\\\hat\\\\\\\\beta_1^2$ by itself is a chi-squared random variable with one degree of freedom, but it has a non-standard variance. And how does $S_{xx}$ play into everything?',)####('Note that $n\\\\\\\\hat\\\\\\\\sigma^2/\\\\\\\\sigma^2$ is a sum of standard normal random variables, while $n\\\\\\\\hat\\\\\\\\sigma^2$ is a sum of $N(0,\\\\\\\\sigma^2)$-random variables. And $\\\\\\\\hat\\\\\\\\beta_1\\\\\\\\sqrt{S_{xx}}/\\\\\\\\sigma$ is distributed as $N(0,1)$.',)####('Hi Max, how is the score from the test use?. Assuming the possible total score ranges from 5 to 15, are the incremental changes in score important?',)####('I\\'m not familiar with this usage of the word \"tariff\"; I\\'ve tried googling it but all I can find are references to taxation :) I think you just mean the rules/weights by which the score is produced, but if I\\'m wrong, let me know.',)####('How large a data set do you have?  Is your metric calculated on the same data used to fit the model, on a test sample, or using cross-validation?  If the relationship really is close to linear, linear regression is (quite likely) to outperform a random forest...  As for techniques that are better than linear regression, depending on the model and objective function, certainly there are better techniques, e.g., logistic regression for binary outcomes.',)####('It would be nice to know how did you arrive at this question. I cannot imagine real life situation where such type of information about regression model would be available. (For that matter no imaginary life situation is possible too)',)####(\"I am preparing for exam, so this is one from the last year's final.\",)####(\"The full data set is roughly 1MM rows X 50 columns, w/ 40 numeric columns and 10 factor columns(each factor has 3 levels).  I have split that into a training set of 200k and a test set of 100k for easier/faster use.\\n\\nI'm trying to use regression as opposed to classification.\",)####(\"Hello everybody thanks for your reply. On the vertical axis there is the perceived velocity. On the horizontal axis I have the velocities, on a 4 point scale. Just velocity 1, 2 , 3 and 4. It is a general case, I don't care about the differences between the velocities (Please note: this velocity example is just a generalization of the real problem that I am trying to solve...). In the first regression I just have 4 points (the average perceived velocity for the 4 velocities). In the second regression I have 120 points (15 participants * 2 trials * 4 estimate of the velocities).\",)####(\"So, in R parlance you're doing something like `lm (S ~ T, data=X)` where X is the data for one day? And the relationship between S and T changes at irregular intervals because of repair work and also changes throughout each day? The changes that take place during the day are due to what? (That is, the temperature changes during the day, but what else affects it? Precipitation, who-knows-what, rail traffic?) If you know what causes the changes within each day, you may be able to add that to your model. Knowing some more details would help shape an answer.\",)####('Where you might recall having ill conditioned matrices is if you put time {1,2,3,.....,} in as an explanatory variable. {1,0} dummies are OK though.',)####('Could you clarify \"highly skewed along some of the dummy variables\"?',)####(\"I have several discrete indicators  - some of these are for a covariates that I'm adjusting for. eg I have 30,000 observations of X=1, and 1,000 observations for X=0. This is true of predictor I'm making inferences on as well (available observational data is unbalanced).\",)####('Intuitively, if $L$ truly is \"luck\" it shouldn\\'t be able to predict anything!  As such, your hope is that $L(n-1)$ captures deviations (of team record relative to run production) that were *not* due to luck but perhaps to other factors, and that such factors may persist to time $n$.  Therefore your best hope is to see whether $L(n-1)$ is useful for predicting $L(n)$ (rather than $P(n)$).',)####(\"Well, logistic regression is used when the dependent variable is binary so, to use that, you'd need the game-level data, not the winning percentage (which is binary data averaged over the season). Unless the diagnostics really don't check out, I don't see any problem using linear regression where winning percentage is the dependent variable. If they don't, use logistic regression on the game-level data. Either way, the results should be pretty similar.\",)####('@mike - I don\\'t understand what your obstacle/difficulty is. If you think you \"could use L(n\u22121) in conjunction with P(n\u22121) to better predict P(n)\" then I\\'d have expected you to try that in a straightforward, 2-predictor regression (checking some diagnostics, probably needless to say). If you do, please share the results!',)####('You can use logistic regression even if you have binomial data.  You just also need the sample sizes.',)####('@Dason, do you see any benefit to using logistic rather than linear regression, though?',)####(\"I guess I should mention, the number of games per season is readily available, I'm just not sure if there's any point in bringing it into the analysis.\",)####('The descriptions you give in both the question and your comment are a little bit muddled. Have you written out what your matrix $A$ must be for the sample variance? Does that help you see how to generalize?',)####('If you have 4 categorical variables with 4 levels each, you should have 3*4=12 coefficients for your independent variables (plus the intercept)...',)####(\"@andrea: I've decided to treat them as numerical variables.\",)####(\"0.02 is barely significant (especially if you consider the fact that you have five tests in total) and 0.11 is not very high. A generous interpretation would be that with a little more power the overall F-test would also be significant (and perhaps the first coefficient as well). A more conservative interpretation is that you shouldn't have much confidence in any of these results (including the coefficient with a .02 p value). Either way, you shouldn't read too much in the difference between .02 and .11.\",)####(\"You're right, @Dason. You don't need the game level data, you could just use the number of wins as the dependent variable - the estimates and inference would be exactly the same.\",)####('Several questions:  Please tell us N and the measurement level of your dependent variable(s)... Are your variables with >100,000 classes Xs or Ys?...For how many variables, or for how many levels of a given variable, are you trying to impute values?...Why do you need to impute values in order to make predictions?',)####('Is this homework? If so, please use the Homework tag.',)####(\"No, it's not. I think it is true bcoz after all, sum of squares is a square of linear combination of Y's given constant X's. But is it? Simple proof like this would be appreciated!http://math.stackexchange.com/questions/47009/proof-of-fracn-1s2-sigma2-backsim-chi2-n-1\",)####('Would using AIC/BIC or the LogLik be sound statistically to make a choice between non-linear and linear models on the same dataset?',)####('Corrected for D. I think the critical point is that diagonal element of D should be something like (1,1,1,...,1,0,0). Is there any way to prove it? or Is there anyway to show that $\\\\\\\\chi^2(n)=\\\\\\\\chi^2(n-2)+\\\\\\\\chi^2(1)+\\\\\\\\chi^2(1)$ where sse/$\\\\\\\\sigma^2 \\\\\\\\sim \\\\\\\\chi^2(n-2)$, $\\\\\\\\sum{e_i^2}/\\\\\\\\sigma^2 \\\\\\\\sim \\\\\\\\chi^2(n)$',)####('... have you tried it?',)####('Yes I have tried it before posting here....just wanted to have a double check on what i am thinking is correct or not...',)####('It might be a case for non-linearity. Can you show a plot of fitted values vs. residuals?',)####(\"@Charlie, your question doesn't contain a question. Please edit it.\",)####(\"Okay!  Sorry to ask, then, but you'd be surprised how often people ask questions that could be answered by just doing it.  Double-checking your thinking is always welcome!\",)####(\"Sorry, but I'm having real trouble understanding this. Maybe some example numbers would help?\",)####('Its best to avoid using the word skew like that because skewness has a precise and different definition in statistics: http://en.wikipedia.org/wiki/Skewness',)####('What are your goals in creating this model?  To minimize the error associated with your predictions?',)####('@MichaelBishop Thanks for your reply. I added a line in my original post: In my particular application, I don\\'t have to generate a prediction every single time (it can simply returns \"no clue\"), but when I do, I would rather it be correct. Which is why I want to find a way to return \"the regression is not confident here\"',)####('Function interpolation (in the form of Chebyshev or Bernstein polynomials) may be of interest to you. From Weierstrass\u2019 Theorem, these approaches provide bounds on the L-infinity error of any function over a closed interval.',)####('\"Confident\" is an interesting word in this context, because linear regression provides information for constructing intervals of \"confidence\" around prediction: [prediction intervals](http://en.wikipedia.org/wiki/Prediction_interval#Regression_analysis). A wide interval reflects lack of \"confidence\" or trustworthiness in a prediction. You could use these widths to screen predictions for likely correctness. This could work well in conjunction with [goodness of fit tests](http://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares) (which would indicate whether to trust the prediction intervals!).',)####(\"Hi Chl, I put this question as I've very little idea on non-linear regression technique. I want to learn it from scratch. The reference I'm looking for is more theoretical, with some application example from any field.\",)####('Could this be done not with polynomials, but with functions of the form a*sin(b*x), for instance?',)####(\"That's a rather vague question. Are you interested in specific applications, theoretical textbooks, or something else?\",)####(\"I only mention the work on polynomial interpolation because I have some experience with it. I THINK the specific type of function you choose is not important as long as the basis functions are orthogonal. Sorry I can't be more help.\",)####('Depends on the sampling distribution (likelihood). A table showing the conjugate prior of various likelihood models is shown on wikipedia: http://en.wikipedia.org/wiki/Conjugate_prior',)####('Can you explain further what you mean? It is very common to test whether the slope parameter for a variable is different from zero. I would call that \"hypothesis testing\". Are you unaware of that, or do you mean something different? What constitutes a scenario for your purposes?',)####('I am unaware of that. I was also unsure if regression-based analysis is used for any other sort of hypothesis testing (perhaps about the significance of one variable over another, etc).',)####(\"Let's say I have k classes, each coded with its index ({1,2,..,k}). I would like to test how linear regression performs on my dataset. I read different approaches to this issue. One uses an indicator matrix, for instance. I wonder whether one need to use a matrix at all. Can't one just apply the regression to a scalar and then classify (somehow). And, of course, the issues coming with this approach that some classes may not be classified (almost) at all.\",)####('This is really two different questions, because the one about piecewise linear regression does not involve sums of t distributions.  If you still are interested in the one about t distributions, please post it as a separate question.',)####(\"The terms you're using makes this somewhat unclear. We typically use the *generalized* linear model (eg, logistic regression) to classify, not linear regression. If you're referring to *discriminant function analysis*, that can certainly be used (it's more efficient than logistic reg if the response variables are multivariate normal), but DFA isn't usually called multiple linear regression in my experience. If you want to classify cases into more than 2 categories, you might check out [multinomial logistic regression](http://en.wikipedia.org/wiki/Multinomial_logit).\",)####('Hello gung. I mean, if you want to apply generalized linear regression to an indicator matrix (0-1), you may get some issue with masking as explained in \"The Elements of Statistical Learning\" (Hastie et al), chapter 4. That is, many examples will be classified to the wrong class. And I cannot understand the theory behind this issue.',)####('@gung\\'s point is still a good one that your term \"multiple linear regression\" is a confusing one in this context.  Could you re-write your question with terminology closer to that used normally?',)####('I would like to clarify that I want to use least-squares regression to perform the classification',)####(\"@whuber, I'll try to clarify: assume that we have a given dataset and model formula (i.e. what factors / variables / interactions should be in the model).  If we perform unconstrained OLS linear regression using this data and formula, we will get a non-unique vector of coefficients $\\\\\\\\theta_0$ which minimize the RSS.  $\\\\\\\\theta_0$ is one of possibly several vectors in $\\\\\\\\Theta_{opt}$.  All vectors in $\\\\\\\\Theta_{opt}$ share the property that they minimize the RSS (all with the same minimal value). Does that make sense?  There may be a $\\\\\\\\theta_1 \\\\\\\\in \\\\\\\\Theta_{opt}$ with fewer negatives than $\\\\\\\\theta_0$.\",)####('Why do you want to do this? (For almost all statistical purposes, the choice of reference level makes no difference, nor does the number of coefficients that are positive)',)####(\"Why isn't $\\\\\\\\Theta_{opt}=(0,0,\\\\\\\\ldots,0)$ always a solution?\",)####('@whuber: $\\\\\\\\Theta_{opt}$ is a set of vectors, not a single vector.  If a vector belongs to this set, it must provide a least-squares optimal solution (i.e. minimizes the squared residual sum).  An all-zero vector would fail to minimize this sum.',)####(\"@guest: statistics isn't the only purpose of regression -- it is often used for modelling systems from data.  In these cases, one might have prior knowledge about the factors being used in the model, and desire to shape the model's form in such a way to improve human-interpretability of the model.  For example, if I have a factor with 5 levels, and I know that only one of those levels causes an increase in the regressand variable, I would prefer a model form where that 1 level has a positive coefficient, instead of the 4 other levels having negative coefficients.\",)####('Mark, the reason for my comment was that $\\\\\\\\Theta_{opt}$ does not have a clear definition.  What do you mean by \"optimal\" vectors?  Obviously $(0,0,\\\\\\\\ldots,0)$ will be of minimal length.  What criterion excludes it from the set $\\\\\\\\Theta_{opt}$, then?  You haven\\'t given us any information in that regard.  Your response sounds self-contradictory: you seek some parameter vector with certain characteristics (all elements are positive), yet you seem to focus on the LS solution, which apparently does not have those characteristics.  What, exactly, are your objective function and constraints then?',)####('Also, note that specifying the model formula is not the same as specifying its specific terms.  I guess this is a bit confusing with my notation, since $\\\\\\\\Theta_{opt}$ may lead one to believe that all of the vectors it contains have elements corresponding to the same terms.  The terms may be freely changed (under the constraint of maintaining OLS optimality), and this is what affects the length of vectors in $\\\\\\\\Theta_{opt}$.  So, technically, one could think of $\\\\\\\\Theta_{opt}$ not as a set of vectors, but as a set of (terms,coeffs) pairs.',)####(\"On second thought, if you have terms for all levels of the factors specified in the model formula, there really isn't a difference in the terms of one solution to the next -- so, I guess thinking of it as a fixed-length vector (which may contain zeroes) is fine after all.\",)####('You pick values for $\\\\\\\\theta_1$ and $\\\\\\\\theta_2$. Then minimise with respect to $\\\\\\\\theta_3$. You get the value $\\\\\\\\theta_3^{(1)}$. Having it you now use OLS to estimate $\\\\\\\\theta_1^{(1)}$ and $\\\\\\\\theta_2^{(1)}$. You got your first iteration. Then iterate until convergence.',)####(\"Are you referring to identifiability problems, Mark?  Usually, one sets up the model so that there is a unique parameter vector minimizing the RSS. It sounds like you are contemplating a more general framework in which the normal equations are of less than full rank and so you seek a positive vector among the linear subspace of solutions.  (If that is so, we're talking about a linear programming problem, which gives you access to efficient solution methods.)\",)####('Ok, thanks, but what do you gain from iterating like that compared to performing a full-blown NLS on the complete model? You essentially perform lots of OLS regressions iteratively if you run NLS using a Gauss-Newton method anyway, right?',)####('You do not perform lots of OLS regressions if you run NLS. The point of the procedure is that it practice it might work better than straight NLS. The NLS is guaranteed to work if your starting values are close to the optimal ones. So the outlined procedure can be thought of a way to get better starting values. Theory is nice, but in practice sometimes is very hard to get convergence. Any trick then helps.',)####(\"You may get a good answer here, but I'm going to flag this for migration to stats.SE, as the answer to this question essentially boils down to understanding how linear regression works.\",)####('Improved convergence is of course desirable. Thanks for that answer! I may be wrong, but I think you may, in principle, use OLS estimates for the so called Gauss-Newton regressions which are used for computing the step size at each iteration in a Gauss-Newton method.',)####(\"Yeah that's fair enough. Would it be better if I deleted it and moved it myself? Or is that unnecessary?\",)####(\"You shouldn't need to do anything. I flagged it, but it may take an hour or two before a mod gets to it, it being a Sunday and all.\",)####('I won\\'t provide an answer here, because the question will be moved. But you can try a few things to understand what\\'s going on: 1. run lm( Y ~x1 + x2 - 1). the \"-1\" will remove the intercept. 2. use relevel to change the reference category of x2.',)####('@whuber: based on what you wrote, it seams likely that it is a linear programming problem -- do you have suggestions on resources (e.g. links, articles) which address this particular kind of problem with linear programming?  (also doing this kind of thing in R?)',)####('In linear models, the explanatory variables are usually assumed to be non-random, so talking about their distribution (or, indeed, correlations) is meaningless. Are you sure that multiple linear regression is a good choice here? That being said, the residuals can be heavily correlated with $Y$ (or $X$) if, for instance, you have non-independent errors or non-linear relationships.',)####('Related, [What is the expected correlation between residual and the criterion variable?](http://stats.stackexchange.com/q/5235/1036)',)####('Dear Andy, I have thought that the residual should be randomly distributed along the Y. If it correlated with Y, there must be some problems.',)####('Can you provide with several other methods for my data?In my data, Y and most Xs are numeric variables and the other Xs are binary variables. Because there are 25 Xs, it is very difficult to formulate the suitable relationship between Y and Xs. Do you have any suggestions? Thanks in advance!',)####('@Procrastinator As I said on my update above, I hope to have training data that has more \"weights\" for application-specific input space.',)####(\"@whuber Maybe I'm rusty on this, but isn't there a single standard error of estimate that applies to all data points, and thus a single width for all points' prediction intervals at a given confidence level?  CodeNoob seems to want or need intervals of different widths for different points, which I don't think is possible.\",)####('If I properly understand, a full column of X missing means that you did not observe an explanatory variable at all. I do not believe it is possible to estimate it unless there is a known relationship with the observed covariates. This actually happens all the time because there are covariates that you do not observe (for a number of possible reasons).',)####('@Procrastinator I think we can do regression X on Y for the missing data estimation.',)####('Are you interested on a [calibration model](http://en.wikipedia.org/wiki/Calibration_%28statistics%29)? Otherwise it makes no sense to estimate $X$ and then use it on a regression model for $Y$.',)####(\"you're absolutely right - the value of $\\\\\\\\beta$ alone does not indicate the strength of the relationship unless, as @gung mentions below, your variables are standardized.\",)####(\"@rolando Yes, there is a single SE of estimation, but it doesn't tell the whole story.  When predicting the value associated with $(x_1,x_2,x_3)$, you also have to account for the (correlated) uncertainties in the parameter estimates $(\\\\\\\\hat{a}_1, \\\\\\\\hat{a}_2, \\\\\\\\hat{a}_3)$.  This causes the prediction intervals to spread hyperbolically as the point $(x_1,x_2,x_3)$ moves further from the mean value used in estimation; the widths can even become infinite.\",)####('It depends, what did you read specifically about $R^2$ that makes it a non-ideal indication of fit in your case?',)####('@Macro thanks macro i think your pointing towardss cross validation. \\nI ran cv.lm with 10 fold cross validation on the datset with 63% corrolation and i found the Sum of squares = 2.91e+08 is Mean square = 1063519 and the data set with 40% corolation has Sum of squares = 23.1    Mean square = 0.08. A low mean square means that it has predictive ability so my main questions is how to i find the the probability of it concurring randomly?',)####('I believe it\\'s because the purpose of `auto.arima` is automatically estimates all coefficients, including the choice of include or not the intercept.\\n\\nSo, it seems the results are different because the intercept is not being calculated in the \"regression\" part of the model, but in the ARIMA. And in this specific case, they disagree: (a) `lm` chooses to include an intercept and (b) ARIMA chooses not to include. Is it correct? EDIT: Actually it doesn\\'t seem to be correct, because if the intercept is on the $N_t$ part, it\\'s the same thing if it\\'s on the regression when we substitute.',)####(\"your first (auto.arima) model doesn't have an intercept... try using a-mean(a) and b-mean(b) to eliminate the need for one. You'll get the same coefficient estimate that lm gives if you do.\",)####(\"Yes, if I remove the mean, the `auto.arima` model generates the same coefficients that `lm`. But why isn't it estimating an intercept when I use the raw data? Shouldn't the procedures be the same inside `auto.arima`?\",)####(\"If you just use `arima`, you can specify `include.mean=TRUE` in which case you'll get the `lm` coefficient estimate; I too find it a little odd that `auto.arima` doesn't allow this option (I just tested it) but I expect there's a good reason for it.\",)####('What makes this a non linear model?',)####('The line you see is not fitted. The data gives you a similar graph like michaellis menten, but in that model there is no time involved, here I have response ~ time not velocity ~ [substrate concentration]',)####('With a non linear regression michaellis menten model, you could predict variables like Km and Vmax, so I like to find a model to predict similar variables.',)####('You could validate by simulation when the predictors truly have no effect or by permuting the responses to give an idea what prediction accuracy to expect purely by chance.',)####(\"@jbowman Thanks for the sim suggestion, I'll give it a try on the weekend.\",)####('If you simulate data where the predictors truly have no effect, this will give you a reference. If you permute the response variables, you will also have a situation where the predictors have no effect, so that would be another way of getting a reference.',)####('A large part of your problem is that you have a highly nonlinear function (in the parameter in question) and not much data.  You might want to try the experiment of using $n=20, 40, 80, 160, 320$ and plotting the posteriors for a given weakly informative prior, to see how fast they become \"reasonable.\"  Plots of the series of generated estimates and the `acf` function will help with determining burn in times and thinning rates.',)####('You may want to look up the different kinds of sums of squares. Specifically, I believe linear regression returns type III sum of squares, while anova returns a different kind.',)####('Of possible interest: [Link](http://www.ats.ucla.edu/stat/r/dae/rreg.htm).',)####('If you save the results of `lm` and `aov` you can check they produce identical fits; e.g., compare their residuals with the `residuals` function or examine their coefficients (the `$coefficients` slot in both cases).',)####('These are good questions, but as the comments and attempted replies indicate, they cover too much ground.  Please focus on one at a time, perform some research, and be specific about what you\\'re looking for. \"I didn\\'t get it\" doesn\\'t give us enough clues to provide the information you need.',)####('[I answered](http://stats.stackexchange.com/questions/28688/how-to-interpret-model-diagnostics-when-doing-linear-regression-in-r/28697#28697) many of these questions last week. You will learn more if you search for similar questions from other sources.',)####('An even better discussion here: http://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output',)####(\"@Procrastinator Your link does show someone who proves examples of robust estimation in R and specifically M-estimation.  I think that qualifies as an answer to the question.  Why don't you want to post it as an answer?\",)####('@MichaelChernick Thanks for your comment. I did not post it as an answer because I do not really understand what $M$-estimation means. I did a quick search by mere curiosity and found that link.',)####('It looks like that link gives examples of _univariate_ regression models using robust ($M$) estimation, but I think the question is about $M$-estimation for a multivariate dependent variable.',)####('And moreover, if my intent was to study the correlation between x1 and x2 and its impact on y... how shall I construct my experiment?',)####('In the linear regression model the dependent variable $y$ is considered continuous, whereas in logistic regression it is categorical, i.e., discrete. In application, the former is used in regression settings while the latter is used for binary classification or multi-class classification (where it is called multinomial logistic regression).',)####(\"That sounds like an answer to me, @Pardis :) I'd +1.\",)####('I am not sure that it is possible to come up with such an interpretation. Simply because what you provided are images in the original space of features and responses. And penalized regression involves the space of coefficients, which is very different.',)####(\"re: your second question - the second model drops the fixed intercept while the first does not; it effectively constrains the mean of the observations to be 0. I don't recommend the second model unless you have substantive theory which supports it.\",)####('Thanks Macro. But I don\\'t understand why it \" it effectively constrains the mean of the observations to be 0. \"? I was thinking that the (1 | county) has intercepts any way and the fixed-effect intercept can be absorbed those intercepts anyway',)####('the random effects have mean 0 so they cannot absorb the missing intercept the same way fixed effects can in a usual regression model.',)####('What the random effect to in a mixed model is increase uncertainty in the estimates because these effects are to be viewed as a sample from a large population as a center is in a muLticenter clinical trial.  You may have chosen 15 centers out of thousands that could have been selected. This unseen variability that would show up of you repeated the trial with different centers need to be taken into account.',)####(\"The random effects are required to have mean $0$, for identifiability. Yes, `lmer` and `nlme` both certainly impose that restriction in the model definition. You're welcome.\",)####('Well, why exactly are you using a mixed model? If you can explain that to your audience, this could go a long way in terms of providing intuition for the audience.',)####('In the Bates\\', yes, 0-mean was emphasized in the model definition; but in Gelman and Hill 2007, 0-mean is not in the model definition. Are you saying that \"lmer\" and \"nlme\" force 0-mean, since they were designed by Bates? Thank you!',)####('I will update my question, @JohnSmith. Thanks!',)####('possible duplicate of [Should confidence intervals for linear regression coefficients be based on the normal or $t$ distribution?](http://stats.stackexchange.com/questions/29981/should-confidence-intervals-for-linear-regression-coefficients-be-based-on-the-n)',)####('How can nominal variables be correlated? Are you just using \"collinear\" as a catch-all for any kind of association here?',)####(\"Is the purpose to draw information from the parameters or to predict the outcome of future games? If you are predicting the you don't need to worry about multicollinearity(you should of course still not include the reference dummy variable.) On another note you should use multinomial logistic regression and not linear regression since you are looking at a categorical output variable. To help you further please provide an example of your data.\",)####('@Macro: I added simplified format of how I am formatting the data. My understanding of \"collinear\" is that there are independent variables that correlate to each other.',)####('@pgericson: The goal is to identify who our \"best\" players are. Unlike other sports where there is a center of the action, it\\'s sometimes hard to identify who is benefiting the team so we are hoping that regression will help us identify undervalued players.',)####(\"This seems like a homework problem. If it is, please add the `homework` tag so that we can treat it accordingly. Also add some comments on how you've tried to solve the problem and where you got stuck.\",)####('There\\'s also no need to email users of Stack Exchange sites asking them for an answer. After I got your email, the first thing I did was search Google for \"bayesian test of linear regression hypothesis\" and this question was in the top 5.',)####(\"Please update your previous question. I'm closing this one as a duplicate.\",)####('It would be preferable to incorporate this into your previous question. Have you searched the site for duplicate and related questions?',)####('If this is homework, please tag it as such.',)####('Your answer is on the wikipedia page about simple linear regression:  http://en.wikipedia.org/wiki/Simple_linear_regression#Confidence_intervals',)####(\"I referred to the wiki article. But I didn't get some of the stuff. I have reposted in the question\",)####('I find significant autocorrelation in the residuals from calculating the rolling return in such a method',)####('multicollinearity is what you get when two or more predictors are highly correlated with each other, but it appears you are generating variables which are not correlated at all.',)####('@Luna, why is that wrong? It appears you used `x3` to generate the `y`s, so it should be included in the model and the $p$-value agrees with that conclusion.',)####(\"@Seth - you are right. I was just giving a toy example of using anova generally in model comparison. So it's not linked to my original question.\",)####('@Macro - you are right. Now I see the point. Thank you!',)####(\"Can you say more about what you'd like to know? (This question is fairly simple / sparse.) You may also be interested in [this recent question](http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr).\",)####('good catch, @gung. I thought this sounded familiar. I voted to close as a duplicate.',)####(\"What do you mean under `nonlinear regression ... with the help of genetic programming`? Could you provide more details? Why don't you simply use polynimial fitting?\",)####(\"R has outsmarted you :), it treats the two regressions differently.  In the first case, it takes the first factor and re-frames it as a smaller number of factors, each representing the difference between a level and the first level (so there are 2 coefficients to estimate in your case instead of 3.)  It then does the same for the second.  Hence you get an intercept and 3 total coefficients.  In the second case, it knows there isn't an intercept, so it doesn't re-frame the first factor, instead leaving it as is.  It still re-frames the second factor, though, to avoid the multicollinearity.\",)####(\"@jbowman, that's a great catch, & I think, exactly the answer to this question. Why don't you turn the comment into an answer?\",)####(\"@gung - thanks for the suggestion, I couldn't think of a good reason why not, so I did :)\",)####('But if we put a design matrix for categorical variable and an intercept together, the combined design matrix will be of reduced rank, i.e. rank-difficient. Am I right?',)####('Some very rough rules of thumb: you should investigate collinearity *before* you do any fitting. If you find it is present, you should either (a) use a method that handles collinearity, (b) remove collinear features, or (c) transform your features (eg using PCA). Once you have fitted a model, you can look for heteroscedasticity in the residuals. In general, if you are making a predictive model you should not remove outliers. Instead, use a method which is robust to the presence of outliers.',)####(\"genetic programming is able to find a mathematical model of data by the evolutionary computing method of iterative test-generate-select. given a set of operators, and training data (input-output), the program constructs randomly a set of expression trees, refined through many iterations of recombination , mutation, etc, (which in effect is exchange of subtrees), fit evaluation, and fitness based selection to arrive at a model of data. i hope that's somewhat descriptive. i tried regression trees, polynomials, and the result on training and test data is not as good as genetic programming.\",)####('Have you tried some kind of regularization scheme (e.g. ridge regression)?',)####('In general, you should have a good reason _not_ to include the intercept, not the other way around. See the thread above, which has lots of useful info.',)####('Luna, I appreciate the good questions you\\'ve been asking here & the productive discussions that have been brought about because of your efforts. As you are becoming more active here, it would be worthwhile to become more familiar with CV\\'s style of asking questions. We tend to discourage repeating the title, & having conversational elements such as \"Hi all\" & \"Thank you!\". The idea is that these questions ultimately become a searchable public record & that stuff ends up getting in the way.',)####(\"How does one best investigate collinearity? Looking at the off-diagonal elements of the predictors' correlation matrix?\",)####('Regression involves fitting a linear model to real data which includes some random variation. Are you really interested in fitting regression lines or just ploting lines like your graphs seem to indicate? There is no point to fitting regression to data without a noise component.',)####('A good way to check things like this is to simulate the data using known coefficients and very small random error terms, and then check that the output is consistent with the simulation.',)####('The best way to investigate collinearity is condition indices and proportion of variance explained by them. High correlation is neither a necessary nor a sufficient condition for collinearity.',)####('i am looking to find parameters for a theoretical model that best fit real world data. If you\\'d like i can edit the images in the question; adding just enough noise to make it look *\"more random\"*, without changing my question at all.',)####('thanks for the clarification.',)####('Question 1 is related to [Look and you shall find (a correlation)](http://stats.stackexchange.com/q/5750/930) and Question 3 to [Is adjusting p-values in a multiple regression for multiple comparisons a good idea?](http://stats.stackexchange.com/q/3200/930). More generally results from [this query](http://stats.stackexchange.com/search?q=%2Bcorrelation+%2B%5Bmultiple-comparisons%5D) may be of interest.',)####('Assuming the factors have independent effects, do you know how that estimate is produced?',)####(\"The model you've written is $Y_i = \\\\\\\\beta_0 + \\\\\\\\beta_1 B + \\\\\\\\beta_2 M + \\\\\\\\beta_3 H + \\\\\\\\varepsilon_i$ where $B,M,H$ denote dummy variables for the levels of the variables in question. The $\\\\\\\\beta$ estimates are chosen to minimize the residual sums of squares - is that what you're asking?\",)####('It would be the mean when `wool==\"A\"` AND `tension==\"L\"` if you also included the interaction between `wool` and `tension` in the model. Without that, I think you\\'re just left with an estimate of the mean, assuming the two factors have independent effects.',)####('Thanks @jbowman, I think now I understand! I must use 0.00340945299? Ok?I just added the covariance matrix to the question.',)####('Yes, you have got it!',)####(\"Ah, sorry, didn't fully answer your question.  $cov(\\\\\\\\hat{\\\\\\\\beta}_1,\\\\\\\\hat{\\\\\\\\beta}_2)$ would be the entry in the matrix with column name, in your case, `as.factor(X1)1` row name `as.factor(X2)1`, assuming what you wished to find the covariance for were the coefficients with values -0.12795 and 0.05666, and similarly for the other covariances.\",)####('**Thanks a lot!!!!!!!**',)####(\"@jbowman, why don't you convert your comments into an answer (since that's what they are)? Then the OP can accept it, & I can upvote it.\",)####(\"It also happens to be wrong... I didn't realize that `summary(reg)$cov` gave an unscaled covariance matrix.  I'll post the correct answer in a minute or two.\",)####('Deleting my erroneous comment to avoid causing confusion... and posting this note to clarify why the comment thread may be incomprehensible afterwards.',)####('I also edited the question to avoid confusion. Thanks @jbowman.',)####(\"This is the same situation addressed at http://stats.stackexchange.com/questions/33193. Both questions derive from the same misconception, so let's be very clear: **there is no normality assumption about the distribution of _any_ of the variables in regression modeling.** The present question also covers the same ground as another (which I haven't looked up) asking for a transformation to convert a categorical variable to approximate normality: the answer is that no such transformation can possibly exist (except perhaps when there is a large number of categories).\",)####('@whuber is right, +1. Also note that in R, `lowess()` & `loess()` are essentially the same thing. [?lowess](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lowess.html) is the older & [?loess](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/loess.html) is the augmented version (although, for simple things, I find `lowess()` easier to use).',)####('The [Gauss-Markov Theorem](http://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) is one major motivation to prefer least-squares regression over some other arbitrary linear estimator :)',)####(\"If you have a single categorical variable you are effectively doing a one-way anova and the concept of a linear relationship between your regressors and your response isn't well defined since you don't have any way to measure the distance between your categories or for that matter even ordination.\",)####('I think you are trying to work on a problem that is too difficult. If you have a single covariate and a single response finding a breakpoint in a line may not be too difficult but with 196 different variables allowed to change how could you even picture the shape of the response in such a high-dimensional space much less identify breakpoints!',)####('Are you referring to a question of *experimental design,* where (for example) you could choose $2n$ $x$ values and you might elect to take $n$ at one value of $x$ and another $n$ at a second value of $x$, as opposed to spreading all $2n$ $x$ values out? Or are you talking about regression with $(x,y)$ data?  And what do you mean that \"$n$ is proportional to $\\\\\\\\sigma^2$? How is that possible when either you control $n$ or the data are already given to you?',)####('Hi @whuber you got the first one right. 2n x values, with n at an $x_1$ and n at an $x_2$. And when I say \"n is proportional to $\\\\\\\\sigma^2$ I mean that in general, if you have an experiment with high standard deviation in the results, you are going to have to set $n$ to be very high to get a good estimate. In my tests I could change $\\\\\\\\sigma$ because I was running a simulation.',)####('And thank you @Macro I will definitely check that out.',)####('Mike OK, but exactly what do you mean by the \"maximum error\"?  Error of what?  Residuals?  Coefficients?  Predictions for $y$ at given values of $x$?  Where you refer to a \"good answer,\" what exactly constitutes an *answer*?',)####('The maximum \"likely\" error of the difference in means of the samples at the points $x_1$ and $x_2$. If one assumes that the means are not going to appear outside the confidence intervals I talked about (this is what I meant by restricting the \"realm of possibilities\"), than the biggest possible difference between the difference between the true means and the difference between the sample means is from the bottom of one interval to the top of the other, so 2 times the ME stated. A good answer is one that will precisely estimate the value of the true difference.',)####(\"I admit to some confusion. You seem to be interpreting confidence intervals in an unconventional way; and I do not see anymore what your question has to do with regression. Aren't you now just asking how to estimate a difference of means in two independent samples?\",)####(\"@whuber, It is definitely not a conventional interpretation, should I call it something different? I'm just assuming I wont have to worry about improbable results happening. To your other question, I am asking why regression would be better.\",)####('My friend\\'s wife constantly nags him to do chores: \"Honey, it will only take ten minutes!\" From experience he has discovered that a \"ten minute\" chore will take an hour and an \"hour\" chore will take up most of the day. So now when she asks him to do something that will only take $x$ minutes, he figures it\\'ll really be $x/10$ hours: that\\'s the bias correction. They\\'re both happier now that they\\'re in agreement :-).  (In many fields, this is called *calibration*.)',)####('Also of interest: [Significance-of-coefficients-in-linear-regression:-significant-t-test-vs-non-sigificant-F-statistic](http://stats.stackexchange.com/questions/24720/).',)####('Because this question makes no sense without the background provided by the link--and not everyone will have the time or interest to sit through it--please explain the context and the meaning of the notation.',)####(\"Thanks @whuber updated to add context.  I'm a newbie to stats, so I'm not sure if this you enough information.\",)####('@whuber thanks for the analogy. I wanted to know how bias is calculated. I mean by linear regression as you said I can find a model that can give me the calibrated value as you mentioned. But I wanted to know how can I know about the bias from the scatter plot. Actually I wanted to know how it actually looks like given some data',)####('My comment included actual data :-). Seriously, the procedure is no different than the intuitively obvious approach taken by my friend: he found a way to predict real experience ($y$) from estimated values ($x$) and used that relationship to adjust future estimates. Are you asking how to apply OLS to such situations? If so, then what have you learned about OLS so far and where are you stuck?',)####('\"Best\" equation for what purpose? Prediction, estimation, understanding, theory-building?',)####('@whuber prediction :-)',)####('OK, then in your researches pay special attention to mentions of holding out data to use for verifying the model as well as more sophisticated versions known as \"cross validation.\" Using these approaches will make you somewhat immune from the dangers of many model selection procedures such as stepwise regression.',)####('This is a question about selection. It is a common topic here & elsewhere. You should *not* only include significant variables, to understand why, you may want to read this: [algorithms-for-automatic-model-selection](http://stats.stackexchange.com/questions/20836/20856#20856). You should also search / read around CV under the tags: [tag:feature-selection], [tag:model-selection], & [tag:stepwise-regression], for starters. If there is still something that you want to know after having read through that, edit your Q here to clarify, otherwise this Q should be closed as a FAQ.',)####('What $\\\\\\\\chi^2$ test you have in mind? $(\\\\\\\\theta-\\\\\\\\theta_{ML})(Var(\\\\\\\\theta))^{-1}(\\\\\\\\theta-\\\\\\\\theta_{ML})$?',)####(\"Yes. One problem is that I don't know $Var(\\\\\\\\theta)$ so I would have to estimate it from the data.\",)####('That depends on the problem under consideration. A polynomial regression always yields a better fit than an affine regression but it could make no sense from the interpretation perspective.',)####(\"If I understand correctly, the difference in the dependent variable $y$ may depend: 1) on another variable $x_6$ you didn't detect (it may be the time); 2) on random errors. Which may be your case?\",)####('By presupposing a polynomial solution, this question implicitly rules out procedures that might be simpler or more effective.  I would like to suggest that you begin your research by reviewing any appealing threads related to [model selection](http://stats.stackexchange.com/questions/tagged/model-selection+-time-series?sort=votes). You may find many of these informative and stimulating. Then consider returning with a more specific question motivated by that information.',)####('Did you checked for linearity!? check for other models for non linear data. or you can always plot the data and try to build a polynomial model that as the same look...then just shape it (fit it)...',)####('What is it that you want to test?',)####('@whuber I am sorry, my limited mathematical capability permits me only so much formal stringency. Please let me know if the formulation is more appropriate now.',)####('What do you mean by \"in general\"?',)####('I changed the title.  Is that suitable to everyone?',)####(\"@Zen What is so strange?  The OP meant the sample means.  The are expected values for the empiricla distribution.  I would not have chosen that notation but I didn't want to go about changing that too.\",)####('For a discussion of the opposite case, you can also see here: [how can a regression be significant yet all predictors be non-significant](http://stats.stackexchange.com/questions/14500/), in addition to the question linked above.',)####('But \"always\" comprises exactly what scenarios?  *You* need to stipulate the circumstances and assumptions to be made, not your respondents! Otherwise some people will assume you are thinking of $X$ as a random variable and others will assume it is not random; some may assume it could be a vector of values and others that it is just a single (scalar) value; etc.',)####(\"The notation in the original title made it look like the question was why $E(Y) = \\\\\\\\beta_0 + \\\\\\\\beta_1 E(X)$ (which is true by definition if the model is correct) or possibly why $E(Y) = \\\\\\\\hat \\\\\\\\beta_0 + \\\\\\\\hat \\\\\\\\beta_1 E(X)$ (which certainly isn't true) but it seems that the consensus is that this question is asking why $\\\\\\\\overline{y} = \\\\\\\\hat \\\\\\\\beta_{0} + \\\\\\\\hat \\\\\\\\beta_{1} \\\\\\\\overline{x}$ where $\\\\\\\\overline{y},\\\\\\\\overline{x}$ are the sample means. This seems correct given the text in the question so I've changed the notation to reflect that interpretation. OP, please revert if this is wrong!\",)####('The notation seems more appropriate now. I meant sample means. @whuber: By \"in general\" I meant whether this always occurs (and if not, under what conditions it does).',)####('As far as I understand robust regression and robust methods in general, they work best when the sample is contaminated with a few outliers. This means that certain percent of the sample points do not conform to any relationship. What you have basically done was comparing regressions with iid noise vs regression with independent non-identically distributed noise. Asymptotically the loss of non-indenticallity is not a big deal, i.e. central limit theorem still holds. Thus you do not see big difference, since in your case simple OLS is robust.',)####('What are the degrees of freedom for the t?  If df is > 5 the tails are not very large and the difference from a standard normal are not that great.  Why not try a Cauchy?  Why mix the t with a normal?  Try all errors Cauchy.  Did you pick a model where the slope(s) of the regression curve (surface) is (are) large?  You could also pick a few points that would have errors that would make them highly influential for the least square regression parameters or the bivariate correlation between a predictor and the response.',)####(\"Is y =ax$^2$ with a>0 the onlt way to get a perfect inverted U?  I don't think so.  If the inverted U corresponds to a different function then there will not be an a that gives a perfect fit to y=ax$^2$.  Then it would be possible for some other model to fit better.\",)####('with three degrees of freedom I see higher variance of estimator for OLS',)####('Note that one perspective on the relationship between regression & correlation can be discerned from my answer here: [What is the difference between doing linear regression on y with x versus x with y?](http://stats.stackexchange.com/questions/22718//22721#22721).',)####('The usual rule of thumb is 10 points for each independent variable.',)####('Please, [register](http://stats.stackexchange.com/faq#login) your account. That will enhance your experience with this site and will allow you to vote on and accept previous answers.',)####(\"I'm not sure there's a problem here, but if you can provide your data, people may be able to say more.\",)####('I suspect that you squared the mean-centered x instead of mean centering the squared x. If you mean center before you square, you get a u-shaped predictor.',)####('@Fojtasek I also tried to mean centering the squared X, then I have a positive beta for the normal independent variable and a negative beta for the squared one. That is exactly the case when there is an inverted U-shaped relation, but in that case there is a multicolliniearity problem because the VIF value is higher than 10.',)####('How are your indices measured? If they include estimates of variability, then two could be enough (using a t-test or its analog). The basic statistical principle that applies here  is that when random variation is an unlikely explanation of what you are observing, then you have the right to attribute any apparent trend to non-random causes. When the trend is strong, very few data values may be needed to come to such a conclusion, all generic \"rules of thumb\" notwithstanding.',)####('You are asking a theoretical question. You can *construct* data by specifying the $X_i$, $Z_i$, and $\\\\\\\\epsilon_i$ subject to the constraints implied by your assumptions. You can easily meet all those constraints (about correlations) by choosing $\\\\\\\\epsilon_i$ of small magnitude.',)####(\"There's very good info at http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor .\",)####('Thanks for the link Rolando2',)####('This question is very broad. can you be more specific?',)####('The trouble with OLS is that it assumes the errors are independent, and, if data are clustered, that assumption is violated.',)####(\"Doesn't this question answer itself (in the negative)? If you make the magnitudes of the $\\\\\\\\epsilon_i$ small, then you can induce any correlation you please between them without appreciably changing any of the relationships among the $X_i$ and $Z_i$.\",)####(\"I can't follow. The $\\\\\\\\epsilon_i$ are the residuals from the linear regressions. They are determined by the model fit. What choice do I have regarding their magnitude?\",)####('The site shows your profile flair so there is no need to sign your posts.',)####('Good suggestion. I edited the question.',)####('I am thankful for the attention my question is receiving from you. However, my question arises from a practical problem where my data have already been sampled and are not being specified by me. I thought it beneficial to try and pose the question as general as possible, but now it appears that posing it in terms of random variables instead of data has made it a wholly different question.',)####('It is correct that questions about random variables are usually different than questions about data. But since nobody has attempted to answer yet, why not just edit this question so that it asks what you really want to know?',)####('Great--good job. My previous comments still apply, though. They show you how to construct datasets in which the residuals can have arbitrarily strong correlations regardless of the correlations among the original variables. Such a construction shows your question about making *general* statements must necessarily have a negative answer. Having seen this, you might want to refine your question further to include any constraints on the generality you seek.',)####('There are many conventions for doing this. Different journals like different things; different programs do different things by default.',)####(\"Bayesians may make the same assumptions about the error terms in linear regression but the inference depends on priors on the regression parameters.  Frequentists don't use priors.\",)####('Thanks Peter. I have put in more details.',)####('Many social scientists will be used to APA format, using the style guide of the American Psychological Association.',)####('See [What is a good resource on table design](http://stats.stackexchange.com/q/3542/1036) and [Some notes on making effective tables](http://stats.blogoverflow.com/2012/02/some-notes-on-making-effective-tables/) for references of interest. IMO for presentations you want to focus information much more than typical. Tables with more than 4~5 rows of information are too difficult to read in that venue.',)####(\"I would say **b** because simple linear regression assumes independence of the residuals, which probably isn't the case with repeated measurements on individuals.\",)####('After giving it some thought, I think the answer is plain \"no\". If you post that as answer, I\\'ll accept. If you already have certain constraints on generality in mind that lead to interesting insights, I\\'d be very much interested. I don\\'t find any, neither in my data nor my imagination.',)####('Broken link: please update the link to \"this function\".',)####('Note the following thread is related, & will be of interest: [When should you center your data & when should you standardize?](http://stats.stackexchange.com/questions/29781/).',)####('Why do you think that the researchers should have used logistic regression? The response variable is clearly continous (although maybe with some censoring). Please elaborate, if we are to help you in providing a good answer to this question.',)####(\"Hi.. that's all what is given :S..so u think the answer is a. Yes?\",)####('For an interesting discussion of thorny issues surrounding this topic, see http://stats.stackexchange.com/questions/9334/determining-best-fitting-curve-fitting-function-out-of-linear-exponential-and',)####(\"And don't forget to make sure the results are CrossValidated ... seriously :)\",)####('Hi Max..can you please explain why you choose b and nt a?.. thanks',)####('@Francois I think Max is right. I will also go with (b). As these are repeated measurements of your variables, so you should adopt models for longitudinal data. You should take into account the possible correlation in these repeated measurements. The responses closer in time are likely to be highly correlated. There are many other practical issues involved with it, I would suggest that you follow this useful book for theories-http://www.amazon.com/Applied-Longitudinal-Analysis-Probability-Statistics/dp/0470380276',)####('I would also have picked (b) for potentially several reasons, the most obvious issue being the dependence.',)####(\"I don't think this kind of assessment is generally used with simple regression models. What would it tell you that you wouldn't find out from using the entire dataset to generate your regression parameters? Normally the reason to use an evaluation dataset is to prevent overfitting, but that's not an issue when you already know that your model is going to contain just one independent variable.\",)####('Are you able to post a copy of the lecture notes, or link to them?',)####(\"SSY = SSE if $\\\\\\\\hat{\\\\\\\\beta} = 0$, which is not the same as $\\\\\\\\beta=0$.  Randomness of the residual guarantees, in the case of continuous residuals, that $\\\\\\\\hat{\\\\\\\\beta}$ won't actually equal 0 even when $\\\\\\\\beta=0$..\",)####('@jbowman Thanks. But if at the same time SSE/DFE also estimates sigma^2, then my question is how is SSE/DFE = (SSY - SSE)/(DFY - DEF).',)####('Max There is no further explanation. The text is basically when beta=0, (SSY - SSE)/(DFY - DFE) estimates sigma^2.',)####('So that we can understand this question, please tell us what it means for predictors to be \"strong\" or \"weak.\"',)####('Having admitted you do not yet understand the math, this might be helpful for you http://stats.stackexchange.com/questions/41794/bayesian-updating-for-a-discrete-rating-value/43048#43048 and for practicle use you might wish to look at this http://stats.stackexchange.com/questions/43471/examples-of-bayesian-and-frequentist-approach-giving-different-answers/43498#43498',)####(\"Cross validation (as Nick Sabbe discusses), penalized methods (Dikran Marsupial), or choosing variables based on prior theory (Michelle) are all options. But note that variable selection is intrinsically a very difficult task. To understand why it is so potentially fraught, it may help to read my answer here: [algorithms-for-automatic-model-selection](http://stats.stackexchange.com/questions/20836//20856#20856). Lastly, it's worth recognizing the problem is w/ the logical structure of this activity, not whether the computer does it for you automatically, or you do it manually for yourself.\",)####('I edited my main post, I hope that makes it clear. Thank you!',)####('The concept of _variable importance_ may be helpful. As may be the `caret` package: http://cran.r-project.org/web/packages/caret/vignettes/caretVarImp.pdf\\n\\nConversely, I would not put too much stock in _statistical significance_, which measures something different than importance (however that concept may be defined).',)####('Of possible interest: [Explanatory power of a variable](http://stats.stackexchange.com/a/11951/930).',)####('Cristina, could you please explain exactly how your model would differ from two independent regressions `y1~x` and `y2~x`?',)####('You can have a look at [Multivariate multiple regression in R](http://stats.stackexchange.com/a/11132/1909) for a walkthrough including multivariate tests.',)####(\"@Cristina: do you mean a PLS 'regression'?\",)####(\"My dependent variables are highly correlated that's why I want to account for both of them together in the same model.\",)####('Type \"conjugate bayesian gaussian\" in Google to find some informations about the derivation.',)####('I discussed the nature of logistic regression in a slightly different context here: [difference-between-logit-and-probit-models](http://stats.stackexchange.com/questions/20523//30909#30909). That answer also briefly mentions the connection b/t linear regression and the generalized linear model. It may be helpful to read it.',)####('The models for linear regression and binary logistic regression are both special cases of the generalized linear model (GLM): Linear regression with an identity link and Gaussian conditional distribution (with equal variance), logistic regression with a logit link and binomial conditional distribution.',)####('It can be ordered from amazon; a full explanation of the algorithm and all the associated caveats and issues is not really the sort of thing you can cover in a few hundred words or even a one page answer.',)####('This is discussed in detail in the book by Davison and Hinkley, *Bootstrap Methods and Their Application*, along with an explicit algorithm (Algorithm 6.4). They explain concepts, pitfalls and details at greater length than is possible in a reasonable answer here.',)####('Does this help? http://stats.stackexchange.com/questions/12900/when-is-r-squared-negative',)####('well, it does shed some light on the topic (now I realize that my question only concerns linear regression where the y-intercept is not constrained)...',)####('@Glen_b Thanks for the reference. Unfortunately I am not in a university or company so I do not have the resources to acquire the book.',)####('See [here](http://en.wikipedia.org/wiki/Law_of_total_variance).',)####('Note the following threads are related, & will be of interest: [When should you center your data & when should you standardize?](http://stats.stackexchange.com/questions/29781/), & [Variables are often adjusted (e.g., standardised) before making a model - when is this a good idea, and when is it a bad one?](http://stats.stackexchange.com/questions/19216/).',)####('Check also out answers to this post: http://stats.stackexchange.com/questions/34769/regression-detecting-significant-predictors-out-of-300-independent-variables',)####('REGRESSION procedure in SPSS uses Type III SS only. Use GLM (=UNIANOVA) procedure instead if you want to choose among types of SS.',)####(\"This seems like a math question, not a statistics one. In statistics, we would want (indeed, require) many more points than the mathematical minimum. There's nothing really about regression here - it's more the minimum points to define certain curves in various spaces.\",)####('Even in statistics one should inspect the design matrix for adequacy, apart from doing the hypothesis testing on coefficients and VIF analysis...',)####(\"I have merged your two unregistered accounts, Murphy. Please, don't forget to [register](http://stats.stackexchange.com/faq#login).\",)####('Probability of what?',)####(\"I guess the other question is type I or III preferred?  I understand when performing a normal anova the differences, but I'm wondering given the hierarchy in her, type I is the way to go?\",)####('Yes, type I is often referred to as hierarchical (or sequential) decomposition in linear model. Default (in SPSS) type III is in no way hierarchical.',)####('You may notice that the ANOVA table lists the degrees of freedom associated with the analysis as **0**; you have the same number of variables in both models, that is the reason that no F or p-values can be computed.',)####('The structure of $\\\\\\\\Sigma(x)$ is unknown, I suppose that in my problem it is close to linear. From some plots it looks like a linear fit is not too bad, even though there are some important outliers which are quite problematic.',)####(\"How much heteroskedasticity is there (roughly, $\\\\\\\\max \\\\\\\\sigma_i / \\\\\\\\min \\\\\\\\sigma_i$ as a guess?  If it's less than about 3 or so, the efficiency loss is pretty small, or so I was taught as a rule of thumb...\",)####('The heteroskedasticity is moderate, but my estimates are suffering because of some big outliers (several standard deviations away from the regression line).',)####(\"The probability of the measurement being correct. Smaller measurements are more accurate, but I'd like to associate a probability with this.\",)####(\"@Procrastinator: I agree that all methods will fail *for some rate of contamination*. And 'failure' in this context can be defined quantitatively and empirically. But the idea is to still favour those methods that will fail only at higher rates of contamination.\",)####('@user603 That is true for any method, there is no Panacea ;). I was simply pointing out another method. +1 to your answer.',)####('Since this is being done repeatedly during an optimization routine, perhaps the data in the regression are (eventually) changing slowly. This suggests an algorithm adapted to your situation: start with some form of robust regression, but when taking small steps during the optimization, simply assume in the next step that any previous outlier will remain an outlier. Use OLS on the data, then check whether the presumptive outliers are still outlying. If not, restart with the robust procedure, but if so--which might happen often--you will have saved a lot of computation.',)####('A method that you did not mention is the use of Student-$t$ errors with unknown degrees of freedom. However, this may not be as fast as you need.',)####(\"@Procrastinator: (It's easy to imagine a configuration of outliers where) this will not work.\",)####('@AdamO I have included the whole Monte Carlo experiment I have used. But more importantly I found a mistake in my assumptions (see the answer).',)####('The arrows appear to point to the locations where the second derivative of $f$ is largest: is this what you mean by \"the point on the curve at which the horizontal component of the curve meets the oblique component\"?',)####(\"Note that you don't need to burn in the simulations, as you're not doing MCMC.  Also how much bias are you seeing? And how many simulations are you doing?  Your simulation based estimate of bias could have a large simulation variance if the number of simulations is not large enough\",)####(\"I'm including a burn in because I don't want my answer to depend on the initialization of the AR(1) process. I've used fairly big sets of simulations (> 10000), and the bias can be around 0.1 if the sample size is small (N = 20). The bias progressively disappears as N increases.\",)####('@prob: The point of the \"burn in\" is that this process does not start at the stationary distribution and so, just as in MCMC, using some burn in period will get you somewhat closer. In contrast to the general MCMC situation, it\\'s easy to characterize the rate of convergence and also almost as easy to just start right at the stationary distribution (which the OP hasn\\'t done). This also relates to the concept of transient response in signal processing, which the OP might want to investigate.',)####(\"How are you assessing the bias in this simulation? I've replicated your code and not found any indication that they give significantly different results. (10000 reps)\",)####('Oh I completely misunderstood you then. Sorry. May I ask: Is the linear model necessarily appropriate? What results do you get when estimating right now?',)####(\"What is the reason to constrain the ranges? If it is just a technical question you should maybe ask on stackoverflow, since this is more of a stats theoretical site. If the reason for constraint is a inheret to your model, I'd advise you to consider a (semi)-logarithmic model depending on what you are doing. Cutting out data point entirely tends to be a really bad idea for inference.\",)####(\"I'm trying to fit some biological data, and need to constrain the coefficients such that they are not nonsensical...\",)####('It depends on what you mean by \"best\" and on the nature of your data. For example, if $y$ is measured with additive error then really $y = \\\\\\\\beta x^\\\\\\\\gamma + \\\\\\\\varepsilon$, which is nonlinear, but if $y$ is measured with *multiplicative* error then $\\\\\\\\log(y) = \\\\\\\\log(\\\\\\\\beta) + \\\\\\\\gamma \\\\\\\\log(x) + \\\\\\\\delta$ is actually a linear model. *The two models are not the same*: typically they will produce (slightly) different estimates of $\\\\\\\\gamma$ and different standard errors. If also $x$ is measured with error, both models get substantially more complicated and yield yet two more estimates.',)####(\"yep, that's what I meant, thanks!\",)####(\"no problem....linear model's a start...right now I get negative values for one of the coefficients, which I can't have.\",)####(\"I fixed up the formatting, but do you really mean to have $C_0$ five times? This would be most unusual; I think you want $C_1$, $C_2$ etc.  Also, you didn't include an intercept; was that deliberate?\",)####('`library(car)` is necessary, for the logit function.',)####('In the part `samples from (Y,X1,....Xn) and then a bunch of conditions by which the OLS estimations behave quite well.` is that really what you meant or did some of your sentence get cut off. Also you have a misspelling in the title of the question.',)####('I also tried beta regression, but got an error...\\n\\n`library(betareg)`\\n`model5 = betareg(acc~scale(iq),subjdata)`',)####(\"@PeterFlom I think the OP is looking for some search algorithm that looks to return a normal distribution from the original data. I've always done this by hand visually inspecting the transformation result, the question is quite interesting.\",)####(\"The problem is that i have a separate model for each region of my country, so i can't inspect one by one. There must be some kind of test or package to automate this!\",)####('You should provide us with ALL the estimated coefficients form Java, then we might know what goes on.',)####('What do your outcome y and explanatory variable x look like?',)####('I put them here https://gist.github.com/4680507 Dimitriy',)####('I have updated as *EDIT1*',)####('It may help you to read two of my answers to related questions: [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523//30909#30909) (which discusses link functions & GLiMs in general--a comment at the end specifically addresses your 1 & 3), & [Difference between generalized linear models & generalized  linear mixed models](http://stats.stackexchange.com/questions/32419//32421#32421) (which discusses how your 4 is different from 1 & 3).',)####('Look into Box-Cox transformations, that should be at least a good start.',)####('Java code only regresses $y$ on $geno_A$, i.e. you get the same result with $lm(test_trait \\\\\\\\sim geno_A)$. I know next to nothing about Java but you probably need to use multiple regression command as opposed to SimpleRegression.',)####(\"But whatever the method you use (expertise included), you begin from 0, and you are 100% having the problem I talk about... It's like omitted variable bias is there every time\",)####('Also, my answer here: [identifying-the-population-and-samples-in-a-study](http://stats.stackexchange.com/questions/31488//32149#32149), is tangentially related, & may be of interest.',)####('thanks, I fieed the title (bias).',)####('Yeah, I meant that. You have the sample/observations, and then the conditions (Gauss-Markov), which guarantee the estimators to be the best unbiassed ones etc',)####(\"You are correct to be concerned.  A lot of inference is based on the assumption that we have the true model.  I've been running regressions a long time and I've never had the true model.  For my purposes it rarely makes sense to even think that one true model exists.  Instead, ask yourself what the goals of your modelling are (prediction in sample, prediction out of sample, estimating the average causal effect of x3, data summary, etc.) because your goals will indicate which modelling strategies are best.\",)####(\"As a side note, stepwise selection methods (such as forward stepwise) are very unlikely to pick out the model that you ought to be using. If this doesn't make sense, you may want to read my answer here: [algorithms-for-automatic-model-selection](http://stats.stackexchange.com/questions/20836//20856#20856).\",)####('Short answer: No.',)####(\"Only conclusion (3) is consistent with your data.  The rest are not. In (1), the number of increases is an exceptionally poor test statistic and so gives you no grounds for any conclusion for or against a linear relationship. In (2), the SD is actually quite *large* compared to the size of the slope.  (That's what a small $R^2$ is telling you.)  In (4), you are being misled by a single anomalous value (the first in the series).\",)####('Because the value of a beta can be anything (including of absolute value greater than $1$), it cannot generally be interpreted as a partial correlation.',)####('@JasonR Good to see you on here. Yes WLS is indeed an interesting contender. I have gone with whubers answer because of the clever polynomial factorization, and because it maintains the error structure nicely.',)####('Why do you call it \"linear\" if you are using a polynomial? Every point you want it to pass through is a constraint that will reduce your degree of freedom. You can then use a constrained optimization algorithm.',)####(\"It is linear because you are finding co-efficients to a *linear* combination. For example, if you want to fit your data to a cubic, then you are finding the co-efficients (the $c$'s) of $y = c_0 + c_1x + c_2x^2 + c_3x^3$.\",)####('@Mohammad: One other way to approximate what you want would be to use a weighted least squares solution, and give very large weights to the points that you want the regression line to pass through. This should force the solution to pass very closely to the points that you choose.',)####(\"Welcome to the site, @nkhuyu. I don't fully understand your question. Are you worried about multicollinearity, or are you worried about finding an incorrect sign due to sampling error alone?\",)####('It is multicollinearity.',)####('This thread may also be of interest: [when-and-why-to-take-the-log-of-a-distribution-of-numbers](http://stats.stackexchange.com/questions/18844/).',)####('We do not \"totally disregard\" that the coefficient is jointly normally distributed with the others. Just look at how the variance of each coefficient is obtained to see that. And, there are plenty of tests that take into account multiple coefficients. The standard $F$-test is one, [Scheffe\\'s method](http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method) is another, Wilks\\' generalized variance and ellipsoids of concentration yet another. It comes down to what statistical question you are interested in.',)####('You should clarify your question: Do you want a time series model? What is nma?',)####(\"polyfit gives the coefficients only if you supply a model too, i.e. the order of the polynomial. You can't get an analytically model from data completely blindly, you need to at least decide on a general form of a model before hand. All the regression can do is find the coefficients (or parameters) that make your model most closely resemble your data. If you want regression on data without specifying a model then perhaps you need a black box approach like neural networks?\",)####('@Dan: Given the question that was asked I think it is important to emphasize taht a neural network is no less a model than a polynomial, it just has more parameters. It\\'s a composition of linear combinations of sigmoid functions (typically). The \"backtracking\" algorithm is an instance of gradient descent in parameter space.',)####(\"@DCS: I agree with that. I guess it's just a case of how expressive you want your model to be. My interpretation of the question is that the OP wants to end up with a model that he can read an understand, NNs won't provide that which is why I called it a black box approach. But yes, it is just regressing a more complicated (more parameters at least) model, but a very general model that is likely to fit many shapes of data.\",)####(\"Do you mean the skewness and kurtosis of the dependent variable or of the *residuals*? If it's the former, your problem is complicated by the fact that those moments depend on the regression coefficients themselves and on moments of the IVs.\",)####('I guess my question is the follows, if my null hypothesis is that a particular beta is zero. How do I do it? Do I have to figure out the marginal distribution of that beta from the joint normal distribution?',)####('Yes, you do: It is $\\\\\\\\sigma^2 e_i^T (X^T X)^{-1} e_i$ where $e_i$ is the elementary vector corresponding to the $i$th coefficient.',)####(\"Thanks for the note Cardinal but can you please elaborate/derive that. In particular I am reading the book by Tibshirani et al, The Elements of Statistical learning. Seems he defines the z score as z = beta/sigma*sqrt(v), where v is the corresponding diagonal element of the XtX matrix. He says that the score should have a t distribution and uses it to test for the significance. I just don't see how we get to that result. And also why that z score should have a t distribution. How do we get to that score, is that the marginal distribution? Many thanks!\",)####('@cardinal I think with a little bit more explanation that would be an answer rather than a comment.',)####('ganesh, your most recent question in that comment looks like a new question to post (\"How is it that this scaled parameter estimate has a t-distribution under the null hypothesis?\") ... but it\\'s one you should search for answers to first. It relies in part on [this result](http://en.wikipedia.org/wiki/Student%27s_t-distribution#As_the_distribution_of_a_test_statistic).',)####('I think it would help to phrase the question without using the term \"significantly\". What do you really want to know?',)####('While @Glen_b gave you an excellent answer, I would like to note that if your $\\\\\\\\sigma_i^2$ (or the proportionality constants $\\\\\\\\tau_i$, in his notation) are not very accurate, then you may end up being worse off with your regression than you would have assuming equal variances. Econometricians have given this topic a lot of thought in the 1970s-1980s.',)####('Could you please provide context, details, and an explanation of what you mean by \"diverging\" and \"significantly different\" regression models?',)####('I think you might be interested in  [prediction bands](http://en.wikipedia.org/wiki/Confidence_and_prediction_bands)',)####('Re the edit: the answer depends on whether the two models have any data in common or not.',)####('@Glen_b - I added an example, thank you.',)####(\"Isn't this just a parallel slopes question? do you want to know if the slope is the same in the two groups? Just put an interaction term in the model, and look at the parameter test for the interaction term\",)####(\"Firstly, apologies I made the original post but made a hash of registering and now can't comment as the person who asked the question! Testing if the the slopes are different is not the same thing. The slopes are different but that doesn't tell me if the solutions are the same between 0< $x$ <10. Continuing from the example, I would like to be able to say at what temperature (x) there is a benefit in using material/case 1.\",)####('You may fix up your registration by merging all your accounts at http://stats.stackexchange.com/help/user-merge.',)####(\"You'd need to define what you mean by 'trend strength'. There are many possible interpretations.\",)####('(1) Could you please explain what you mean by \"input\"? What is the form of your model and what aspect(s) of it constitute \"input\"? (2) In ordinary least squares regression there is no penalty on (dependent) values, *per se,* but only on the *residuals.* In what sense can \"values\" (presumably *data*) be \"constrained\"?  They are, after all, your data! Are you proposing to change them or are you really talking about constraining the fit? Or maybe constraining any predicted values? All in all, it\\'s hard to guess what you\\'re asking.',)####(\"I am referring to the first part of his answer. I believe the reference to $R^2$ is there only because he was trying to get clarification of your original question: with your edits, it is evident that $R^2$ is not relevant (and may even give results you find to be erroneous.)  You are asking about the *size* and amount of *trend* and that is measured by the slope term: it's that simple.\",)####('(+1) The edited question is exemplary in how well it explains what is needed and provides a context for answers.',)####('@whuber - thanks, I hope somebody will answer my question, I am sure this is not so complicated issue for local statisticians.',)####(\"What's the matter with Peter Flom's answer? Your comments to it do not seem to acknowledge what he has told you, but instead ask about things that--even after your edits--appear to me to be irrelevant or extraneous to the question.\",)####(\"@whuber - probably I didn't understand Peter Flom's answer (R^2). I'll check it once more time (I deleted these comments)\",)####('Input in this case is a sparse vector of ~1500 binary variables.  The output is a number between 0-100.  Essentially I am attempting to \"learn to rank\" given a human generated ranking.  But, I never mentioned the inputs above.  Maybe you misread?',)####('\"Standard least squares regression\" is a *procedure,* not a problem. If you restrict yourself to a procedure that happens to be inapplicable or inferior for your data, you may get bad answers (or the good answers will try to make the same points I\\'m making). That seems to be the case here. By describing your *problem* and allowing respondents to suggest *solutions,* you are more likely to get useful and correct answers.',)####('Yes, I misread \"output\" as \"input\" (during multiple readings--a strange trick of the mind!). But although your problem is now clearer, many of my questions remain: it\\'s hard to guess what you\\'re doing when you have supplied such a broad and abstract description of your problem and it would be difficult (in any responsible manner) to suggest a solution without knowing more.',)####(\"It's just standard least squares linear regression, nothing fancy.\",)####('@tibL Yes, I meant proportional to the likelihood $\\\\\\\\times$ prior. I already change that part in my question.',)####('This sounds a question asking for re-assurance, and basically, yes you\\'ve got the interpretation correct.  Bayes is similar to the \"loss function\" + \"regularising penalty\" framework of machine learning.  In bayes you have log-likelihood and log-prior as the equivalents - but you also get accuracy measures as well as predictions.',)####(\"Yes, it has a statistical basis: and that basis shows your technique is not a valid way to identify outliers! If you use *Mathematica's* `NonlinearModelFit` procedure, it will give you influence measures, prediction confidence intervals, prediction standard errors, and standardized and studentized residuals: all of these are useful for identifying outliers (of various types).\",)####(\"Quick comment (I hope I haven't misunderstood your question): it is $\\\\\\\\text{posterior}\\\\\\\\propto\\\\\\\\text{likelihood}\\\\\\\\times\\\\\\\\text{prior}$ and not equal to. There is a proportionality constant that should not be neglected and that ensures that the posterior is a probability density for any likelihood and prior.\",)####(\"@probabilityislogic Yes, I'm looking for re-assurance and in the process, I'd like to get another detailed description to make sure I understand how it works. The part that I thought was a bit weird is that we have a likelihood with $\\\\\\\\bf{w}$ as unknowns, so it's not possible to solve it (although we can plot it for every value of $\\\\\\\\bf{w}$) but after multiplying likelihood by the prior, we obtain a probability $p({\\\\\\\\bf{w}}|{\\\\\\\\bf{t}})$ where is natural to ask for a given $\\\\\\\\bf{w}$ and get a number.\",)####(\"I am confused by your question because there don't seem to be any interactions that include your two covariates of interest.\",)####('Agree with @whuber, the trend strength is the slope. Maybe you  want to consider here how different the slope is from zero. You might want to do this by looking at the ratio of the slope estimate and its error. If you are familiar with R, this is the t value given by the summary of a linear fit. But this is different from the \"trend strength\".',)####('I have plotted the data, and I don\\'t see a straight relation; can you please elaborate on - \"It might give you some inspiration as to the kinds of formula you\\'d want for a4\"?',)####(\"Hey can you explain me why the solution here: http://stats.stackexchange.com/questions/53324/spotting-trends-in-time-based-data/53350#53350 doesn't bring you what you want ? (just curiosity)\",)####('Because you appear to be in an exploratory mode (\"trying to come up with a model\"), why not first use exploratory methods like loess smoothing?',)####(\"Sorry, I thought it might work, but I just tried and it's not even close. I've deleted my previous comment.\",)####('We can\\'t tell you if the model is \"right\" or not without more information about the goals for your analysis.  Perhaps you want to describe what you think the output of the regression mean and people can offer confirmation or correction.',)####(\"`nls` does non linear least squares in R. It is used differently from `lm` because the models aren't linear; you can't leave the parameters implicit. They appear in the formula. See `?nls`. There's also the possibility of using glms if your model has a linear predictor (i.e. a transformation of the mean of the response is linear in predictors). But I would agree with @whuber - if you don't have a specific functional form in mind, start with some exploratory tools.\",)####('I tried `gam`, and I am getting better prediction accuracy. This might be a dumb question - but I am getting this vibe that a linear model is generally sufficient for most cases, may be this is a topic for another question, but comments are welcome.',)####(\"You've got 167 variables and 35 observations.  You're problem may well have a solution, but it is likely to be a complicated one for someone who is unfamiliar with basic regression mechanics.  I'd suggest collaborating with someone with better statistical fluency, and taking the opportunity sometime to work with simpler datasets (many observations, few variables, normal distributions) to get you to a place where you'll be able to deal with more complex situations.\",)####('The answer appears in [your previous thread](http://stats.stackexchange.com/questions/54896/regression-on-means-of-log-transformed-variables): the methods are not equivalent. Consider, too, that a few thousand observations is by no means \"large.\" But your final question looks worthy of consideration: what should be done when you have so many observations that they are more than you need and more than your computer can cope with? The answers depend partly on the purpose of the regression: is it for exploration, testing hypotheses, prediction, or perhaps something else?',)####(\"The coefficients will all be estimates of the same population parameters, and averages of them would eventually converge to the whole-sample estimates but the p-values won't 'average' in the way you suggest. It's not clear quite what you intend by 'insignificantly different' though.\",)####('Basic question like this are almost always duplicates - [How can adding a 2nd IV make the 1st IV significant?](http://stats.stackexchange.com/questions/28474/how-can-adding-a-2nd-iv-make-the-1st-iv-significant) Also essentially duplicated [here](http://stats.stackexchange.com/questions/30363/factor-significant-within-model-but-non-significant-after-drop) and [here](http://stats.stackexchange.com/questions/26014/univariate-non-significant-results-becoming-significant-on-multivariate-analysis). That took approx 40 seconds.',)####('First question: Your method sounds like some sort of bootstrap; these methods are well developed and there is a huge literature on them. Probably the most common reason for using a bootstrap is that there is not a good existing estimate of the variance of whatever you are trying to estimate. Why are you suggesting this method instead of the usual one? What are you hoping to learn or gain from doing this? Second question: 1000 is by no means a \"large\" number of observations these days. If you had many millions or billions of observations, you might want to sample to cut down on running time, bu',)####('since you say that there is a curved relationship between the predictor and the response, have you considered a quadratic term in your regression model?',)####('+1, I think this is a good question. Note however, that some similar question have been addressed before on CV. It may be worth your time to [search the site for regression assmptions](http://stats.stackexchange.com/search?q=regression+assumptions) & see what already exists. I can recommend [what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression](http://stats.stackexchange.com/questions/16381/) & [what-does-having-constant-variance-in-a-linear-regression-model-mean](http://stats.stackexchange.com/questions/52089/).',)####('This sounds like a strange thing to want to do. WHY do you want to see if two series are \"significantly correlated\"? Also, time series are tricky; if you \"struggle with statistics\" you may want to find a consultant who knows more.',)####('You could interpolate using either nearest neighbor, linear, or spline/pchip.  You could use a Kalman Filter.  You could use a smoothing spline and AIC and interpolate in the spline.  What is the question that the correlation is going to answer?  I prefer to use MatLab but many folks on this site seem to prefer \"R\".',)####('I tried square and root transformations; but if you mean adding squares of predictors to the regression equation, then no.',)####('yes, if you see a curve linear relationship between predictor and response, then perhaps a model that contains a quadratic term, $X^2$ in one of the predictor terms would be a better fit.',)####(\"if transforming either leads to a reasonable description of the mean, I'd focus on which one did a better job of describing the variance. What are your responses and predictors measuring?\",)####(\"Can you write down what the physical background suggests the relationship is? Surely the physical background doesn't have an ambiguity about where the logs go!\",)####('My predictors are signals going through a medium with attenuation and response is the distance. Physical background of all that says that the dependence is logarithmic. @Glen_b',)####('@Glen_b\\tPhysics would lead to transforming predictors, however, then their distributions differ from normal, and R2 is less, regression is worse. I maybe would prefer to transform the response for better prediction (its distr. becomes closer to normal after transf.). So I want to understand _statistical_ rules for regression.',)####(\"So ... distance becomes left skew after taking logs? What is the functional form of the expected relationship of the original variables? [Note that regression doesn't assume the response is unconditionally normal, and it doesn't assume the predictors are anything.]\",)####(\"I wouldn't bother comparing these models until after checking their goodness of fit.  I think you will find in the second one that neither the response nor its logarithm are linear functions of time.  This calls (seriously) into question any comparison of the slope estimates.\",)####(\"what does the prime (') mean here? is that the derivative?\",)####('transpose of the matrix.',)####('and |X| is the determinant of X?',)####('That is correct',)####(\"When you say 'optimal', what is to be optimized? Is $\\\\\\\\phi$ to be minimized?\",)####('@Glen_b The transformed response is not skewed, almost normal (log with some empiriclly found coeff.). [The original disributions](http://pp.vk.me/c407129/v407129874/6c39/9NLhYpU0DjQ.jpg) (the last is the response). All predictors are in linear relationship between each other (highly correlated) and in nonlinear with the response [(like this)](http://pp.vk.me/c10418/u17020874/153949434/x_9898cf38.jpg)',)####(\"Thanks. However, there are no assumptions about the distributions of the predictors, so they may look like anything whatever, and it's not possible to tell from this whether the *conditional* distribution of the response is non-normal - you have there a plot of its unconditional distribution, about which no assumptions are made. In any case, if that was the conditional y-distribution, I would hardly be concerned by it at that sample size. I'd worry much more about likely heteroscedasticity.\",)####('@curious_cat homework tag is [deprecated](http://meta.stackexchange.com/q/147100/182549) on SO, no need for it here either',)####('I thought I had responded to this, sorry = $\\\\\\\\phi$ is to be maximised',)####('Self study? Homework? If so, tag it so.',)####('Typo - as you pointed out I should have said set the intercept to 100, not 0.',)####(\"When both $y_1$ and $y_2$ are linearly dependent on $x$, *of course* they will be correlated to each other by virtue of those relationships. What do you want to get out of your proposed model that isn't already produced by the two separate regressions?\",)####('Dose is continuous.',)####('Forget dose for a second. How would making the intercept 0 in that equation make the survivorship 100% in any case?? How is dose measured? Is it categorical with different levels?',)####('Is dose categorical or continuous?',)####(\"My initial reaction was to start with a `glm(..., family=poisson)`, b/c you have count data.  I would start with `summary(glm(Nodes_removed~(.), data=study_data, family=poisson))`, but I notice the QQ plot doesn't look so great (if you plot the glm() object you get diagnostics).  Also, there are a lot of terms in there, and I don't know what types of interactions etc. seem sensible.  With data like this, I would do careful thinking about what makes sense, b/c blindly including a lot of terms can be dangerous.\",)####(\"I'm not sure the sense in which you mean 'will also constrain the dose'\",)####('Survivorship = b1*time + b2*dose + b3*time*dose + 100\\ntherefore suvivorship only equals 100 when dose and time (or there coefficients) are equal to zero. I would like to be able to define the model such that the dose at the start of the experiment (time=0, survivorship=100%) is unconstrained.',)####('Since described feature is not linear, MLR cannot do this. In my opinion, survival analysis (e.g. fitting proportional hazard model) can solve the problem',)####('@ O_Devinyak thanks for your suggestion. I will look into this. Also, please can you explain why the described feature is non-linear. I do not understand how you have reached this conclusion.',)####('This looks like standard bookwork. If this is for some subject could you add the self-study tag please?',)####('Welcome to the site, @Jrod. Just for the sake of clarity, what do you mean by \"binomial\"? For example, do you mean that it\\'s *binary* (the only values that exist for this variable are $0$ or $1$), or that these are counts of \\'successes\\' out of some total number of trials with a given probability of success?',)####('Yes, I meant binary.  I have two different areas where the data was collected and have coded them as 1 or 0 and used it as an interaction term in my regression.  However the variable by itself had a significant p value in some of my analyses.',)####('I do have an interaction term.  The equation works out to be be y=x*g+a+g  where g is the binary variable.',)####(\"It doesn't constrain the dose - it constrains the effect of the dose. Your problem is that your concept is something like multiplicative in the hazard, but your model is not. Imagine some baseline survival curve against time, and them imagine the effect of dose... what should happen? That governs the effect. Regression is not directly a suitable tool for this kind of work; issues like censoring make it less suitable again. There are both parametric (e.g. Weibull models) and nonparametric/semiparametric (e.g. KM, proportional hazards) survival models.\",)####('I have just realized that to constrain surviorship to 100% would be incorrect as I do not know at what time point outcome moves below 100%. I will look into survival models. Thank you for you help.',)####(\"Hmmm. Can you edit your question? Do you have an interaction term in your model? If so, that's crucial information.\",)####('Confidence intervals for the mean difference.',)####('(+1) Thanks for your help!',)####('I have edited your title so it matches your question  - which seems to be about observations, not variables.',)####('Confidence intervals for what?',)####(\"I'm having trouble picturing what you're doing. Are you after a confidence interval for a coefficient in your model? If not, perhaps you could explain both the model and the source of the mean difference in the model.\",)####(\"Does this apply to models that don't use probit or logit? My example specifically uses two continuous variables.\\n\\nDoes scale still just account for heteroscedasticity?\",)####('Scale parameter makes it possible to account for nonhomogeneity variances http://stats.stackexchange.com/q/48237/3277',)####(\"Alright you got me, I was rushing to think of an example between lab work and got sloppy... pretend I had an example on hand that didn't simplify. I'm asking about a theoretical statistical method, not about a specific equation.\",)####('Specifying skewness and kurtosis is not sufficient to determine the distribution. Can you explain more?',)####(\"Furthermore, can you show what you've tried?\",)####('It is SD. I just want to know how to fit the regression line to the mean and SD of the points. Thx',)####('Hi @gung How did you make \"LaTeX\" look like that? It\\'s cool.',)####('@PeterFlom, $\\\\\\\\LaTeX$ is \"\\\\\\\\$\\\\\\\\LaTeX\\\\\\\\$\".',)####(\"That example is still readily simplified in a way that doesn't require $\\\\\\\\ln0$: $Y = B + \\\\\\\\frac{(M-B)}{1+(X/IC_{50})^H}$.\",)####('Is your question on how to *technically* plot the lines and points in `R`?',)####('Your title says \"SE\", while your text says \"SD\"; which is it?',)####('Welcome to the site, @user26091. I took the liberty of editing your question with $\\\\\\\\LaTeX$. Please make sure it still says what you want it to.',)####('Thanks!  It does not quite say what I want it to however but close.',)####(\"I thought there was something wrong with c, but I couldn't figure out what it was supposed to be.\",)####('Since it\\'s for the purpose of study, please add the self-study tag. I took a stab at improving \"c\" - if that\\'s wrong, please clarify.',)####(\"I think that works but I'm just trying to find notation for each random value minus the residual. And will add!\",)####(\"What's the matter with rewriting $\\\\\\\\exp{( \\\\\\\\ln{X} - \\\\\\\\ln{IC_{50}})}$ as $X \\\\\\\\exp{\\\\\\\\left(- \\\\\\\\ln{IC_{50}}\\\\\\\\right)}$? All your numerical difficulties vanish.\",)####('Can you not algebraically simplify your function? To: $Y=\\\\\\\\frac{IC_{50}M + BX}{IC_{50} + X}$',)####(\"I used that equation because I'm familiar with it, not because it's a specific equation giving me trouble. I've updated my question with a less convenient example.\",)####('Thanks for your answer! By SAR I mean \"simultaneous autoregressive model\", which is a type of linear regression model that accounts for spatial autocorrelation in the model residuals. Is there not something like an assumption that a continuous predictor variable is actually genuinely continuous?',)####('If, as seems to be the case, you\\'re talking about constructing an independent (predictor-, $x$-) variable, there are no assumptions you need to satisfy for OLS or GLMs. Your question \"or in general\" is unanswerable. I\\'m not sure what you mean by \"SAR\"; seasonal autoregression? something else? As for what reviewers might say, one could only speculate as to what your reviewers *might* come up with. It tends to be a bit of a mixed bag at the best of times.',)####(\"Yep, that's pretty much how it looks. But I really like your second approach of visualizing it, thank you!\",)####(\"But there's no assumption in OLS or GLMs that a predictor should be continuous; it is what it is and you condition on the observed values. If it were observed with error, that might be an issue; discreteness shouldn't be.\",)####('And should I worry about the scatterplot? Right now there are two vertical bars full of data points at zero and one and just a few data points in between. The regression line seems somewhat lost in that plot. Should I maybe instead report a boxplot based only on the datapoints that are either fully within or without a participating country even though I used intermediate values in the regression models?',)####(\"Do you mean that you don't think the scatterplot is very informative? (Does it look something like [this](http://i.imgur.com/wPjf2V4.png)?) As for what you should do instead, if you find a boxplot more informative, you could certainly do that, but there are [other possibilities](http://i.imgur.com/raBRe66.png) that may work well.\",)####('How many times did you re-run the test.  Replication can indicate variation (or consistency) in results.',)####('\"exponential\" usually implies something based on `exp()`: what you have here is more commonly called power function, power law, or scaling law. Other names no doubt exist. There is no connection with power in the sense of hypothesis testing.',)####('Oh. I have one response with more than one predictors.',)####('Please have a look at [this post](http://stats.stackexchange.com/questions/59784/regression-for-a-model-of-form-y-axk) that deals with a similar question. [This paper](http://onlinelibrary.wiley.com/doi/10.1017/S1464793106007007/abstract) might also be of interest.',)####('The fact that kNN can beat SVM is not really surprising, nevertheless I somewhat feel your problem will look way better after taking a log of decision (at least).',)####('As this is basically standard bookwork, please add the `self-study` tag. You might like to read its [tag wiki info](http://stats.stackexchange.com/tags/self-study/info).',)####(\"@mbq Sorry, I am somewhat confused. Do you mean that I should take the log of the prediction (SVM's output) before calculating the correlation coefficient?\",)####(\"The errors of your observations are function of a random variables (the y's) and are therefore themselves random. Conditional on X alone, they are not given.\",)####('Rather log the decision before both making a model and calculating correlation coefficient. If your values have a huge and skewed spread neither SVR optimization target nor Pearson correlation make any sense.',)####('Yes, I fully agree on that. But what you say works in theory. If I draw, say, 100 random samples of identical size from the same population, each observation error will be a random variable with (0, sigma^2). What if, instead, I only draw one sample? In that case, the mean of the error of each observation is the error itself. Is it clear what I am saying? So, what I am trying to understand is, how does a package like Stata calculate the variance-covariance matrix using only one sample drawn from the population?',)####(\"See @gung's answer [here](http://stats.stackexchange.com/questions/33433/linear-regression-prediction-interval). Nothing to do with heteroscedascity.\",)####('Check out the BTYD R package, which has some models for calculating CLV based on RFM statistics. http://cran.r-project.org/web/packages/BTYD/index.html',)####('Or even heteroscedasticity.',)####('You assume equal variances around the true regression line, not your fitted one.',)####('About the intuition behind question #2: I am wondering why knowing $\\\\\\\\sigma$ ought to change the df at all. Suppose you knew *all* the parameters of the problem: $\\\\\\\\beta$ and $\\\\\\\\sigma$. Obviously that does not change SSE, SSR, or SST (unless you used them to compute $\\\\\\\\hat{y}_i$ and $\\\\\\\\bar{y}$, but that would change their definitions). So how could *your* knowledge of the true parameters change the *sampling distribution* of the statistics (could this be a new form of ESP? :-)?',)####(\"Yeah, I already checked that. Great approach by Fader et al. However, I'm right now interested in regression-type models like the one described above to tackle this task.\",)####('Please spell out your abbreviations. Presumably, \"SSR\" is the sum of squares of residuals and \"df\" is degrees of freedom. If that\\'s so, then how do you determine that df = p-1? Usually it has $n-p-1$ degrees of freedom when a constant is included in the regression and there are $n$ data values. That leaves me wondering whether I have correctly understood your abbreviations.',)####(\"Oops! I'm so used to using SSR for Regression Sum of Squares and SSE for Error Sum of Squares that I forgot that these are not standard abbreviations.\",)####('How does time relate to your stated model? Is the $x$ variable time or is time yet another variable?',)####('@Nick Cox Time is the independent variable. The model should read $y=at^b + c$',)####('Looking at R-squared and your auto-correlated residuals, it seems to me that you are having here what is called as \"spurious regression\". These are more discussed in detail under non-stationary or unit root process.',)####('+1 for the provocative title and a followup that actually makes sense',)####('ANOVA and linear regression using dummies are identical, with identical assumptions.',)####(\"What's a 'predictory' variable? From your text, it actually sounds like you mean 'response variable'\",)####('yes ! response variable or independent variable . I am editing it . thanks',)####('Whoah. Response variable = dependent variable = y-variable. Independent variable = explanatory variable = predictor variable = x-variable. Which is it?',)####('please edit my question ! i am not comfortable with statistical terms . Thanks',)####(\"If I could tell for certain which kind of variable you meant where, I would do so. Consider the regression model $y = \\\\\\\\beta_0 + \\\\\\\\beta_1 x_1 + \\\\\\\\beta_2 x_2 + \\\\\\\\varepsilon$. The first variable, $y$, is the dependent variable, the response. The second and third variables, $x_1$ and $x_2$ are predictors, explanatory variables, independent variables. As it is right now, it looks like it's right, but you must check whether it says what you mean.\",)####(\"Is there are any particular reason you're suppressing the intercept by including `+ 0` in the formula?\",)####('What do you consider \"really weird coefficients\"? `*` means interaction plus main effects in formula syntax. Indeed factors are not something that can be multiplied. Think about what a factor actually is.',)####('This seems to be less a coding problem and more a conceptual problem. You need to clarify the statistical questions first and then get a handle on how to interpret interaction models... and looking at coefficients is NOT how one goes about accomplishing that task. These are issues that should be handled in a regression course.  SO is not set up to repair major gaps in your statistical education.',)####('Ok, so this is a regression model that I am following from a paper. It is not one that I came up with. The reason that time is being put into factor is there are several observations per day and to minimize the dataset, the series is put into fifteen minute intervals, afterwhich assessing time as a dummy variable - if the observation falls into that interval - tells you about the intraday seasonality for the day. Putting the firm into dummy variables tells you the firm effects and the influence it has on the model but you want to know how the firm interacts with time, hence the product.',)####('I have no idea what you\\'re asking. You want to fit a non-monotonic function of $x$... what exactly is your problem with polynomial regression or sine regression again?? Also... \"link function\"... you keep using that word... I do not think it means what you think it means.',)####('(1) Your `R` code has syntax errors: `group` should not be quoted. (2) The plot is beautiful: the red dots exhibit a linear relationship while the black ones could be fit in several ways, including a piecewise linear regression (obtained with a changepoint model) and possibly even as an exponential. I am *not* recommending these, however, because modeling choices ought to be informed by an understanding of what produced the data and motivated by theories in relevant disciplines. They might be a better start for your research.',)####('@whuber thanks! Fixed the code. Regarding theoretical motivation: where do these come from in the first place? My bench scientist collaborators will happily dichotomize the predictor variables and do t-tests on them. So it falls to me find a way to stop wasting data by finding a mathematical relationship that captures the transition from \"y correlates positively with x\" to \"y has little response to x\" to \"y correlates negatively with x\". Failing that, I\\'ll have to recapitulate what, e.g., Michaelis and Menten did when they found a relationship between enzyme, substrate, and product.',)####(\"Are the points where those things 'kink' known in advance?\",)####('(previous comment continued) so another way of putting my question is: what should I learn in order to be able to go from seeing a pattern in the data to a parametric model I can fit to future data and measure the effect of experimental variables on various parameters? What is the workflow from the exploratory phase (that apparently my subfield is largely in) to the hypothesis testing phase?',)####(\"@Glen_b no, the 'kink' points aren't known, and might not exist-- it might be some sort of continuous function. And if the kink-points do exist, there is no evidence that they would be the same under different treatments.\",)####(\"@JakeWestfall my problem with polynomial regression: `plot(lm(y~poly(x),updown))`, i.e. not everything that goes up and then down is a polynomial. Sine regression also gives odd residuals, but at least they're symmetrically distributed. My bigger problem with sine regression is that for non-cyclical data, it will make completely wrong predictions. And, if the data had ranged over a wider area of low response beyond the peak, the fitted curve would be a series of sine waves instead of a unimodal function!\",)####(\"My question is: Should I use partial R\u00b2 or the coefficients to show how much influence each factor has on the outcome? I was assuming both to point in the same direction. You are saying that's not true because there is multicollinearity in the data. Alright, so when I want to make a statement such as factor 'young' influences the result x times more/is x times more important than factor 'urban', do I look at partial R\u00b2 or coefficients?\",)####('I am going to word my opinions on this as an answer',)####('Is the extra structure scientifically or practically interesting or important? From the high $R^2$ you have it is presumably a smaller part of the structure than the overall shape. That might drive a decision to regard it as notable but not worth including in the model. On the other hand, approximate sinusoids often correspond to some overall intelligible periodicity (e.g. seasonality) which in turn may suggest a modification to your model.',)####(\"The extent to which this could be described as an experiment is an important detail. Even taking gender as given, area is standing proxy for an enormous number of possible influences including climate, nutrition, standard of living, age and race composition. (You don't say which country this is, but the bigger the country, the more the problems. I guess the US.) I think @Russell S. Pierce is hinting in this direction too.\",)####('Thanks Glen_b, I am delighted with the learning of types of variables in regression models andthe answer given below by Maaten buis made me clear the concept.',)####('Another terminology you might come across is \"regressand\" for the dependend variable (the $y$) and \"regressor\" for the independend (the $x$)',)####('@bioinformatician Here are lists of terms that may help you. Let\\'s start with synonyms for \"dependent variable\" = \"explained variable\", \"predictand\", \"regressand\", \"response\", \"endogenous\", \"outcome\", \"controlled variable\". Next are some synonyms for \"explanatory variable\" = \"independent variable\", \"predictor\", \"regressor\", \"stimulus\", \"exogenous\", \"covariate\", \"control variable\". Some of these terms are more popular than others across different disciplines.',)####('@Graeme Walsh This is Awesome ! I am  very happy today as i got to learn a lot that i could not do so in past couple of days.',)####(\"I don't really understand the problem. I am most familiar with your crop yield/water example. There are (often quite complex) models available that are based on micro-meterology, soil physics, plant physiology, nutrient supply (fertilization), and so forth. You find them in the literature. If you fit an empirical model it can only approximate such a complex model over a limited range. It may be that a linear model is appropriate to answer your specific question, it may be that you need to be able to model saturation (use an appropriate non-linear function).\",)####('What are you trying to do or show exactly? The estimated influence? The significance?',)####(\"Yes, I'm familiar with t- and F-tests. I'd like to show estimated influence, for which afaik t- and F-tests are not suitable.\",)####(\"I don't understand yet what you want to achieve. So you are trying to find the R\u00b2 of a restricted model with one less variable, correct? But I am not sure why you would use this to quantify the influence of the variable. The influence on the response is your parameter estimate, your partial R\u00b2 would be the cp reduction of residual sum, yes? But that would only work as a metric if there is no multicollinearity in the data, and there surely is. That is probably the reason why the variance of your variables does not relate to the reduction in residual squares\",)####('Sorry I just followed the description of the option `stdp` in `Stata`. It can be thought of as the standard error of the predicted expected value, mean or the fitted value.',)####(\"How would the regression output change if you were, say, to add $10^6$ to each `pop` value and add $-0.0116584\\\\\\\\times 10^6$ to each `fuel` value? Intuitively, that shifts the data far from `pop=1029` without altering the regression line and therefore should result in a much wider prediction interval. That means you can focus your research on those elements of the output that change.  (Even if you don't have the actual data you can make some up and run both regressions to see what happens.)\",)####('Thanks very much! Only the standard error of the intercept (therefore t, p-value and CI) changes. This inspired me to figure out that $Var(\\\\\\\\hat{\\\\\\\\beta}_0)=\\\\\\\\sigma^2(1/n+\\\\\\\\bar{x}^2/SXX)$, then I can get $\\\\\\\\bar{x}$ to calculate the standard error of prediction.',)####(\"The standard error of a *predicted value* isn't what you said. What you have there is the standard error for the mean at a given $x$.\",)####(\"@COOLSerdash Sorrt, somehow I missed the question about calculating the mean. If it's normal on the log scale, then conditioning on knowing the parameter values, you'd be computing the mean of a lognormal ($\\\\\\\\exp(\\\\\\\\mu + \\\\\\\\frac{1}{2} \\\\\\\\sigma^2)$). If you don't condition on at least the variance-parameter, the exponentiated estimate is instead log-t ... and then it doesn't have a mean.\",)####('Hint: what is the expectation of a constant?',)####('Could you add more details about your model? That would help us to answer the question. Here are some comments: Normally, you would just exponentiate the regression coefficient, so just $\\\\\\\\exp{(\\\\\\\\beta)}$. If the coefficient is negative, $\\\\\\\\exp{(\\\\\\\\beta)}<1$ and if the coefficient is positive, then $\\\\\\\\exp(\\\\\\\\beta)>1$. I think the interpretation is like this: the exponentiated coefficient is the multiplicative term to use to calculate the estimated dependent variable when the independent variable increases by 1 unit. In this case, the multiplicative term is $0.945$. See also [here](http://goo.gl/SJPWh).',)####('Thanks @Glen_b for the clarification. I will delete my comment and wait until the OP provides additional information about his goals. How would one calculate the mean?',)####('This question appears to be off-topic because it is about a statistical method and thus belongs on Cross Validated.',)####(\"@COOLSerdash - I think it should stay; it's a relevant comment, it just needs some qualification.\",)####(\"george, you can comment on your preceding question if you need further clarification or if you want to add details. You could then vote to reopen your post rather than reposting the same question. Please, tell us what's not clear with the possible duplicate, and we'll merge this question with your other one.\",)####(\"Thanks a lot @Glen_b. That's very interesting. Coming from epidemiology, I've never thought about Poisson regression in that way.\",)####('if $X = USD / SDR$  then the ln approach should give the expected result. The only flaw I can see if you have took the inverse exchange rate...',)####(\"They're both testing independence against dependence, but in different ways. DW explicitly tests lag 1 autocorrelation, the runs test is somewhat sensitive to that, but also responds to other forms of dependence. It's a bit like comparing a Jarque Bera test against a Shapiro-Francia test when you're interested in normality; they both are tests of normality against non-normality, but they respond differently to different aspects of non-normality. If you're *specifically* interested in what a more specific test looks for, that's a good thing, otherwise it might not be.\",)####('Thanks, @Glen_b! Is test for non-correlatedness viewed as a kind of test of randomness?',)####(\"While it's not usually referred to that way, obviously it is the case - clearly if its rejected the data are dependent.\",)####(\"@COOLSerdash Sorry, I must have missed something. My comments weren't intended to apply to Poisson regression, only to the case where you take logs and then fit a linear regression (thereby, I was thinking, assuming normality on the log-scale, at least of the coefficient-estimates). That action's not really appropriate for the Poisson case. Did I misunderstand?\",)####('@Glen_b Ah now your comments make perfect sense to me! I thought by \"log linear model\" the OP meant Poisson regression rather than a linear regression with a log-transformed outcome (I think Poisson regression is sometimes called log-linear model). But re-reading the question I think you are perfectly right in assuming that the OP meant a linear regression with log-transformed outcomes (otherwise he wouldn\\'t mention that the outcome was log-transformed, I guess).',)####('@COOLSerdash Yeah, I agree that normally statisticians would use log-linear model refer to a model whose linear predictor has a log-link (which is natural in the Poisson regression case), but as you note, the question says \"where the dependent variable is logged\", clearly suggesting modelling $\\\\\\\\log(y) = \\\\\\\\alpha + \\\\\\\\beta x+\\\\\\\\varepsilon$. Needless to say, I *don\\'t* think it\\'s a duplicate of a Poisson regression question, which would model $\\\\\\\\log(\\\\\\\\text{E}(y))$ as linear in $x$, not $\\\\\\\\text{E}(\\\\\\\\log(y))$.',)####('@Glen_b I totally agree and voted for reopening.',)####('X would be a number of 60 different currencies, expressed X/SDR. Is that what you mean? The correlation should be pretty much the same, no matter if take absoulut values or relative, right?',)####(\"Although written in a different context, it may help you to read my answer here: [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523//30909#30909), which contains a lot of information about what's happening in logistic regression that may help you understand these better.\",)####('The following paper by Ruppert (2002) might give you some help. D. Ruppert. Selecting the number of knots for penalized splines. Journal of Computational and Graphical Statistics, 11:735\u2013757, 2002.',)####(\"As a simple example where you have one explanatory and a dependent variable: A linear regression of the *ranks* of $x$ and $y$ would yield Spearman's correlation coefficient as regression coefficient.  And in this case, $x$ and $y$ are interchangeable in the regression.\",)####(\"Just a few thoughts. Kendall's $\\\\\\\\tau$ and Spearman's $\\\\\\\\rho$ are both correlation coefficients based on ranks. The sought after relationship between $x$ and $y$ would then need to involve their ranks. However, computing the ranks introduces dependence between the observations, which in turn imposes dependence between the error terms,  eliminating linear regression. However, in a different setting, modeling the dependence structure between $x$ and $y$ with copulas would make a link with Kendall's $\\\\\\\\tau$ and/or Spearman's $\\\\\\\\rho$ possible, depending on the choice of copula.\",)####('@bgbgh you can\\'t estimate the confidence set by estimating the confidence intervals individually, because it does not yield any \"global\" information: just imagine you were measuring something that is actually a ball. With only measurements along axes, you might claim that you were measuring a cube, which is incorrect.',)####('The equation for a logistic regression is $y=\\\\\\\\ln \\\\\\\\left(\\\\\\\\frac{p}{1-p}\\\\\\\\right)=\\\\\\\\mathbf{XB}+\\\\\\\\epsilon$. What you have written is not a regression.',)####('Concerning whether there is any other difference, does there need to be to make you lose interest in fitting a linear model to binary $Y$?  But there are several other differences such as the interpretation of $\\\\\\\\beta$ and the need to put restrictions on $\\\\\\\\beta$ (and to add complicated interactions that are difficult to interpret) to force the output be in $[0,1]$.',)####('Thanks for editing it. Equation is correct by the way. I was expecting different answers. One answer would be If one fits a linear reg instead then predicted response might be <0 or >1 which is clearly nonsensical as a response probability. I want to know your understanding to this. Is there any other difference?',)####('This model is often called the linear probability model and is much discussed e.g. in econometrics. It has many detractors, not least for the reason you cite, that it can return predictions outside the allowed range, and some defenders, not least because it sometimes fits about as well as other models more acceptable in principle.',)####(\"That's not correct; the equation for a logistic regression is $\\\\\\\\ln(\\\\\\\\frac{p}{1-p})=\\\\\\\\bf X\\\\\\\\boldsymbol\\\\\\\\beta$. There is no error term.\",)####(\"If this is for *self study*, you need to add the `[self-study]` tag, & tell us what you understand at present & where you're stuck.\",)####('I edited the question so that it hopefully gets to the actual question the user is asking.',)####(\"If you're not the one making a distinction, what reason to suppose there is one? I'd just call the latter a special case of the former ($q>1$). And note that often enough general linear models come with several more assumptions than a conditional expectation of zero.\",)####('What distinction are you making between \"general linear model\" & \"multivariate linear regression model\"?',)####(\"@Scortchi: general linear model doesn't assume anything on the errors, except their expectations are zero. I am not sure about how people generally assume on multivariate linear regression model?\",)####(\"When you do `as.numeric()` to a factor, it doesn't always convert it back to the original numeric values the factor labels are comprised of. For example, type `as.numeric( as.factor(runif(10)<.5) )`. You'll see a string of `1`s and `2`s, not `0`s and `1`s. That's what happened here. BTW, you may want to reconsider using OLS regression for a binary outcome. (BTW: I didn't post this as an answer because I think this question is off topic. This is purely a programming issue.)\",)####('http://r.789695.n4.nabble.com/Partial-R-square-in-multiple-linear-regression-td4632130.html',)####('@Macro: Sorry but I confused something. When I type `typeof(learn_set[,\"learn_IV\"])` I get `integer`, so it\\'s not like I\\'m doing `as.numeric()` to a factor. I will repost it later, thanks for the advice.',)####('@semibruin : My question is not about selecting the number of knots but the knots themselves. I would like an estimation procedure which would give me the points in the data where the slope of the line is changing.',)####('Can you clarify what you mean here? Eg, what do you want to do w/ this information when you get it? Are you wondering how to conduct the R-squared change *test*? Do you want to know the variance in `var` marginally associated w/ each `VAR`, or partially associated? etc.',)####(\"@Patrick: it's in the intercept, is that relevant here?\",)####(\"@Macro: Thank you for the explanation! That is some strange behaviour, `0` and `1` are are more intuitive. I didn't know it is a programming issue so it was safer to post it here I think.\",)####(\"@KarolPrzybylak, Fair enough. Glad I could help, anyway. Since your issue is resolved (and it turned out not to be statistical) this may be a situation where it makes sense to delete the question and  possibly repost it over on stackoverflow, if you think the question and answer will be helpful to the community over there. Of course, it's up to you. Cheers.\",)####('@Karol, I\\'m not sure what the function `typeof` is doing but it seems to always call factors \"integers\" and so does not preclude my solution. Based on the output you\\'re seeing, I\\'m sure what I pointed out was the culprit.',)####(\"The answers can be found at http://stats.stackexchange.com/questions/9131, http://stats.stackexchange.com/questions/44845, and http://stats.stackexchange.com/questions/16493 (even though they deal with prediction intervals). If that's not enough, more would likely turn up in a search for [regression prediction interval](http://stats.stackexchange.com/search?tab=votes&q=prediction%20interval%20regression). As far as the second part goes, it comes down to the difference between a standard error and a standard deviation: the standard errors vary with values of the IVs but the SDs do not.\",)####('I want to get the contribution of each variables VAR1, VAR2, VAR3, etc to explain var.',)####('Look up (1) Flexible linear regression, (2) Kalman Filters.',)####(\"You seem to be confounding errors and residuals together there. The assumptions apply to the errors, but the diagnostics are done on the available estimates of them (the residuals). You're interested in checking whether the entire set of errors are consistent with being sample from $N(0,\\\\\\\\sigma^2)$\",)####('The issue with errors versus residuals aside, I think you also need to ask yourself why does the QQ plot tell us that the residuals follow a normal distribution. Think about what it means to be on the straight line in the plot.',)####('I think you should be using NB or SVM instead.',)####(\"If I'm understanding you correctly, your line shouldn't be a worse fit at the ends (at least not as much as this).\",)####(\"I'm pretty sure this has a duplicate somewhere. Ah, found it. Not quite an *exact* duplicate, but the approach [here](http://stats.stackexchange.com/questions/44246/nls-curve-fitting-of-nested-shared-parameters/44249#44249) should work\",)####(\"Ok, If I do that approach I can combine all my data into a single fit. I still don't know how to determine whether the coefficients are different.\",)####(\"There's a couple of ways that should work. Oh, actually, rather than try to type it all in a comment, see for example, pages 16-17 [here](http://stat.ethz.ch/wbl/nlreg). There are other tests, but hopefully that will assist you enough.\",)####('Could you add a plot of your regression against the original data? (by the way, you can also directly use `model$residuals` or `resid(model)` to find the residuals)',)####(\"This answers the question! Don't think the site will let me accept a comment as answer though.\",)####('[This post](http://stats.stackexchange.com/questions/18606/does-it-make-sense-to-study-plots-of-residuals-with-respect-to-the-dependent-var) discusses the question, if it makes sense to study the residuals vs. the original variable (outcome).',)####('You can either (i) fit two models, one where prep2 and prep3 have the same dummies, and one where they have different dummies and compare them using an F test via the information I indicated, or (ii) reparameterize so that you have a single dummy representing the contrast (that difference in the two preparations) and test it via a t-test or F-test. (Well there are other ways but those should work okay). Failing that, if you post some data I could see if I could explain how to do it using your data as an example.',)####(\"I am not sure what is on those pages that is relevant to what I'm trying to assess, but on page 13 and 14 there is an example using the puromycin dataset that is helpful. I can add one or two explicit difference variables to the model \\n\\n`V~(Vm+deltaVm1.2*indicator2+deltaVm1.3*indicator3+S)*S/(Km+deltaKm1.2*indicator2+deltaKm1.3*indicator3S)`\\n\\nand use dummy coding to determine the difference that going from prep 1 to prep 2 and prep 1 to prep 3 produces. But I'm still missing the comparison from prep 2 to prep 3.\",)####(\"On second thoughts, Yashka, I hadn't picked up before that you were actually doing a mixed model. I'm not sure that I understand enough about nonlinear mixed modelling to write a reasonably intelligent answer. So I'll leave it alone now that I get that. Hopefully someone with more experience with nlme's will respond.\",)####(\"Since the previous answer I first pointed to didn't solve it, I will expand a little on the above and post it as an answer.\",)####('Why would you consider least squares then, rather than say a GLM?',)####('Heteroskedasticity is present, no linear relationship (partial residual plot), and non normal error terms.',)####('because my response variable is continuous.',)####('violate in what way?',)####('So... kind of like all the neatly available **continuous**, non-linear, non-normal, heteroskedastic GLM models then? Gamma, Inverse Gaussian, Tweedie, ...',)####('\"Normalization\" typically means changing values to fall within a given range such as $0$ to $1$ or $0$% to $100$%.  You describe *standardization,* which consists of a linear transformation making the mean equal to zero and the standard deviation equal to unity.',)####('It is very unclear why it is worth more than a few seconds of your time to deal with standardization.',)####('This reads like a standard textbook question. In what context does this question arise (i.e. what are you doing that it would lead you to ask this?)',)####(\"The trick is to rewrite $(y-X\\\\\\\\beta)^T\\\\\\\\Sigma^{-1}(y-X\\\\\\\\beta)$ as  $(y-X\\\\\\\\hat\\\\\\\\beta+X\\\\\\\\hat\\\\\\\\beta-X\\\\\\\\beta)^T\\\\\\\\Sigma^{-1}(y-X\\\\\\\\hat\\\\\\\\beta+X\\\\\\\\hat\\\\\\\\beta-X\\\\\\\\beta)$ and to then rearrange terms. I'll show you how if you still have not figured it out later.\",)####('@BabakP Thanks. I will work on those lines.',)####(\"If you fit a model of a straight line with constant gaussian error to a bounded variable, obviously this is going to happen. The correct way to deal with it is not to fit a model you know to be wrong, but one that's at least plausible. If y's can only be positive, you don't fit a model that says they can be negative.\",)####('Alternatively, you just expand out $(y-X\\\\\\\\beta)^T\\\\\\\\Sigma^{-1}(y-X\\\\\\\\beta)$ as $y^T\\\\\\\\Sigma^{-1}y -y^T\\\\\\\\Sigma^{-1}X\\\\\\\\beta-(X\\\\\\\\beta)^T\\\\\\\\Sigma^{-1}y+(X\\\\\\\\beta)^T\\\\\\\\Sigma^{-1}X\\\\\\\\beta)$ then simplify and complete the square in $\\\\\\\\beta$, which then gives you result of the form $(\\\\\\\\beta-\\\\\\\\hat\\\\\\\\beta)^T\\\\\\\\text{<something>}(\\\\\\\\beta-\\\\\\\\hat\\\\\\\\beta) + S$ where $S$ doesn\\'t contain $\\\\\\\\beta$, and where the form of $\\\\\\\\hat\\\\\\\\beta$ and \"<something>\" are obvious by inspection of the expanded terms',)####('If you want your account deleted just flag the question and leave a comment to the moderator to delete your account....',)####('The answer at http://stats.stackexchange.com/a/1448 includes an illustration showing this result is merely the Pythagorean Theorem.',)####(\"Aren't you integrating over $\\\\\\\\mathbb{R}^p$?\",)####('@Stijn Yes, thank you for pointing out this typo.',)####('You cannot quantify the power until you have proposed a specific and quantitative alternative model.  *All* discussions of power occur (necessarily) within such a context.  Unfortunately, \"correlated with $x$ by $r$\" is too indeterminate to be of use.  For instance, $x^{*}$ could be $y$ itself!',)####('Thank you so much everyone this really helped me out (to Comp_Warrior for editing it and for those who answered). This was a question in the lecture notes I had and I tried for about an hour to get it but because the textbook I use focuses more on the estimates and not true value of a regression I was getting confused. Thanks again.',)####(\"(Note that the p-value at the bottom is the p-value of the null that all the coefficients - bar the intercept - are zero.) For a clear illustration of the danger of just using regression output like the above, try working your way line by line through the example code at the bottom of [`?anscombe`](http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/anscombe.html) (i.e. just cut and paste one line at a time into R and see what it's doing).\",)####('Do you mean \"[coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination)\" for \"correlation\", & \"[multiple regression](http://en.wikipedia.org/wiki/Multiple_regression)\" for \"multivariate regression\"?',)####(\"Yes.  (My bad on $r^2$ but isn't multiple regression the same as multivariate regression?)\",)####('No; multivariate regression means a multiple response (target or outcome or dependent variable). Having multiple predictors (independent variables) does not itself make a regression multivariate.',)####('Your question is puzzling. If the predictor in bivariate regression has been standardised, then its coefficient equals the correlation: this is in essence an inevitable consequence. If not, then not in general. The way to think of this is in terms of units of measurement or dimensional analysis. A correlation, and hence its square, has no units, but a regression coefficient has units (units of y)/(units of x). Standardising washes out both units and leaves you with dimensionless numbers.',)####(\"Ok so you are saying it's not possible, with standardized data, to have large $\\\\\\\\beta$ and small $r^2$, because they are both the same thing - a measure of effect size.  $\\\\\\\\sigma_\\\\\\\\beta$ meanwhile tells me the significance.  Thanks btw, learning a few things here :)\",)####('No; as already pointed out correlation is $r$ and so $r^2$ is different in general (except for $r$ of 0 and 1). And no; the slope has nothing to do with significance. This should be easy to see in your favourite software. Standardise variables, do a regression, look at the correlation, look at the slope. Square the correlation.',)####(\"Just tried this - and it's not working for me - please see my edit\",)####(\"I can't see what you are doing wrong. If you can post your data, we can check. Otherwise see my answer.\",)####('Usually, when people use transformations to improve characteristics of the fit (like the normality of the residuals), the transformation is applied just to the LHS / $y$. Of course you *can* transform the RHS too, but that typically addresses different issues, such as curvature in the data that the model is missing. However, your non-linear model ought to be flexible enough to capture whatever curvature is there, I would think. Note also, that there is no reason the *same* transformation needs to be used on both sides.',)####(\"You can't really assess the suitability of the regression assumptions from this output; try [`plot(reg1)`](http://stats.stackexchange.com/a/65864/805) (that's a clickable link, by the way) as a first step. As for what a p-value means, try [this](http://stats.stackexchange.com/questions/14928/meaning-of-p-values-in-regression) (the first sentence [here](http://en.wikipedia.org/wiki/P_value) has a basic definition).\",)####(\"Sorry, that's a typo. It should range starting from 0.\",)####('An alternative is to revisit the model.  Do you perhaps need another additive term, say $(a x_1^C + b x_2^d) x_3^e + f$ ? The transformation you have maps $(0,0,0)$ to 0; but you say that $y$ is at least 10...',)####(\"I guess my other question is, given my nlsResiduals plot looks the way it does, should I be using a transformation? I'm not interested in confidence intervals (which, I think normally dist'd residuals are required for), but just the coefficients for the equation to use as they are.\",)####('Thanks for the answers! \\nI will take a look at the Doornik-Hansen test, the scatterplot and the normality plot. But will mostly consider the scatterplot.\\n@pontikos I will come back with the data as soon as I have them!',)####('the middle term should be $e^{Bz}$',)####('I think you are supposed to propagate the uncertainty in the fitted values of a and b (i.e., a-hat and b-hat), not the parameters a and b (which have no uncertainty) and the random error term from the formal model.',)####('Could you please clarify what you mean by \"total uncertainty\"? Are you referring to some measure of variation in $y_i$ or in the estimated parameters $\\\\\\\\hat{a}$ and $\\\\\\\\hat{b}$? What measure of variation are you using?',)####('Have you tried simply checking the correlation between the variables? Not meeting assumptions usually reduces power, but if you have statistically significant results, that might be a good start.',)####('You may want to read these threads: [pearsons-or-spearmans-correlation-with-non-normal-data](http://stats.stackexchange.com/questions/3730/), & [is-normality-testing-essentially-useless](http://stats.stackexchange.com/questions/2492/).',)####('Could you show us your data? With such few points it very hard to test for normality (to illustrate this generate 18 numbers from normal distribution, does the data look normally distributed? probably not).',)####('Welcome to the site, @pontikos. This is not an answer to the OP\\'s question, it is a comment. Please only use the \"Your Answer\" field to provide answers. I recognize it\\'s frustrating, but you will be able to comment anywhere when your reputation >50. Since you are new here, you may want to read our [about page](http://stats.stackexchange.com/about), which contains information for new users.',)####('Boris, it is not necessary for *either* variable to have a normal distribution and therefore a test of normality would be of little use. What matters most is that on a scatterplot the data appear to trend along a line and there are no extraordinary (\"outlying\") excursions from that line. A secondary issue is that the scatter of the data around the line should be approximately cigar-shaped or (American) football-shaped. All these can be checked simply by looking at the scatterplot.',)####('Could you please give an example of such an alternative model?',)####(\"If you have uncertainty in your $x$'s, you generally shouldn't use ordinary linear regression because it's biased (though small uncertainties will result in small bias; maybe you don't care so much). Are the uncertainties in your y's always constant or do they vary? How are you fitting your line?\",)####('One alternative is $y=\\\\\\\\beta_0+\\\\\\\\beta_1x+\\\\\\\\beta_2x^2+\\\\\\\\varepsilon.$  This is often used for checking linearity. It is almost never proposed as a \"perfectly right\" or even a realistic model--but by comparing the two models and finding that the data are more likely in the alternative model than the original, we have some evidence that the original is *not* very good (we conclude its fit is bad). Obviously there are many other possible alternatives, depending on *how* the fit could deviate from linearity. (Your question seems to assume the alternative $y=\\\\\\\\beta_0+\\\\\\\\varepsilon$.)',)####(\"I'm mainly asking about uncertainty in y's. But I'd be happy if the solution would consider uncertainty in x's as well.\\n\\nGenerally they aren't constant, but a solution that requires them to be constant would be fine as well, I tried fitting with linearfit, fit, findfit in mathematica and linest (and a custom function that I found that weights values by uncertainties) in excel.\",)####('Could you explain how that would relate to the first sentence of my question please?',)####('Can you explain as clearly as possible how the \"uncertainty\" values would be related to what would happen under say a repeat of the experiment? i.e. what, precisely, do these uncertainties represent?',)####('Err for example if y is weight, but the digital scale is only accurate to +-6. (Not the best example with these values, but for example when scale shows 255g it could be 255.0 or 255.9)',)####('Upvoted your comment, but experimentalists are even crazier than I thought if this is syntactic sugar for them! Which version is more intuitive.... ANOVA hypothesis test on $\\\\\\\\beta$: is the ratio of explained variance to the unexplained variance sufficiently high? T-test on the $\\\\\\\\beta$ term of a regression model: is the effect of $\\\\\\\\beta$ sufficiently different from zero? And, with the latter formulation you also get the direction of change. And, if you had to transform the data, you can back-transform the parameter estimate into a physically meaningful quantity. Unlike SS.',)####(\"What I am asking about is that I am trying to identify your model (in the probabilistic sense) for what the numbers we get (both the observations and the uncertainties) are, in the sense that if the whole experiment were repeated (but with the same $x$'s), what the distribution of the observations might then be.\",)####('Let me see if I can rephrase your question (please tell me if I am misinterpreting it).  You have the following equation $$Y=aX+b+\\\\\\\\varepsilon$$ and you basically want to calculate the standard deviation of $\\\\\\\\varepsilon$?',)####(\"I think what you're looking for is in my answer here: [How to generate correlated random numbers (given means, variances and degree of correlation)](http://stats.stackexchange.com/questions/38856//38867#38867), which both explains the basic idea conceptually, & includes some custom R code & discussion of the standard function. If there's anything you're asking about that isn't covered, edit your Q to clarify what you still need to understand.\",)####('As I said, if done exactly the same the measurement would be the same. The real weights 11.63g, 23,265g and 34,9g would still be measured as 10g, 20g, 30g.',)####(\"I'm not sure if I really understand you, example would be measuring density, where you would have volume on x axis and weight on y axis.\\n\\nFast example: Let's say real weights are 11.63, 23.267, 34.9. But our bad scale can only show with +-5 accuracy, so it shows 10, 20, 30. (Volumes are 1*V0, 2*V0, 3*V0). If we try to fit with a program, it will show density of 10/V0 and 0 error. But the real value is 11.7.\",)####('Can you clarify what you mean by *\"My data does not show linearity\"*? Depending on what you mean, it may (or may not) be a violation of the assumptions of your model.',)####('SPSS makes this *much* harder to do than R or Stata, but a regression spline is one of the best general purpose solutions.  As a side question what made you pay $ for statistical software?',)####(\"@FrankHarrell As much as I dislike the business strategy of SPSS, I have to frankly say that it's relatively learner-friendly: interface is simple and functions are well categorized. Plus some lecturers/teachers specify the software to go with their class, it does not have to be Lauren's own decision to pick up SPSS.\",)####(\"I would hazard to say that it is learner-friendly only because it is restrictive and doesn't implement best statistical practice.  If it doesn't make regression splines easy then watch out.\",)####('So these uncertainties are instead some kind of truncation/bias? Is there any other consistency to their behavior? (say, if I measured an object of say 11,62g would I also be certain to get 10g?)',)####('That would be a bad fit. Why would you ever do that?',)####(\"I think there's a minus sign missing in the exponent, because the inverse logit function (a.k.a. the logistic function) is $1/(1+e^{-x})$.\",)####('Linear regression coefficients may serve as a good initializer for logistic regression, but as @whuber said is right.',)####('why a bad fit ?',)####('I think what @Memming means to say is that linear regression already supplies *some* kind of fit in the form of $\\\\\\\\beta_0 + x_1\\\\\\\\beta_1 + \\\\\\\\cdots + x_i\\\\\\\\beta_i.$ Applying the logistic function to these values is *really* going to screw up that fit, likely (but not invariably) making it worse. You can find out more by searching our site for (high-voted) answers about logistic regression and its interpretation: there are lots of them that should give you the insight you seek.',)####('can you give me a link? please',)####('A recent one is at http://stats.stackexchange.com/questions/69820 (I remember this because I provided an answer :-).  For others, you could begin with a [search on \"logistic\" and \"regression\"](http://stats.stackexchange.com/questions/tagged/logistic+regression?sort=votes&pageSize=15), but you\\'ll have to wade through many of them because (unfortunately, IMHO) they often have a strong `R` orientation which makes them readable only by `R` aficianados.',)####(\"Great thanks! That's what I needed to know.\",)####(\"There's no cut-and dried answer.  If you're serious about learning to handle a variety of situations like this, you'll want to get familiar with the literature on transformations, exploratory data analysis, and ways to use linear regression flexibly (e.g., making use of partial plots; squared or interaction terms; etc.).  Eventually you may want to look into more advanced topics such as regression splines.\",)####(\"To get some insight, it sounds like you are supposing the data $x_i=f(t_i)$ have a particular functional form; namely, one in which the intercepts vary linearly with $t$. Because the intercept is $f(t)-f'(t)t$, we can differentiate it to obtain the slope $\\\\\\\\beta$. The derivative is $f'(t)-f''(t)t-f'(t)$ = $-f''(t)t$, implying you believe $f''(t)\\\\\\\\approx -\\\\\\\\beta/t$ for a range of $t$. Integrating gives $f(t)\\\\\\\\approx C_0-C_1t-\\\\\\\\beta(t-1)\\\\\\\\log(t)$. Is this consistent with how you are thinking about your data?\",)####(\"I'm guessing there isn't anything left to this question that hasn't been answered, so we will close this Q (if not, you could always edit it to clarify what you still need to understand & we could re-open it).\",)####('composite scales',)####('Just to clarify, are you talking about single items with 4 and 7 response options respective or scales that are the result of taking the sum or mean of a set of items where each item happens to have 4 or 7 response options?',)####('Yes sir I am talking about the same scenario.',)####('which one: individual items or composite scales?',)####('For future reference, please be aware it is both impolite and against site policy to crosspost simultaneously at different SE sister sites. Choose the best one and, if it later turns out that it will get more or better attention at another site, use the migration facility. Cheers.',)####(\"I'd start out by writing out what $L^TL$ is in terms of $X$ and $y$, then noting the dimensions of the various elements, and trying to figure out what the determinant is.  If you don't know how to do this latter, there are well-known (ok, well-known to a very, very small fraction of the population) formulae for the determinant of block matrices with various structures that you can find on the web.\",)####(\"@jb I believe this formula has a simple, beautiful geometric description.  The LHS is the squared length of a vector orthogonal to the space spanned by $X$.  Multiplying by the denominator on the RHS therefore gives the squared volume of the hyper-paralleliped spanned by $X$ and $y$. But that's precisely what the numerator of the RHS is, QED. (This generalizes the familiar theorem of Euclidean geometry: the area of a parallelogram is its height times its base. Turned around, it says the squared height equals the squared area divided by the squared base.)\",)####('@whuber Interesting interpretation! Thanks!',)####(\"@whuber - that's pretty cool, I have to agree!\",)####('At this link, http://en.wikipedia.org/wiki/Determinant#Block_matrices  you can find why this relation holds, and also what references you should look up in order to be able to prove it.',)####('No I am not assuming that the sequence of intercepts has any functional form. Local linear regression is non-parametric. Also $y$ is the dependent variable and $x$ is the independent variable, so we are working with $E(y| x = x_i)$ as a function of $x_i$.',)####('Please edit your question to explain your notation and terminology. What are $x_i$ and $y$ and how are they connected? What exactly do you mean by \"intercept and slope of the values of $y$ centered at that point\"?  What is the \"sequence of intercepts\"?  A small worked example or illustration might help people understand what you are doing.',)####('Closing this thread as there is an accepted answer on another SE site.',)####('I understand local linear regressions, thank you. What I do not understand--and I suspect few others understand either, given there are no responses to this question so far--is *your particular terminology*. A local linear regression is used as a smoother (or \"estimator\" of a curve); thus, it does not (in any natural sense) have any \"intercepts\" at all, nor does it have any \"slopes\": it fits values to a set of specified independent values. My first comment represented my initial guess concerning what you might possibly mean; evidently I did not read your mind correctly. I still cannot.',)####('To @whuber: Please google for \"Local linear regression\". You will find a Wikipedia entry and a number of technical articles which explain the terminology. With reference to the first two articles, the sequence of intercepts is represented by the function $g(s)$, and the slope by \\\\\\\\beta. It is clearly stated in the second article that \\\\\\\\beta estimates $d g(x)/dx$.',)####('A little sample data would help people illustrate potential solutions.',)####('You can ensure positive predictions by using a generalized linear model with logarithmic link function. By the way, although your $R^2$ value is quite encouraging, a better check of whether the model follows the main shape of the data is a plot of residual vs predicted. Plots of observed vs predicted may also help illuminate your problem.',)####('@NickCox gave one suggestion. I would plot the data in more ways than just residual vs. predicted. However, you can certainly rescale money variables. One common method is to take log(cost) as the dependent variable. (I think this winds up equivalent to the log link function, but might be easier to comprehend). Log(cost) can, of course, be negative. And logs of money variables are often sensible because, e.g. a difference between 0.01 and 0.02 per click is important, but difference between 1.01 and 10.2 per click is not.',)####('@Peter Flom I think meant 1.02 not 10.2.',)####('Yes, you are being specific, but your specific question is covered by all the answers to the duplicate: you are testing the hypothesis that the intercept equals zero. That is well defined and specific.',)####('Look into equivalence testing.',)####(\"You can't *show* that it's 0, since it can be arbitrarily close to zero while being unequal to it. e.g. if $b_1 = 0.0000001$ then $b_1\\\\\\\\neq 0$ - and you'd ideally reject that point null, yet with reasonable values for and moderate sample size (and for the disposition of the $x$'s I guess), you can't. Peter's suggestion to consider equivalence testing is a good one (but it's showing something a bit different from what you're asking).\",)####('Thanks @PeterFlom, equivalence testing is what I was looking for. If you write it as an answer I will gladly accept it.',)####('I think there\\'s an overlooked distinction between the most up voted answer in the purported duplicate, and the answer sought here.  The \"dup\" provides an understanding of the p-value, whereas this question seeks an answer that is an example of precise wording of the interpretation of the p-value.  A concise answer, such as that in the comments here might be valuable to some practitioners.',)####(\"I am asking this question because this scenario arises in Genome Wide Association studies, very much in the mode in today's genetics. We have chips for getting genotype info at different spots of DNA over the whole genome, but we don't need all of them, because in every small region there are those highly linked (correlated) spots, thus for every such region we only pick one spot, to save time and money.\",)####('The theory is that if one of the spots is related to a disease, then even if we only have info on another spot nearby, we should be able to detect that this region is related to the disease through that other spot which is included in our testing chip. This is intuitive, but not at all rigorous. I tried to find out an explanation in the literature with no avail. I am thinking this could be the topic of an interesting paper. But maybe someone has published something somewhere not so easy to find.',)####('\"*The negative slope indicates that the values are decreasing together negatively*\".  If they \\'decrease together\\' (i.e. one decreases when the other one decreases) they\\'d have a *positive slope*. You mean that one *decreases* as the other *increases*, which is the opposite of any sense of \\'together\\'. Adding \\'negatively\\' to the end of that doesn\\'t serve to make it less confusing.',)####('I would add the regression line to them.',)####(\"I tried to resist but could not.  Have you read Arthur Goldberger's wonderful chapter on micronumerosity?  It's quoted in full in this blog post:  http://davegiles.blogspot.com/2011/09/micronumerosity.html\",)####(\"Why just the log?  Shouldn't this question apply to any data transformation technique that can be used to minimize the residuals associated with mx+b?\",)####(\"Your initial comment about resisting commenting seems to suggest that you think my research into ridge and other regularization methods is misguided? I have read about micronumerosity in the course of my research. Unfortunately I can't get any more data for a given day, but I can add data in the bayesian sense by building priors based on previous days observations. Could this be used to address micronumerosity?\",)####('The trace is invariant under cyclic permutations.',)####('[$tr(AB)=tr(BA)$](http://en.wikipedia.org/wiki/Trace_%28linear_algebra%29#Trace_of_a_product)',)####('@Glen_b Thank you :)!',)####(\"No, the point of Goldberger's chapter is that multicollinearity is not a problem to be solved via statistical technique.  Just like micronumerosity is not a problem to be solved via statistical technique.  The standard errors are big because your data don't reveal the thing you are interested in, not because you analyzed them incorrectly.  Of course, you can always make the standard errors smaller by bringing in outside information, but then it's the outside information not the data which are identifying the parameter(s) of interest.\",)####('Understood but its not just that the standard errors are large the coefficients themselves tend to unfeasibly large and offsetting. Although these numbers provide the BLUE fit they are pretty much non-nonsensical in terms of real life values there are supposed to represent + I would not trust and prediction made using these extreme values. I do take your point though that the problem would probably disappear if I could just get more data.',)####('I understand now.  When you have multicollinearity, usually you can make pretty precise comparisons and good predictions as long as you make them \"with the grain\" of the multicollinearity.  For example, suppose that X3 is always pretty close to equal to X4 in the data.  Predictions at points where X3 and X4 are pretty close to equal will usually have pretty low variance and be reasonable.  Similarly, if you want to measure the effect of X3 and X4 rising together by one point, you will get precise estimates.  It\\'s only if you try to ask a question the data can\\'t answer that there are problems.',)####('To use an example I always use in class, suppose you have a sample of shoe factories with variables for total production cost, number of right shoes, number of left shoes.  You can get excellent, plausible estimates of the extra cost from producing an extra pair of shoes and excellent forecasts of how much a shoe factory producing 10000 pairs of shoes will cost to run.  But, your estimates of how much it would cost to produce 1000 left shoes and 6000 right shoes will be terrible, because nothing like that ever happens in the data.',)####(\"student; ocram beat me to the punch, with an even more general result (though it's a consequence of the one I mentioned, since you can replace either $A$ or $B$ with arbitrary products and get the result that it's true for cyclic permutations)\",)####('One rule of thumb: use enough points that it looks like a smooth curve when you plot it.',)####(\"@whuber $\\\\\\\\hat{y}=Z \\\\\\\\hat{\\\\\\\\beta} + \\\\\\\\hat{\\\\\\\\epsilon}$. The epsilons are the error terms and $\\\\\\\\sigma^2$ is variance. I still however don't know why this holds...\",)####('please define $r$, $S_x$ and $S_y$.',)####('This certainly looks like a homework question. Please add the \"self-study\" tag. See [homework questions](http://meta.stackexchange.com/questions/10811/how-do-i-ask-and-answer-homework-questions/10812#10812)',)####(\"@PeterFlom I've added the tag. It's not homework but part of my exam-preparation.\",)####('Please tell us what $Z,$ $\\\\\\\\varepsilon,$ $\\\\\\\\hat{\\\\\\\\varepsilon},$ and $\\\\\\\\sigma$ represent.  In particular, what formulas do you know that express $\\\\\\\\hat{\\\\\\\\varepsilon}$  in terms of $\\\\\\\\varepsilon$ and $Z$?  (In so doing, I suspect you will find the answer to this question yourself.)',)####('That\\'s because you expressed $\\\\\\\\hat{y}$ rather than $\\\\\\\\hat{\\\\\\\\varepsilon}$ in terms of $Z$.  You might find that searching our site for [\"idempotent\"](http://stats.stackexchange.com/search?q=idempotent) gives particularly helpful pointers.',)####('@charles Thanks, did you saw any previous study adopted this kind of fitting to get optimal cutoff point?',)####('If that is a homework, then add a \"self-study\" tag.',)####('Is $\\\\\\\\sigma^2$ assumed to be known?',)####(\"A regression with one response and a one binary predictor is in essence equivalent to Student's t test comparing two means. This is often regarded as too puzzling to be mentioned in introductory texts and too obvious to be underlined in more advanced texts. (In practice, not everything in your favourite software to do either procedure may match exactly.)\",)####('Why would the second order partial derivatives be between -1 and 1?',)####('@Stijn yes ....',)####('You forgot to include a vector of 1 representing the intercept. The rest looks correct.',)####('Corrected! Thanks @mpiktas',)####('I suppose you meant $\\\\\\\\alpha=0$ and $\\\\\\\\beta=1$ in your third question?',)####('That small detail that was a bit important... I thought you were talking about `R`. OK, a bit more rigorously then : $-\\\\\\\\frac{N}{2}\\\\\\\\log(2 \\\\\\\\pi) - N\\\\\\\\log(\\\\\\\\sigma) - (1/(2\\\\\\\\sigma^2))  \\\\\\\\sum( lm_{residuals}^2)$ where $N$ is your number of readings, and $\\\\\\\\sigma$ is the standard deviation of your error. Do you have `lscov` functionality, so I can present an example using that or you want the standard *matrix left division* notation for MATLAB?',)####('Hello Glen, In Prism, they use Cov to refer to the normalized covariance matrix, where each value is between -1 and 1. I do not really understand why it is (might be) important',)####('Is $b$ under \"**Notation**\" supposed to be the same as $u$ under \"**Question**\" (etc)? Are you thinking of this as a mixed effects model?',)####('@gung Thanks for pointing out this notation inconsistency. I think it is now fixed. Yes, it is a mixed effects model, as you cleverly guessed.',)####('@gung Although this is not a \"self-study\" question (it is not homework), it may look as such since I tried to be concise, I will leave this tag since I just need some hints to get a solution.',)####('Our policy regarding what to call `[self-study]` is that it does not have to be from an actual class. Instead it refers to the type of question that it is.',)####('@gung Thanks for the clarification.',)####(\"Maybe I do not understand what you want, but assuming you want the logLikelihood of a OLS model you just fitted (eg. `lm_test <- lm(weight ~ age)`) won't that just be `logLik(lm_test)` ?\",)####(\"Thanks but unfortuantely this calculation has to be done in Matlab and I don't have the function mle() which is in the newer versions of matlab.\",)####('It seems I have lscov() :-) (Although I am prolly more used to using left divide).',)####('See the answer below, I take account for both cases.',)####(\"@NickCox Perhaps, i couldn't able to express  what i actually want. Please, see my edit.\",)####('I do not agree with @IMA. Partial R squared is directly linked to partial correlation, which is a nice way to study confounder-adjusted relations between iv and dv.',)####('Just some comments and then a question: (1) this is not \"linear regression.\" If you\\'re interested in researching such techniques, look instead at \"nonlinear regression\" or \"curve fitting.\" (2) The data shown clearly are badly described by a formula of this type, as evidenced by (1) the values reach $0$ for large but finite $t$ and (2) the values reach $1$ at a strictly positive $t$. What does this \"series\" represent, by the way?  *This matters* for the analysis. The apparent consistent decrease in the values suggests the plot shows derived, rather than original, data, which is important.',)####('This question appears to be off-topic because it is entirely about which software syntax is appropriate. See the help for more advice if unclear. ',)####('Btw, none of these methods answer the question about normality of the errors.',)####('It makes no difference whether you teat them as categorical or numeric. (To make them categorical in R, you would `as.factor` them, and then the dummy for the second level of the factor would replicate the original 0-1 variable.)',)####('i see. thank you glen.',)####(\"They'll probably just be parameters (constants) that need to be estimated from the data, and then the estimates of the constants are used to project the curve past the red line.\",)####('@NickCox the first two questions in the body of the post -- \"*How can I diagnose the normality assumption about time? Should I need to perform a linear regression to measure the dependency of time on age and karno?*\" seem completely on topic to me. They\\'re not directly questions about syntax but about statistical issues.',)####('@Glen_b Why then ask \"Are the following commands all to answer the question\"?',)####(\"I didn't say there weren't any syntax questions. There are statistical questions - ones which can be addressed - and so the question looks to be on topic for CV, to my eyes. But even the directly R questions seem to be borderline (since they're about implementation\\\\\\\\* of a statistical calculation). -- * not useful, perhaps (though addressing why is again a statistical issue) -- I think even that arguably falls under the last topic in the list of what's on topic in the help.\",)####('It seems that the title means diagnosis of assumptions of the linear model (normal distributed residuals), while the body means asks about diagnosis about the response itself. Linear model assumption has nothing about the response distribution itself, but the normal residuals. But maybe response distribution is your particular study in your project?',)####('I agree with @Glen_b .',)####('user2983722 As above, your question has produced quite different interpretations. Please clarify: Do you want general advice on assessing normality in a linear regression context? specific advice geared to R syntax? specific advice geared to the dataset you refer to? Why you suppose a linear regression with time as response makes sense, any way? In short, precisely what is \"the question\" you are addressing?',)####('As @Michael Mayer hinted, any normality assumption in regression is about the errors, not the original data, and as such is best examined by looking at the residuals from a (serious) model.',)####('Necessarily b=1 or x=0 in the equation as written.',)####(\"What problem(s) do you run into when you simply regress `lILC6` against the original five variables?  Since the third PC still accounts for 20% of all variance, with 100-78 = 22% left to explain, none of the remaining two PCs (not shown) can have more than 20%, whence each has at least 2% (and likely more): this implies the eigenvectors don't vary a whole lot--the ratio of the highest to the lowest is at at most $\\\\\\\\sqrt{30\\\\\\\\%/2\\\\\\\\%} \\\\\\\\lt 4$, indicating there should be no multicollinearity problem at all!\",)####('@conjectures sorry there was a typo here. I have corrected it directly in the question',)####('Why do you want to do this? Categorizing variables is not usually a good idea.',)####('Dear Peter, I want to calculate the odds ratio and conduct logistic regression with the group based on new cut-off.',)####('Why would you do that when you have a continuous measure already? Hardly ever a good idea.',)####(\"It sounds like a better method might be spline regression; but that article did stuff I don't recommend\",)####(\"the link is to a biomedical journal article. They often use cut-points to provide clinically useful thresholds, with the understanding that you're trading a bit of statistical quality for a bit of clinical utility.\",)####('Related question: http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression/44841#44841',)####('If you set e.g. $x_2$ to 0 then $b_1$ is only the contribution of $x_1$ when $x_2 = 0$. Is that what you want?',)####('This is not the right approach. Use instead the \"matching\" method described at http://stats.stackexchange.com/a/46508.  Equivalently, perform multiple regression.  Note that in any event the \"contributions\" of the factors are not additive unless all factors are orthogonal.',)####('Yes, just use simple algebra to isolate that extra intercept, the answer should not be equal to $y$, but $y + \\\\\\\\beta_0$. In a way, you\\'re adding two regression planes on top of each other without noticing that the \"height\" of the regression planes was double counted. In other words, if you subtract the intercept from the sum of $y|x_1 + y|x_2$, you should be able to recover $\\\\\\\\hat{y}$. Notice that it will not be equal to $y$, cause you haven\\'t added the error term back to either side.',)####('I think the article says it pretty clearly. One may write a syntax to name a cut point, run the two regression, export the two $R^2$, and repeat the process with another cut point, so on so forth. When all repetitions are done, plot the two $R^2$ against the cut point, and look for the one that optimize the models (aka give highest $R^2$ for both models). Unless you have spotted a very clear inflection in $x$, there can be multiple optimums, which will cause some problems.',)####('How are you assessing distributional shape? Some kind of diagnostic display?',)####(\"@JuneKang  I haven't seen something like this. I'm off campus so can't access article. But as someone mentions in your other question it sound a lot like what the -segmented- package does. For that package you have to provide the number of cut-points. If you don't have a preset number this can sort of be dealt with by running a number of cut-points(1-5) and comparing AIC and choosing best model.\",)####(\"To reliably estimate a coefficient takes a deal more than a couple of points per coefficient. You're talking about estimating 5050 parameters (plus an intercept and $\\\\\\\\sigma$). Do you have enough data for all that?\",)####('Thanks for your answer - will try to digest and act on it when next in the office. Yes, I used scatterplots where Y = Studentised residuals and X = predicated values / predictors in the model. I also had a look at the mean, SD, etc., of the residuals.',)####('For your 1st question: The $\\\\\\\\hat{\\\\\\\\beta}_0=3.2$ is given, so you know this parameter. Does that help? If this is a homework, add a self-study tag.',)####('@Stat: What does that mean? I thought the simple linear regression had two parameters, the intercept and the first term?',)####('It means that they may be some situations where you know one of the parameters. For example: suppose you want to find the relation between the weight $Y$ of a person and the time $X$ due to a special diet. Then for sure you know that at time zero the weight is for example 65 kg. So when $X=0$, your $Y=65$. This means that in linear model $Y=\\\\\\\\beta_0+\\\\\\\\beta_1X$, your $\\\\\\\\beta_0=65$. So you know this parameter before fitting. Also you may expect your estimated $\\\\\\\\beta_1$ to be negative!. This may not be a perfect example, but will give you an idea.',)####('@Stat: Oh, so if one of the parameters are known, it is subtracted from n? So in this case, since the intercept is known, we subtract just 1?',)####('Take a look at the **party** package and see if this suits your needs. It can handle a number of types of models in the nodes of trees IIRC.',)####('The intercept $3.2$ is assumed to be known ?',)####('You might be interested in http://en.wikipedia.org/wiki/Ecological_fallacy, which applies here.',)####('Consider this strategy: average your averages, but keeping two distinct x values. With any luck, your y values will differ. Now what is your $R^2$? Correct or not, the key point is that you changed your regression problem to a different regression problem. The more you average, the more variability you eliminate. Feature and bug, all in one.',)####('As long as you don\\'t define what you mean by \"want to just \\'control\\' variables\" you won\\'t get an answer to your question. From the figure caption in the (unsprecified) publication you are referring to it should be possible to deduce how they \"controlled\" the covariates.',)####(\"Thanks @whuber! Cheers for seeing the log(a) error. I do optimize it as a nontransformed parameter, as it is prone to creating NaN's. And after thinking some more about this, I think you are right and there is no problem. I was surprised it gave such different exponent values, but when looking closer at the $x_0$ differences and how the two solutions optimize it, it does make sense. Glad you made me see it this way!\",)####('The differences in $b$ and $x_0$ do not appear large: check their standard errors. The difference in $a$ is due to a basic mistake with logs: $\\\\\\\\log(y) = \\\\\\\\log(a) + b\\\\\\\\log(x-x_0)$ (which I guess is compensated for in the last line), whence the \"a\" in the log model should be close to the log of the \"a\" in the original model. When this change is made (by replacing \"a\" in the first model with \"exp(a)\") the changes in all three coefficients are within the ranges indicated by their standard errors. In short, all appears fine: is there really a problem to be resolved here?',)####(\"An example in which a scientist's claims were wholly discredited due to the use of average values in a regression is documented in this [2007 EFSA Review](http://www.efsa.europa.eu/en/efsajournal/doc/19r.pdf). For recent (amazing) developments see http://retractionwatch.com/2013/11/28/controversial-seralini-gmo-rats-paper-to-be-retracted/: the same scientist misused statistical analyses so badly that six French scientific academies joined to denounce his work.\",)####('I have a couple of practical suggestions then for achieving stable and reliable solutions. (1) Use $\\\\\\\\exp(a)$ in model 1 and $a$ in model 2 to obtain comparable results without NaN problems. (2) Run model 2 first and feed its estimates as starting values to model 1. (3) Break model 2 into two parts: only $x_0$ is nonlinear; $a$ and $b$ are efficiently determined by least squares (*e.g.*, use `lm`). You can then use a *one* variable application of `nls` to estimate $x_0$. If you want the full output of `nlsLM`, feed it those estimates as starting values.',)####('Are you referring to a deterministic relationship?',)####('Offhand I would consider orthonormalizing the basis of polynomial functions $x^i,$ $i=1,2,3$ (as evaluated at the data) and using ordinary least squares.',)####('Thats a great tip on the starting values. The parameters are very strongly correlated, and I guess the solution is prone to local minima as well. The reason I apply a nonlinear solution is to estimate the 3 parameters simultaneously to quantify the correlation and hence get a more realistic uncertainty estimate. Now I just need to come up with a good way to do the same with a 3 degree polynomial with zero intercept. Any good tips on that @whuber ?',)####('In what way is the data \"not good\"?',)####('Actually they are not clustered very well. And if I perform ANOVA approximately no significant gene I would get. I think it should be because of fewer number of replicates.',)####(\"One thing that might help you visualize what's happening in the plots is to jitter in both y and x (note that if it is in completed years, as is typically the case, Age is also discrete). Which is to say, add a small amount of 0 mean uniform (say) noise in both directions, scaled so that the fuzzed data is still distinct if the x or y values differ (that is, the width is less than half the gap between values)\",)####(\"Also note that if y is limited to values between 1 and 6, a linear model seems unlikely to work over a wide range - you'd expect the relationship to flatten at the ends, as it gets closer to the bounds in y. One thing you could do is compare a smooth of the relationship with a linear fit. (What software are you working in?)\",)####('If you omit one $x$ from $X$, expect all coefficients to change.',)####(\"Thank you very much for the hints! I'm working in sas.\",)####(\"Hmm, can't directly help with SAS.\",)####('One regression does not remember what you did previously. So, just exclude what you want to exclude.',)####('Would it be possible to supply a reference for that? I searched for omitted variables and mostly I got derivations on how the bias of the regression would be affected, not on how the other coefficients would change.',)####('I mean I want to exclude the variable without having to do the training again.',)####('What \"training\" is involved here? You don\\'t give any details of what you did except \"regression\". If you are using some flavour of machine learning, you should explain what that is.',)####(\"In my case I'm using Lasso, but I wanted an agnostic answer in relation to the training procedure.\",)####('It should be stated or at the very least implied by every regression text. The converse is that if you add a new predictor, expect coefficients to change unless the new predictor is uncorrelated with all the others. Try it with data.... (You seem to be trying to learn statistics by searching the internet. You need to work through reliable textbooks, not browse for what you think you want.)',)####('I am not intimately familiar with Lasso, but I imagine that repeating Lasso with one predictor omitted is quite sufficient for Lasso to \"forget\" it. Again, if you want something different, you need to spell out what that is. Different advice is to edit your question to make it clearer: many people are reluctant to read a trail of comments to see whether the question was changed.',)####(\"I've edited the original question for more clarity.\",)####('@Macro could you please help me understand the concept of \"standardized\" variables.',)####(\"There are two main reasons you'd expect to reject the deviance test (i) the model has a lack of fit; or (ii) the model fits okay but is overdispersed relative to the binomial.  Might the second main possibility be the case?\",)####('Thanks, Glen_b!.. However, I am not sure I follow what you say. Could you please develop a bit more, or maybe give me some pointers?.. Thank you very much!..',)####('Please show your code, and, if possible, some idea of your data.',)####(\"I've never used Weka, and I wouldn't ask my worst enemy to do regression using Excel, but it appears that you have reverse coded some of your dummy variables. You've put Woche4=0 in Weka, but (assuming it's coded 0, 1) the variable that's used in Excel will be Woche=1.\",)####(\"Yes it's reversed, but that shouldn't change the intercept, should it...? I'll try it anyway.\",)####('In general.And may be better then in R.',)####('Yes. The coefficient of smoking will be equal to the difference in mean results for the two groups. The p value of the null hypothesis of no smoking effect will be similar to the one of the corresponding two sample t-test (using equal variances). In SPSS, you can use smoking as a numeric covariable or as factor. The numbers will be the same. Just try it out.',)####('Thank you peter, very kind of you',)####('Should follow from the delta method, and also probably be part of the output of whatever statistical package you are using. Can we see some code?',)####(\"I use python with  scipy and lmfit. lmfit's confidence finding algorithms are currently flawed and I don't know how to fix it (that's another story)\",)####('I have made some edits to your question to incorporate a description of the model being fitted. You should check it still says what you want.',)####('I think this Q is on-topic, then. Is there any specific model diagnostic you are interested in? Are you just interested in a lack of fit test, or what all of the diagnostics that should be standard are?',)####('Are you using `anova` from the stats package?',)####('What is your model?',)####('See also related question and visual explanation http://stats.stackexchange.com/q/70899/3277',)####('yes. Not the `car::Anova`.',)####('Diagnostic for Univariate model is well set up, but I did not see a routine diagnostics method for multivariate models where the response has more than two variables like here.',)####(\"How do you mean? Unfortunately I can't post actual data if thats what you mean.\",)####('R is pretty terrible with anova, given that it defaults to sequential sums of squares. Why are you fitting your model with `lm`?',)####('What is your dependent variable? What independent variables? How big is the sample? Are the variables continuous or categorical? that sort of thing. The actual context would be nice, too, even without data.',)####('You should not be doing ordinary linear regression with a \"bounded (0-1) response variable\".  Neither the assumption about linearity of the mean nor the constant variance assumption are likely to be true, except in some special cases, and even then usually only approximately. Is your bounded variable compositional data, or is it a proportion based on counts, or something else?',)####(\"what's the best way to fit a multivariate regression model where both the multiple responses and predictors are continuous variables?\",)####('Is your dependent variable naturally bounded?  Sort of looks like it.  And yeah, you need to provide more info.',)####(\"What is your goal in estimating the model? There are plenty of direct and indirect diagnostics but they aren't all appropriate in every situation.\",)####(\"I guess I usually use `glm`, but I don't deal with much continuous multivariate data.\",)####(\"Anyway, Pillai's trace is a multivariate statistic (like Wilk's lambda or Lawley-Hotelling trace). The first df is the number of degrees of freedom of the model term (which clearly are all continuous) and the next two dfs are the numerator and denominator degrees of freedom for calculating the F statistic. The p-value is the test that the probability of the observed data is greater than F.\",)####('@PeterFlom: the singularities are cause by the setting of `x1`, `x2` and `x3`: for example all males are adults and the first six months, and all females are juveniles and the second six months.',)####('More details added',)####('Are you asking in general, or are you asking specifically how to do this in R? Note that the latter would br off-topic for CV (see our [help page](http://stats.stackexchange.com/help)).',)####('It looks like your data is bound between 0 and 1.  Is the outcome actually a proportion?',)####('You should probably take a course in regression, there is much more to it than just looking at the output, at least when the output is so limited. You have assumptions to check, graphs to make, etc.',)####(\"As Peter correctly suggested a short course in regression will probably benefit you greatly. For self-study, very approachable and hands-on expositions of linear regression with R coding examples are presented in J.Faraway's books: `Linear Models with R`. Additionally, the following thread : http://stats.stackexchange.com/questions/170/free-statistical-textbooks offers a variety of free e-books that can take you a very long way.\",)####('Your question amounts to \"teach me to understand regression\", which would take a book to answer. \"*...are the two variables highly correlated?*\" - that depends on what counts as high, and there\\'s no single standard; a chemist might call that $r$ very low, while a psychologist might say it\\'s unbelievably high. It depends on what we\\'re dealing with (but then why would a high $r$ matter in any case?). \"*The r-value is something that I think points to the correlation*\" -- no, it *is* the correlation. \"*what would be ways to find out which ones do not satisfy this constraint*?\" -- what constraint?',)####('@Glen_b Since the correlation coefficient is not 1, there are values which are not showing direct relationship. Is there a way to figure out such values.',)####(\"If there was a perfect linear relationship, you wouldn't need regression at all. Because there's going to be variability about any underlying linear relationship, you don't expect it to be 1.0 even when the population relationship is linear.\",)####(\"Now I have no clue what you're even asking. What information do you want to show? If you're not interested in summarizing contribution to the sum of squares, what *are* you interested in showing?\",)####('The most economical way to present this regression in a table is to show an empty table, because the regression indicates there are no significant linear relationships between the response and *any* of the variables you have included!',)####('question again edited',)####('No, you cannot average the t values or p values across months. I would also worry about the \"two not defined because of singularities\" and the odd pattern of coefficients for months.',)####(\"I think this is mostly (if not as directly) covered by Macro's answer in [this post](http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr?rq=1). It even includes discussion of the use of `anova`. If for some reason that doesn't cover it, I can undelete my answer here.\",)####('question amended with more details',)####('Where is the month of April?',)####(\"Come on...  Everybody knows that blindly following a recipe is better than true understanding.  Besides, I don't have the luxury of too much time.  Not being a statistician, I get bogged down by the jargon and the notation.  I was hoping for some narrowing of the subject, so I don't spend a day or two to discover I'm going the wrong direction.\",)####('Have you considered natural splines rather than higher order polynomials?',)####(\"the issue is that in practice you don't observe $\\\\\\\\epsilon$ (only $\\\\\\\\hat{\\\\\\\\epsilon}$) so you can never compute $\\\\\\\\mbox{cor}(X,\\\\\\\\epsilon)$\",)####('As said above, $\\\\\\\\epsilon$ is unobservable so you need to rely on your own knowledge and use the common sense to decide about exogeneity.',)####('The correlation between regressors and residuals (not errors) in a linear regression model estimated by least squares _is_ always zero. You cannot test exogeneity (conditional uncorrelatedness) without instrumental variables.',)####('Thank you for your comments, as putting them all together answers my question. Is anyone willing to organize these together into an answer which I can accept?',)####(\"I changed only the  'X' as you suggested, but then I got different coefficients again.\",)####('The expectation of the errors is zero.',)####(\"How do you define X' ? Existence of X' would imply that X' is square, which, unless I am mistaken is not the case.\",)####('The very closely related question at http://stats.stackexchange.com/questions/25068/interpreting-plot-of-residuals-vs-fitted-values-from-poisson-regression/30276#30276 may be instructive in this context.',)####('This is an *enormous* subject. I can see that narrowing your question might be difficult, but any adequate answer would--and has--taken several large books.  You might begin your research by linking through the [tag:model-selection] tag I added; it will show you some threads in which these topics have been discussed.',)####('Yeah, I was worried the question was too broad and fluffy.  But I\\'d be happy with someone just saying \"I\\'d try the elastic lasso vector  extension support (elves) method first.  Good luck!\"',)####(\"Maybe you shouldn't be! As @whuber is implying, your quest for good predictions is much more complex than the matter of choosing the right model. You might want to spend a couple hours studying regression, if you haven't already. Your second question is pretty basic, but the answer may not be. Your information is all relevant to other, variably complex (or simple) issues that you may want to understand better. I can throw some jargon at you to help you narrow down the topics worth studying if you like, but this is just as likely to raise other questions as answer yours. Does that sound good?\",)####('Also, how do you know there are errors in the predictors? Do you have any quantitative information about how much error? This might include redundancy among multiple indicators of the same latent construct (i.e., if you think some of your predictors are measuring roughly the same thing)...',)####('The output will look better if you add this to your analysis:\\n## from the AER book by Zeileis and Kleiber\\noptions(prompt=\"R> \", digits=4, show.signif.stars=FALSE)',)####('Are the two models run on the same population? Is Y the same? Is X? (it would help to use subscripts on Y and X to distinguish them).',)####(\"Some of the measurements we are making are inherently correlated (e.g. the temperature in one spot is almost the same the temperature in a nearby spot, bigger things weight more, etc).  We have done repeated measurements of a single object and how the predictors vary.  But that's only for that one object, the next one will be different.  And I wouldn't know what to do with the variation anyway.\",)####(\"I rarely make this recommendation, because people come here out of self-sufficiency, but since you have a complex problem and don't have the time to learn how to analyze it, it seems like the best course of action is to collaborate with someone who has the knowledge and experience to help you.\",)####('What did you read where? What was the context?',)####('I dont think it can be enforced with real input data. You can of course do PCA in your data to transform it to orthogonal series.',)####('I am not sure what you mean by \"enforcing\". One can check whether the variables are orthogonal. But if they are not orthogonal then they aren\\'t. Multiple regression is very often used when the input vectors are not orthogonal.',)####('@adam this is my belief also, however I am very fresh to statistics to really know that',)####('@PeterFlom by enforcing I understand a way to design the experiment that would cause the inputs to be orthogonal\\nI am not sure however what authors means by enforcing. That was the question. If it exists such kind of design of an experiment in order to get orthogonal inputs. Solely my imagination produced only the scenario for nominal inputs, but for continuous I am not able to design such thing.',)####('**There are several things fishy about this summary:** [Last] and [Score] range from $0$ through $100$, so how did they take the logarithms of the zeros? Moreover, because it is clear that [Last] has some *negative* skewness (notice how close its mean is to its *maximum* value), using its logarithm would usually not improve anything--it would only give the extreme low values more leverage. Furthermore, it is strange that \"Model 4\" would include interactions of [Win R] and [Win F] with other variables without actually including [Win R] and [Win F] themselves.',)####(\"This has a flavour of: I get 42 as the result of a calculation with data I won't show you. Did I do it right? Better answers: Your tag indicates use of SPSS. Does SPSS reproduce regression results from elsewhere reliably with known data? You can carry out your own checks. Have you searched for literature on bugs, limitations or reliability of SPSS? As of 2014, it's a fair guess that regression routines in all major statistical software have been banged on many, many times, but that's no absolute guarantee of correctness.\",)####(\"There's no way to be *certain* of the results without the data (and even then, validity would be only for the data you were given, whose provenance may well be doubtful). You can however, often do some reasonableness checks. (You can rely on SPSS for basic regression calculations, since it's easy to check it works on your own data.)\",)####('Thanks a lot for response;  I have descriptive analysis of variables and I edited my question. I do not have access to data set; I found these results in a paper and I want to be sure of the validity of results.',)####('*Ahem*. (i) The expression of the problem is nonsensical. How can 3=18? Surely the intent is something like $f(3) = 18$; (ii) if you can see enough to write $18=3\\\\\\\\times 6$, $32=4\\\\\\\\times 8$, etc., surely you can then see enough to split the second term in each of those ($6=3\\\\\\\\times 2$, $8=4\\\\\\\\times 2$, and so on) to then write: $18=3\\\\\\\\times 3\\\\\\\\times 2$, $32=4\\\\\\\\times 4\\\\\\\\times 2$, etc, and *instantly* spot the quadratic, $f(x) = 2x^2$. (You did the hard part, the next step is even simpler!)',)####('\"the line vertically closest to all the points\" ?\\nOne usually takes the sum of squares -- see the nice picture on Wikipedia\\n[Coefficient_of_determination](http://en.wikipedia.org/wiki/Coefficient_of_determination).\\nThe sum of vertical distances is the L1 norm, \\nwhich is less sensitive to outliers but much less common.',)####('Running models on lagged data is the essence of predictive modeling. If your data are lagged 4 months, you need to take that into account. Just make sure your output corresponds to the predicted, non-lagged value when you are calibrating your model.',)####(\"Additionally, did the problem specify a minimum information content criterion on the answer? If I remember my math correctly, there are an uncountably infinite number of functions that fit these points, all giving different answers for $f(10)$. I'm not typically pedantic, but time-waster emails deserve it.\",)####(\"I know this doesn't answer your question (sorry), but have you tried a univariate time series? How far into the future do you want to predict?\",)####('Is this the only way you can get the data?  It would be better to have a dataset with one observation for each battle, identifying the two operators, the two robots, and the outcome.  If the data have to come as summary information, can you get different summary info, or is this it?',)####('@TrevorAlexander if you think this question is a waste of time, why bother to respond to it? Clearly some people find it interesting.',)####('@jwg because [someone is wrong on the internet](http://imgs.xkcd.com/comics/duty_calls.png). ;)',)####(\"Sounds like you're not fitting a mixed-effects model which you should do here to account for variation between the operators and (nested within that?) the robots. As for the issue with negative damages, you can get around that by doing some kind of transformation\",)####(\"Thanks for your advice, although I must admit I don't know how to apply either of the suggestions.\\n\\nI tried plugging in `lme` instead of my `biglm`, but obviously I need to do a lot more reading on this to understand what exactly to provide as parameters to it.\",)####(\"Linear regression might have one x-variable (simple linear regression) or more than one. Multiple linear regression can *only* refer to the case where there's more than one.\",)####('I once realized that I tended to dislike the phrase \"confidence intervals\" in the reports devoted to prediction. I thought I had two types of predictions: predicting the mean and predicting the individual observations. Since then I found several references to the same way of thinking. For example, [here is a link](http://jackman.stanford.edu/classes/350B/07/predictionforWeb.pdf) that answers your question in the style I like.',)####(\"This is it, the data is received from an external system owned by another company and this is unfortunately the extent of data available.\\n\\nI have a few more summary variables available which I didn't mention here (you can consider them DamageB, DamageC, etc...) but they are closely correlated with DamageA & DamageB so I don't think they are that useful and I didn't mention them to avoid confusion.\",)####(\"You're mistaken. $Q$ *is* $n\\\\\\\\times n$. However, $QR$ is of the same dimension as $X$, $n\\\\\\\\times p$. The $p \\\\\\\\times p$ dimension given for $R$ in the text is what is called 'little R'; the full dimension of $R$ is $n\\\\\\\\times p$ but the lower part is zero.\",)####('It looks like there are two versions of the QR factorization. I was taught The \"skinny\" version where A is mxn,Q is mxn and R is nxn. See http://www4.ncsu.edu/eos/users/w/white/www/white/ma580/chap3.3.PDF',)####(\"Since you only care about how far a score is from the mean score, just make up a mean score as its value won't affect the answer.\",)####('Hint: write down the prediction equation using deviations from means (which is what the question asks). Then remember how the constant term in a simple regression is calculated.',)####('Ok I got it, thanks!',)####(\"@Glen_b, I didn't exactly get it. you say that Linear regression might have one x-variable  or more than one. Multiple linear regression can only  more than one x-variable  ?\",)####('Correct. Linear regression can encompass both [simple-](http://en.wikipedia.org/wiki/Simple_linear_regression) and [multiple-](http://en.wikipedia.org/wiki/Multiple_linear_regression#Simple_and_multiple_regression) linear regression (though perhaps more frequently refers to simple linear regression). But those latter two terms are mutually exclusive categories.',)####(\"See Penguin_Knight's answer, [here](http://stats.stackexchange.com/questions/73765/does-stand-alone-dummy-variables-in-linear-regression-models-make-sense).\",)####('Why is symbol not present as a main effect?',)####(\"I know this doesn't answer your question, but robust standard errors (Huber-White or otherwise, check out the `sandwich` package for those) won't do? Sure, the estimation will be less efficient but at least you'll be sure the variance-covariance matrix will be consistently estimated.\",)####('I think you would benefit from mixed effects models. A model where you can specify the variance structure would foresee heteroskedasticity. See Zuur et al. 2009 page 75+ for more details. Another good resource on MEM is Pinheiro and Bates.\u017e',)####('I second @RomanLu\u0161trik as to the use of mixed effects models. Within that framework you can explicitly model variance, and therefore heteroscedasticity. You can do it in R as well.',)####('The function is `gls` from `nlme` package.',)####('Thank you so much for all your help. I will try all of these suggestions and let you know how it goes.\\nThanks again!',)####('You can find this in every decent textbook on regression.',)####('Additionally, you can look at the code behind `effect` to se how they are calculated.',)####(\"Short answer: no. The error doesn't appear in that image, and is not observed. Your best estimate of the error is the residuals (but sometimes it's a demonstrably poor estimate).\",)####(\"Just some comments, in case they're of any help: it looks like your responses must be non-negative and converge to $0$ (or close to it) at $0$, whereas these bands evidently are erected using a model of independent additive error.  That makes them unrealistic, especially at the left.  Moreover, the patterns of blue dots suggest the error has strong serial correlation, which also needs to be accounted for in constructing these bands.  Although you might not want to cope with this additional complexity in your data to do the fitting, it indicates the bands you have drawn aren't worth much.\",)####('Your question is unclear, because you shift from asking if they are \"supposed to be symmetrical\" in the 1st sentence, to implying that they are not in sentence 2 & asking (presumably) why they aren\\'t in sentence 3. Can you make this more consistent / clear?',)####('OK, let me ask it this way - why are the confidence and prediction bands symmetrical around the regression line when the regression is non-linear but take on an hour-glass shape when it is linear?',)####('You are right.  The band does cross into the negative territory.  However, I am not interested in the values of the bands themselves, but rather in the EC50 values corresponding to the band limits.  Is there an alternative to constructing the bands this way?',)####('By \"not observed\", do you mean the error is not not observed in the plot or it is not observed in linear regression?',)####(\"Errors are not observed at all, including in both those senses. If the errors were observed you could calculate the population parameters in simple regression from two observations and their associated errors. The error term is the model for how the data values don't lie on the population line\",)####('http://orm.sagepub.com/content/15/3/339.abstract To all who come across this in the future, I recommend reading Dalal & Zickar (2012) -- The \"solution\" of using mean centering is not as straightforward as one would think.',)####('Yes, but as I intimated they can get complicated. Generalized least squares and time series methods can cope with the serial correlation. Nonlinear transformations of the dependent variable are one tool to handle non-additive error. A more sophisticated tool is a generalized linear model. The choices depend partly on the nature of the dependent variable.  BTW, although I\\'m unsure what you mean by \"EC50 values\" (it sounds like you are modeling dose-response relationships), anything computed from the illustrated bands will be suspect.',)####(\"(1) I have no idea (2) you don't need to remove correlated predictors in linear regression (the coefficients for the correlated variables will be poorly defined but not always an issue) (3) I assumed the correlated variables would be retained in a random forest with relative importance spread between correlated variables, while in a linear model variable selection will often get rid of some of the correlated variables.\",)####('http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression/44841#44841',)####(\"@ocram, thanks, but I'm not quite capable of handling matrix stuff, I'll try.\",)####(\"@ocram, I've already understand how it comes. But still a question: in my post, the standard error has $(n-2)$, where according to your answer, it doesn't, why?\",)####('Welcome to our site!  Please use the formatting tools available when you are editing so that your code is readable.  Your \"code\" for $b$ obviously is not correct `R` code so it\\'s impossible to tell what you\\'re doing wrong.  I believe there is `R` code posted on this site doing exactly what you\\'re trying, so searches on relevant keywords like [tag:r] and [tag:regression] as well as on likely parts of the code (such as `solve`) might turn up some useful stuff for you.',)####('Thanks so much for your guidance! I rewrote the code that I am trying to evaluate and I have searched for \"solve\" and \"ginv\" although they are not working as they should, or most probably I am using them wrong!',)####(\"You don't invert a matrix with `^-1` in R, and in fact shouldn't explicitly invert $X^TX$ at all. (Also you don't do matrix multiplication with `*`). You should solve $(X^TX)\\\\\\\\hat\\\\\\\\beta = (X^TY)$. See `?solve`, which does both solution of linear systems and inversion. That's still not the best function to use if you're trying to be accurate (QR decomposition is probably the most common way these days), but it will do for getting the ideas down.\",)####('For general information about the effects of suppressing (ie, not fitting) the intercept in a regression model, it may be useful to read this excellent CV thread: [Removal of statistically significant intercept term boosts $R^2$ in linear model](http://stats.stackexchange.com/questions/26176/).',)####(\"Shouldn't the last row of your w matrix consist of a pair of 1's rather than w_4?\",)####(\"Welcome to the site. With just the information you have given, it isn't possible to give good advice. I wrote [how to ask a statistics question](http://www.statisticalanalysisconsulting.com/how-to-ask-a-statistics-question/) on my blog. It may help you improve the question.\",)####('I second the advice of @PeterFlom. You could also take a look at my answer [here](http://stats.stackexchange.com/questions/77196/anova-with-huge-dataset-use-only-the-mean-for-each-condition/80288#80288) as a starting point to analyzing reaction time data.',)####('I have an example worked out here: [What if residuals are normally distributed but Y is not?](https://stats.stackexchange.com/questions/12262//33320#33320)',)####('Please explain your notation: what do the values of the $E_{ij}$ *mean*?  What is an \"event\" (the word has a conventional meaning in probability that seems to differ from your usage)?  What does it mean to \"produce an observation\"?  What does it mean to \"match the observation\"?  Where you speak of a \"total contribution,\" what is contributing to what?  What are the $a_{ij}$?',)####('Trivial example: X has a Bernoulli distribution (ie, taking the values 0 or 1); Y = X + N(0, 0.1). Neither X nor Y is normally distributed on its own, but regressing Y on X still works.',)####('I guess you are thinking about the distribution of the residuals, not the distribution of the variables.',)####('Y appears to be bounded above and below. What is Y?',)####('I am afraid I only have time for a quick comment, but if you search for \"compositional data analysis\" you should quicky find some useful info. In short, if I have 3 variables that sum to 1, as soon as I know the value of 2 of them, I must then also know the value of the third...thus trying to estimate 3 independent \"effects\" is nonsensical. Also, please see this related question http://stats.stackexchange.com/questions/68944/analysing-data-measured-as-proportional-composition',)####(\"Correct - the last row should be 1's - fixed. The Eij's are just the known data for Event i variable j (so it's just a real number). I'm using Event in the context of the problem - not in the conventional sense - it is a predictor matrix. The aij are the traditional regression coefficients.\",)####('Are you sure you did not simply regress one of the *independent* vectors against the rest?',)####('(1) \"Lagged\" in what sense? This does not appear to be any definite *series* of data. (2) A good choice of model depends on the nature of the data: one would use two quite different models depending on whether $X$ were considered to be measured with substantial error or not, for instance. As another example, one would also choose different models depending on how the truncation of $Y$ at $0$ and $4$ occurs: are these natural limits or artificial cutoffs? Please edit your question to provide more information about the data so that it can be answered appropriately.',)####(\"(1) what have you thought of? and why don't you like those approaches (2) lagged y? is this a time series? (3) the issue - to me - seems not to be the prediction, the giving honest estimates of variance around mean.\",)####(\"I don't know where you found this quote, but it sure is poorly phrased.\",)####('The question is edited. Thanks beforehand!',)####('@whuber I hope my edit contains the extra information you requested? Would you like to give my question a second thought? Thanks a lot.',)####('This is the website where I found some more information. It seems to only deal with 2-dimensional data though. http://easycalculation.com/statistics/learn-regression.php',)####('There is no way to tell unless either 1) There is theory to support a model or 2) You run the models.',)####(\"Just a note to say I've used the `mtcars` dataset (built into R) so that any R users can themselves run the models and respond about which would be the best model. I've already ran the models but wasn't sure which one works best.\",)####('Welcome to CV. Can you clarify your question? Are you trying to predict Z as a function of X & Y, or are you trying to predict an outcome in another dataset from X, Y & Z here? If so, is there any way to establish a correspondence b/t a given triplet in this dataset (Xi, Yi, Zi) & a particular outcome in the other dataset (Oi)? You mention a \"bias term\", that terminology is more common in machine learning than statistics; are you trying to train something like a neural network, or do you want regular (OLS) regression in statistics?',)####('The Mathematica function `FindFit[...]` could achieve what you desire.',)####(\"Besides the things mentioned in the answers, your prediction intervals also won't have the right coverage.\",)####('Kindly post comprehensive answers. THANYOU',)####(\"This website is for questions of mathematical research. I don't see a research angle in your question.\",)####('I don\\'t really need to remove the male data, I just need to run the regression on only females.  I have a \"female\" column in which females are denoted 1, and males are denoted 0.  My current code looks like this:  lm(y~x,data=mydata)  I just don\\'t know what to add to that to make it only run for rows that have female  =1',)####('this is the same as stepwise variable selection. see multiple posts on this topic elsewhere on this site',)####(\"why not doing it on the logit model directly? You'll get a better assessment that way..\",)####('Thanks for your answers. The purpose of using Linear Regression was just to help with the computational complexity. ..I will read up on  the backwards/forwards selection posts.',)####(\"Yes, your formula from matrix notation is correct. Looking at the formula in question, $1-\\\\\\\\frac1{n}\\\\\\\\,=\\\\\\\\,\\\\\\\\frac{n-1}{n}$ so it rather looks as if you might used a sample standard deviation somewhere instead of a population standard deviation? Without seeing the derivation it's hard to say any more.\",)####(\"This looks like routine bookwork. Is this for some subject? If you do need (for some non-obvious reason) to actually run a regression on a subset (see Andre's comment for why in general it's not necessary), the following approach is pretty typical. If `mydata` is a data frame containing `y,x1,x2` and `sex`, then instead of `lm(y~x1+x2,data=mydata)` you replace the `data=` argument with one that appropriately subsets the rows of `mydata` by the relevant value of `sex`.\",)####('I believe that the problem is asking to fit a line to 3-dimensional points. There are 100 points with 3 data values each, as well as accompanying result values. As an example:\\n\\nX value (0.442, 0.798, 0.708) has matching Y value (-6.228) and I need to fit a line to 100 points like the X value, while also using the Y data.\\n\\nI was directed here as a better location to ask my question. I was not informed that it was a research website. I apologize for any transgressions.',)####('Normalising the data should not change $r^2$',)####('This question appears to be off-topic because it is about how to use software.',)####('Your question is then actually how do I *subset a data frame in R*? which can be very helpful if you google it. Using `mydata[as.logical(mydata$female),]` as your data set in `lm` is one of half a dozen obvious ways (`mydata[mydata$female==1,]` is another; as is using `subset`)',)####('I thought I\\'d just go ahead and answer, because this is quite easy. However, the others are right; simple code requests belong on Stack Overflow, and in this case, you could probably Google it very quickly (that\\'s why I\\'m not flagging to migrate \u2013 it\\'s a little *too* simple \u2013 though I don\\'t mind if it gets migrated). In fact, [the first Google hit for \"code for excluding conditions in r\"](http://www.statmethods.net/management/subset.html) contains my answer, and several alternatives (hence I\\'ve edited it in)!',)####('This question appears to be off-topic because it is about how to use r.',)####(\"Well, I wouldn't do it with Excel, but you could at least get parameter estimates with its optimization. You need to use the Solver add-in. If you want least squares fitting of nonlinear functions, it's very easy in R.\",)####('\"Linearity\" (or lack thereof) refers to the relationship between the predictors and the response, about which you have offered no direct relevant information. Could you please amend your post to provide that?',)####(\"In addition to @whuber's point, the marginal distribution of the response is not really of interest, but rather the conditional distribution / the distribution of the residuals. On another note, are all Y values integers / counts?\",)####('Some useful searches you can investigate include [multinomial logistic regression](http://stats.stackexchange.com/search?q=multinomial+logistic+regression) and [ordinal regression](http://stats.stackexchange.com/search?q=ordinal+regression).',)####('The prediction interval should be of the form you stated except for the degrees of freedom. A little more though should be given to it.',)####('\"*whether a person\\'s weight is correlated with their weight*\" --- I\\'m pretty sure that correlation is 1',)####('indeed, thanks.',)####('This has been answered many times.  The short answer is no.  But instead of asking if a certain parameter makes sense to examine, state what you would like to estimate and go and estimate that.  I find this is most easily envisioned through differences in predicted values.',)####('So you mean plug in different values for the predictors then study the differences in fitted values?',)####('Can you write out the regression equation you want to estimate using maths instead of R code?  I think people are getting confused about what your statistical model is.  To start, you could use $ y_{tsi} $ as your response/dependant variable, where $ t $ is the time factor, $ s $ is the symbol factor, and $ i $ is any \"repeated\" observations with the same value for symbol and time.',)####('Yes, but be more intentional about it.  An effect of interest may be changing hp by $x$ units holding wt to $y$.',)####('seems related to this question as well http://stats.stackexchange.com/questions/89353/difference-between-iid-data-and-non-iid-data-for-a-simple-regression-problem',)####(\"Note that if $\\\\\\\\sigma^2$ doesn't vary across $i$, it won't affect the maximization, so the weighted least squares results would be the same as the log-likelihood results.\",)####('you might want to expand your questions to include (1) number of predictors examined (2) method of examining predictors (feature selection) (3) number of observations (4) motivation behind building model',)####(\"What is the purpose of your document and who are the audiences? I'd start from getting similar articles and look for some examples on how they are done in your own field. I am more familiar with biomedical literature and most of the time, we just use a table. Illustrations are more often seen when the authors try to explain an interaction.\",)####(\"@Penguin_Knight, this is in computer science domain, however I think this is a generic rather than restricted to a particular domain. Please correct me if I'm wrong.\",)####('Try `s<-lm(tcyc ~ tinst+tmem+tcom-1, data=fit)`',)####(\"Hmm... though question. I'd say the only generic part, for me, is don't show more than you should, and make sure the components to be emphasized really get emphasized. Even in just my field, I have seen all three options. 1) tabulating the results is the most common, followed by 3), but mostly the form of plotting predicted outcome, and then 2). But for 2), I'd use what @gregory_britten suggested: use adjusted X instead of each individual X.\",)####('@Georg, as far as I know, `+0` and `-1` are fully equivalent in R. They both suppress they intercept.',)####(\"yes, patrick is right. I tried using nnls package but still it was giving some coefficient as 0, again it doesn't hold good.\",)####(\"I expect it's a formatting typo, and the intention is $(\u03b2_0, \u03b2_1)'$ (equivalently $(\u03b2_0\\\\\\\\;\\\\\\\\; \u03b2_1)^T$)- which denotes the *[transpose](http://mathworld.wolfram.com/Transpose.html)* of the row vector $(\u03b2_0, \u03b2_1)$ -- that is a column vector, consisting of those two elements.\",)####('Thanks, this makes a lot of sense now. I\\'m still trying to comprehend how to go about this solution, but with the \",\" in place, the question is much clearer now.',)####('You can omit one of the variables and still obtain an unbiased estimate of the other if they are independent.',)####(\"You have to explicitly remove the intercept from the model, if you don't want to have it. This can be achieved by appending $-1$ to the formula, i.e. `E.curr ~ Y.17 + HE.last3 -1`\",)####(\"Well, that solves the problem with the intercept but the coefficients are still wrong. The missing intercept also was just one reason why I think I'm using the wrong model. Apart from that, my results are just plain wrong.\",)####('Please make the reference more precise than \"a book\".',)####(\"I didn't use it since it's a German book. I'll nevertheless put it in.\",)####('In English-language discussions becoming zero is sometimes  described as vanishing. That however is the least of the puzzles here.',)####(\"Although I don't read German, I second @NickCox's suggestion. The reference (including page #) is needed at a minimum. In addition, an excerpt might be nice. On a different note, are you sure that the denominator isn't square-rooted (ie, $\\\\\\\\sqrt{1-{\\\\\\\\rm corr}(X,Y)^2}$)?\",)####('@gung The denominator is not square rooted. I will try solving it with a rooted denominator though.',)####(\"@Glen_b thanks for having a look at my question. I've provided the whole solution to the problem. There is actually two methods to arrive at the answer. Is it possible to arrive at the answer ($-\\\\\\\\frac{1}{2}$) without assuming the all the y's have mean 0?\",)####(\"I don't see anything that establishes the y's have mean 0\",)####('Why do the implicit assumptions contradict one another?',)####('Substitute the definitions and use the linearity of expectation. For instance, the first term within the curly braces is $\\\\\\\\mathbb{E}(y_1y_2)=\\\\\\\\mathbb{E}\\\\\\\\left((\\\\\\\\alpha+\\\\\\\\epsilon_1)(\\\\\\\\alpha+\\\\\\\\beta+\\\\\\\\epsilon_2)\\\\\\\\right)$, which you expand algebraically into terms like $\\\\\\\\mathbb{E}(\\\\\\\\alpha^2)=\\\\\\\\alpha^2$, $\\\\\\\\mathbb{E}(\\\\\\\\alpha\\\\\\\\epsilon_2)=\\\\\\\\alpha\\\\\\\\mathbb{E}(\\\\\\\\epsilon_2)=0,$ and $\\\\\\\\mathbb{E}(\\\\\\\\epsilon_1\\\\\\\\epsilon_2)$ (plus others of similar forms). The latter is $\\\\\\\\mathbb{E}(\\\\\\\\epsilon_1)\\\\\\\\mathbb{E}(\\\\\\\\epsilon_2)=0$ due to independence of the $\\\\\\\\epsilon_i$. Evidently, your text is confusing the $y_i$ with the $\\\\\\\\epsilon_i$.',)####('This question is an obvious duplicate, which you seem to recognize. If previous threads did not provide the information you needed, you should state what you learned from them & what specifically you still need to know.',)####('Method 1 is plainly wrong concerning the expectations of the $y_i$.  In fact you can work out algebraically that $\\\\\\\\mathbb{E}(\\\\\\\\hat\\\\\\\\alpha\\\\\\\\hat\\\\\\\\beta) = \\\\\\\\alpha\\\\\\\\beta-\\\\\\\\frac{1}{2}$.',)####('@whuber could you please show me how you worked out $E(\\\\\\\\hat{\\\\\\\\alpha}\\\\\\\\hat{\\\\\\\\beta})$? I am stuck at the part with the curly braces. Do I expand them or can they be further simplified first?',)####('The terminology is all standard from any regression text, with the exception of `Residual standard error` which is the conditional variance.  Never did figure out why.',)####(\"What are the variables?  Also, usually Y is the dependent variable and X the independent variable (there's no rule that it has to be so, but it's pretty  common)\",)####('I will add, however, that if I assume a lag of 1 between each data point regardless of actual ing and run a Durbin-Watson test or a Box-Ljug test, I get non significant p-values suggesting no autocorrelation. However, I am skeptical that I can trust this result, given the uneven spacing.  Any thoughts?',)####(\"(1)have you looked at the residuals, done a lag1 test (Durbin-Watson), white-noise test (portmanteu) or looked at auto-correlation plots? (2) if this is OLS then an assumption that the residuals are not autocorrelated, in this case where there is high suspicious one should either prove they aren't or correct (3) there are a very large number of methods to correct/account for autocorrelation including praise-winston or cochrane-orcutt (if this is OLS)\",)####('In the presence of autocorrelation, the parameter estimate is unbiased, SE is biased (downward), as are t (upward) and p (downward) in favor of significance...I think?',)####(\"1) Residuals look reassuringly random.  I can't make an autocorrelation plot, I don't think because the results are unevenly spaced and autocorrelation assumes the presence of a time series object, which has spaced data. Furthermore I don't think lag1 is appropriate either for the same reason. 2) it is an OLS.  I i'm just not sure how to show residuals are not autocorrelated if the data structure violates the assumptions of autocorrelation tests. 3) I can't figure out if these would work for unevenly spaced data.\",)####('If there is a dependence structure in time, it is usually a good idea to think about modelling it explicitly. Time series people are defined by their thinking there is no other way to do it.',)####('The R-squared measure indicates \"fit\", i.e. how well the regressors \"explain\" the available data set, namely, the past. With time series data, in many cases we are interested in the future, in acquiring a model that would forecast adequately future values of the dependent variable. Fit and forecasting usually don\\'t improve together, and one should attempt to strike some sort of balance. What is your target here?',)####('Yes, given that it is time series, your strategy should change.',)####('Yes you are right. Edit is done. Most of the variables are physical  parameters such as: temperature, pressure, speed...',)####('To statistical people temperature, pressure, speed are variables, not parameters.',)####('Is this, perchance, time series data?',)####('@AlecosPapadopoulos   Okay, I edit my post to explain more the context of my problem.',)####('@NickCox Yes in this case it is. I just meant that those variables are physical. Sorry.',)####(\"@PeterFlom Yes indeed Y is a time serie. But I don't know anything particular about this field. Would it change the strategy to choose?\",)####('You have a deeper problem: the implicit assumptions in your multiple models contradict one another.  Why, then, are you conducting all these different regressions?  What are you really trying to accomplish?',)####('@AsymLabs - The log might be special in regression, as it is the only function that converts a product into a summation.',)####(\"It's reasonably straightforward if you start from the fact that the line goes through $(\\\\\\\\bar x,\\\\\\\\bar y)$ and write the slope estimator as a kind of average.\",)####('Let your data be $(x_i,y_i,z_i)$, $i=1, 2,\\\\\\\\ldots,n$. In the first regression, \"y=mx+b\", the implicit assumption is that the values $\\\\\\\\epsilon_i=y_i-(mx_i+b)$ are identically distributed but independent random variables (iid) with expectations of zero. The second regression assumes $\\\\\\\\delta_i=x_i-(m_0y_i+b_0)$ are iid. Plugging that into the first gives $\\\\\\\\delta_i=x_i-(m_0(mx_i+b+\\\\\\\\epsilon_i)+b_0)$. That simplifies to $\\\\\\\\delta_i=(1-m_0m)x_i-m_0b-b_0-m_0\\\\\\\\epsilon.$ Unless $m_0m=1$ the expectation of $\\\\\\\\delta_i$ varies with $x$ and even so its expectation is $-m_0b-b_0$; either is contradictory.',)####('Take a look at http://stats.stackexchange.com/q/22718 for more information about this asymmetry and http://stats.stackexchange.com/q/16381 concerning the assumptions you are making in your regressions.',)####('see http://stats.stackexchange.com/questions/88461/derive-variance-of-regression-coefficient-in-simple-linear-regression',)####('General answers have also been posted in the duplicate thread at http://stats.stackexchange.com/questions/91750.',)####(\"I don't recognize your formula from [Wikipedia's version](https://en.wikipedia.org/wiki/Akaike_information_criterion#Definition), but I wouldn't \u2013 I'm lousy at manual calculation. Don't worry about negative AICs though; see [Negative values for AICc (corrected Akaike Information Criterion)](http://stats.stackexchange.com/q/486/32036).\",)####('@user2806363 if you are satisfied with my answer could you please mark it as accepted so we can close the question?',)####('The **parameters to be estimated** are (multi-)linear. If you were *estimating* the values of the exponents, the estimation problem would not be linear; but squaring a predictor fixes that exponent at precisely 2.',)####(\"My understanding is that @user777's comment, as well as the answers below, apply not only to polynomial regression, but also any regression that uses a [bijection](https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection) of the predictor variables. e.g. any reversible function, like $log(x)$, $e^x$, etc. (plus some other functions, obviously, since 2nth powers aren't bijective).\",)####('Thanks everyone; all of the answers and comments were helpful.',)####('The same formula as used are using in this article page 13\\n[link](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-multreg.pdf)',)####(\"Why would you want that? What would it tell you that isn't in the summary of linmod?\",)####('I found [this page](http://www.duke.edu/~rnau/testing.htm) quite helpful, for a review of the assumptions and the ways of testing them.',)####(\"I think this should be left open; it's pretty clear what the person is *asking*; it's not clear *why* he or she wants to know.\",)####('Please explain your dataset a little bt, do you have n measurements of the independent variables and n variations of temperature in time and you would like t use for predicting the k-th all the k-1-th preceding ones?',)####('It would allow me to t-test whether r might be 0.',)####(\"Apologies, I was typing on my phone so couldn't Latex things correctly.\",)####('So would `cor.test()`, and the $t$-test of the slope coefficient (same test regardless of whether the variables are standardized).',)####('This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post - you can always comment on your own posts, and once you have sufficient [reputation](http://stats.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](http://stats.stackexchange.com/help/privileges/comment).',)####(\"i knew that, but since at the time I didn't have enough reputation I couldn't do it, but since I thought I could be of help I decided to answer, sorry if I acted wrongly, the intent was good.\",)####(\"No serious harm; just against site policy, hence the collective cleanup action. Hopefully we can just convert this to a comment for you, or if not, hopefully you'll find other ways to help :)\",)####(\"you can't gain anything from imposing arbitrary conditions on error terms. you gain something only if your new conditions better reflect the reality.\",)####(\"@Aksakal, I see what you mean, and the normal distribution represents reality, that's why we use it? Suppose we were to impose a bell curve shape distribution to $\\\\\\\\epsilon_{it}$ that had the appropriate bounded domain. Would this be a good solution to the problem? Suppose $Y_{it}$ was a person's age, and we choose a nice bell curve distribution for $\\\\\\\\epsilon_{it}$ that had domain $[0,150]$, this would be better than just using the normal distribution for $\\\\\\\\epsilon_{it}?$\",)####(\"if normal distribution assumption seems reasonable, then you use it. you can also test the assumption using Jarque-Bera or similar tests. in physical sciences normal assumption often works very well. there are bounded distributions, such as beta distribution. frankly, i don't understand what is the problem to solve here. bounded distributions have their own issues. the point is to come up with the distribution which reflects the reality, in your case it could be lognormal, for instance\",)####(\"@Aksakal, your making very good points here. Okay, suppose you were modeling student's performance on test score. Maybe you wanted to see how parental wealth had a impact on student performance, your model was $T_{it} = \\\\\\\\beta_0 + \\\\\\\\beta_1 W_{it} + \\\\\\\\epsilon_{it}$, and the test scores were in range $[0,100]$, what distribution would you use for the error term in this situation. My problem is that using the normal distribution is not realistic, I would suppose a bounded distribution from $[0,100]$ would be better.\",)####('in this case normal assumption may or may not work very well, depending on what is your goal. secondly, you seem to have a panel data set, not the cross-sectional data, which suggest me that MIXED effects regression, instead of simple linear regression. i would start with normal assumption and test it. you always start with simplest stuff and try to make it work before going for fancy things.',)####(\"it's STATS 3093: Probability and Mathematical Statistics II. (A third year course at a so-so Canadian university with an awful stats department). I'm a math major, (statistics minor). It's the 4th stats (13-14 week) course I've taken. The course had nothing to do with the topic, it was just sort of a project to do something that's (mostly) new to us to try to push us. Heteroscedasticity was in no way covered in any course I've taken.\",)####('Thx. Answer 1 to http://stats.stackexchange.com/questions/88461/derive-variance-of-regression-coefficient-in-simple-linear-regression helped me perfectly.',)####(\"I'm sorry, but it is in no way clear what beta 1 hat is! Can you elaborate on the model in question?\",)####(\"Yes, you should do multiple regression. But since your dependent is a proportion, you should probably use [beta-regression](http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf). I don't think you should try correlating the dependent against all explanatory variables independently.\",)####('What do you mean that this is for a paper? Are you writing a paper on heteroscedasticity?',)####('Guess I should have worded things better. The paper is on the topic of \"How should you fit ANOVA and linear regression models, if the equal variance assumption is violated?\"\\n\\n(that topic, word for word, was assigned to me by my professor)',)####(\"Our policy is not to provide direct answers to questions that pertain to people's class assignments, but to provide hints to help people get unstuck. To understand the process more fully, see the [wiki](http://stats.stackexchange.com/tags/self-study/info) for the `[self-study]` tag. You should probably be able to learn a lot by searching the site & reading relevant threads, though.\",)####(\"What class is this, I wonder? Who assigns a 20 page paper on how to fit an ANOVA w/ heteroscedasticity? I've never heard of such a thing. Are you a statistics major at Harvey Mudd?\",)####(\"I had an identical problem, solved it with R's `segmented` package: http://stackoverflow.com/a/18715116/857416\",)####('X1=68 Y1= 55 X2=54 Y2= 38 etc...',)####(\"My first question, would be, what tool would you prefer to use to do a linear regression? Would love to help with a solution that can show you how to do it vs just doing it for you (which I don't think will help in the long run).\",)####('Dear Nathaniel, actually i have no idea what is the best tool to use :((',)####('Please have a look at the solution provided (needed a bit of a distraction for a moment), generate in excel. While I would rather help teach folks how to use the tools and interpret, I hope that this helps. If it does, please feel free to accept it.',)####('Cross-posted: http://math.stackexchange.com/questions/766939/what-is-rm-cove-i-hat-y-i-in-simple-linear-regression',)####('I cannot understand your last sentence. It may help you to read: [What is the difference between linear regression on y with x and x with y?](http://stats.stackexchange.com/q/22718/7290)',)####('The typical issue with identifiability in data analysis is when you have sets of equations to solve (eg, in SEM) that share the same variables & there are more variables than equations. Thus, there are multiple possible sets of parameter values that will reproduce the data exactly, & you cannot tell which set might be a reasonable one to pick. I cannot tell if that is what you are asking about. You seem to have really just 1 equation. If you have multiple paired data for X1 (a predictor) & X2 (a response), you can solve for a1 or a2 using regression methods.',)####('The answer depends on what assumptions you make about the independent variable in each model. When $X_1$ is assumed normal in the first model or $X_2$ is assumed normal in the second and $\\\\\\\\varepsilon$ is assumed independent of the independent variable, then both are equivalent to the standard bivariate normal model (with zero means).',)####('the context can be compared to SEM. I have many regressions between variables in $X$. Variables on the right of the regression and the residuals of the regressions are independent. The objectif is to find the structure of the generative sem knowing only $X$ the set of covariates. So the question is : \"are there several (linear) structures leading to the same distribution for $X$ ?\" This questions stands for X all Gaussian and then (if necessary) $X_1$ following Gaussian Mixture models.',)####(\"What's alpha? Your regression doesn't contain one.\",)####(\"What are the column headers? What is your N? What are you trying to do? Also, stepwise is a bad method, but that's not your question\",)####('The headers are the usual: B, standard error, Beta, t, Sig., and correlations (zero, partial, part).\\n\\nStepwise maybe bad method, but it SHOULD drop variables instead of leaving empty the significance.\\n\\nN is 40, and I am trying to get an interpretable result. :)',)####('Dear Eupraxis and Maarten, thank you for your response: **@Eupraxis** - Here is the picture, the way I understood your explanation about using function prediction intervals to calculate 95% CI bounds of estimated X value. Is it what you proposed (sorry for hand drawings)? ![enter image description here](http://i.stack.imgur.com/cZafb.jpg) What if my function cannot be re-expressed in linear way? I am not even sure how to do it for logistic curve anyway. Please also see my comment for Maarten. **@Maarten** - Thank you for the article! I tried to read it, but frankly it was quite hard to compreh',)####('Could you please [merge your two accounts](http://stats.stackexchange.com/help/merging-accounts)? A moderator will convert this non-answer as an edit to your question, unless you want to do it yourself.',)####('This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post - you can always comment on your own posts, and once you have sufficient [reputation](http://stats.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](http://stats.stackexchange.com/help/privileges/comment).',)####('In R I\\'d use `read.table(stdin())` to read the data into a data frame, copy-pasting the data direct from this question (followed by return on a blank line to end data entry), then `lm` to fit the model (assigning to a model object), and then `predict` to get information at $x=100$ and - assuming that by \"standard error\" you mean residual standard error - `summary` on the fitted model object to get that. But the sequence of actions will be different for every tool you use, and there are many good tools available.',)####('Might help if you share the model output.',)####('What exactly do you mean by \"error for linear regression\"? Do you mean an estimate of the variance of the errors, standard errors for the coefficients, standard error of prediction, or something else?',)####('hi whuber, the question is incomplete. I was searching for the RMS error of regression. I found it here http://www.stat.berkeley.edu/~stark/SticiGui/Text/regressionErrors.htm#rms_error_of_regression . Basically it is the vertical residuals.',)####('Since you have $\\\\\\\\hat y_i$ on the left, your right hand side should have $\\\\\\\\hat \\\\\\\\beta$\\'s. While you\\'re editing, please remove the \"e\" from the end of the word \\'formula\\'; you have only one formula so you should not use the plural.',)####('\"ENTER\" methods are not based on sound statistical principles.  Avoid that at all costs unless you use the bootstrap to find out how poorly the method performs and you are still satisfied with the result.',)####(\"What do you mean by 'reliable'?\",)####('Have you checked the possibility of high-degree multicollinearity among your regressors?',)####(\"The point that @jbowman is trying to make is that $\\\\\\\\hat{Y}$ is a function of $\\\\\\\\hat{\\\\\\\\beta}$ NOT $\\\\\\\\beta$, so even though you are confident that $\\\\\\\\beta=0$, that doesn't mean $\\\\\\\\hat{\\\\\\\\beta}$ is 0 (it will probably be close, but not exact.)\",)####(\"thank you all. however, a new question appeared. I thought correlation only measures the strengh of the relationship. but it is the same as the standardized beta if I only have one predictor. this means it also measures how y changes if x changes. why can't it be greater then one then, let's say for example x increases by 0.5 standard deviations and y then by 1,5 standard deviations.\",)####(\"I've edited your post to include LATEX formatting. Please double check that it still says what you want.\",)####(\"You could always read this: [Coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination), and then refine your question in the light of what you don't understand.\",)####(\"You don't need to remove correlated predictors. In fact, they're almost always correlated.\",)####(\"Technically, in OLS you can also test H$_{0}\\\\\\\\text{: }r=0$ (where $r=\\\\\\\\sqrt{R^{2}}$, or, if your prefer to be pedantic $r = \\\\\\\\text{sign}(\\\\\\\\beta)\\\\\\\\sqrt{R^{2}}$), and you can also go wild with a test of H$_{0}\\\\\\\\text{: }\\\\\\\\alpha=0$. Just sayin'...\",)####('Velcome to the site!',)####(\"The problematic nature of the last statement becomes clear when you consider that first, distributions do not have standard errors: they have standard deviations; and second, if that's what you mean, any $t$ distribution with 2 or fewer degrees of freedom does not even have a standard deviation.\",)####(\"This result is the [Cauchy-Schwarz Inequality](http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality#Statement_of_the_inequality) in disguise: the standardized $x$'s and standardized $y$'s can be viewed as unit vectors (in a space whose dimension equals the data count, but that doesn't matter because according to Euclid two vectors--which implicitly include their common origin--determine a plane or just a line). The standardized $\\\\\\\\beta$ is the dot product of these vectors, which is the cosine of the angle between them. Cosines of angles always lie between $-1$ and $1$.\",)####('You say you \"know\" the subgroups exist and then you say you want to \"find out\" if they do. Do you mean you want to find which subjects go into which cluster? Or something else?',)####('Sorry i was not clear there, i want to assume i dont know subgroups exist and i want to find out whether they do or not.',)####('Can you clarify your question / the thinking behind it? Note that regularization can be seen as the application of a prior centered on 0.',)####('@gung: Yes. My question is that how to achieve small error if you apply linear regression for a given data set. Because the problem of linear regression is that overfitting. So we want to avoid that term by other term. How to avoid it?',)####('@gung: Please see my edit question',)####('If this is a self-study question please add the appropriate tag.',)####(\"Oh my model was lm( yVar^2 ~ xVar+xVar+xVar ), i transformed it by squaring because otherwise the residuals weren't normally distributed. The Y^2 label is a bit confusing though as it seems like the Y should be the transformed Y already.\",)####('Which sign do the resulting parameters have for each variable? Ara all of them numeric variables or are there factors?',)####('You should only have two dummy variables for the three shops.',)####('What makes you think this data fits a nonlinear function and what function do you think that is? To me it looks pretty much like a horizontal line with a couple of outliers are 0.',)####(\"why does your x-axis have the label `Y^2`? That would imply you're either squaring $y$ or plotting $\\\\\\\\hat{y}_2$ -- and in either case that's not sounding like what your text says it is. Is your y-variable really called `Y^2`??\",)####('What is the size of your sample? What is the goal of your model? (inference or prediction in future samples)',)####('You have described a lot of useful things--except for the most important, which is the purpose of your additional experiment.  Presumably it will be testing some hypothesis.  What is it?',)####('You have two good answers below. If you want more information, it may help you to read my answer here: [Linear regression prediction interval](http://stats.stackexchange.com/a/33642/7290), which pertains to prediction intervals, but the idea is very similar.',)####(\"@whuber, I tried to answer your question with an edit to my own. Basically, my new observations would be meant to test a hypothesis that another variable affects y. I phased this as a new variable *q*, but it could also be one of my *x*s at a values outside the range it's been seen historically. I know that in order to design any experiment that I need to know or assume something about the random variation that will be present. What I'm wondering is how my regression results help me fill in the necessary assumptions. Thanks.\",)####(\"There's a detailed intuitive explanation given in this post: [Shape of confidence interval for predicted values in linear regression](http://stats.stackexchange.com/a/85565/805)\",)####(\"Can you add some more details to flesh this out? I'm not sure I follow your question.\",)####(\"Is this a real experiment or an exercise? If it's the latter, please consider adding the [tag:self-study] tag.\",)####('It is not clear what you want. You finish with *\"The mean of this production rate is compared with the mean of the initial production rate\"*. Do you want to know how to compare those means? Or how to get them?',)####('N=44, not perfectly balanced accross factor levels. The goal is not predictions at this stage, just finding the best way to model sample data variance.',)####('TA for the helpful answers and the excellent links.',)####('Are you referring to the intercept of your model being nonzero?',)####(\"If I had to guess, I think OP means that $\\\\\\\\hat{b}=-0.3$, when he has substantive knowledge that it should be zero, but that's meeting OP more than half-way. One way to enforce $b=0$ is to simply not estimate an intercept term in the model, which is the same as fixing it at precisely zero.\",)####('The question is unclear. Please try to edit to make what you want more explicit.',)####('Are you trying to model a structural change in the data? If so, then the package [strucchange](http://cran.r-project.org/web/packages/strucchange/index.html) might be appropriate.',)####('There are so many possibilities...could you be more specific as to what you are trying to achieve?',)####('possible duplicate of [When is it ok to remove the intercept in lm()?](http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-lm)',)####('You will also want to read: [Removal of statistically significant intercept intercept term boost $R^2$ in linear model](http://stats.stackexchange.com/q/26176/7290).',)####('There are several votes to close this thread as a duplicate of one of those referenced in the preceding comments.  Those votes are valid based on (1) and even (2), but (3) looks new.  I would therefore like to suggest that respondents focus on the third question.',)####('Describe polynomial order and how higher orders may be required for a specific application?',)####(\"It's not quite clear to me what you're asking for.\",)####('I\\'m stuck just trying to understand the data.  What do you suppose those *negative* \"concentrations\" mean?',)####('My apologies for the cross-over with other questions, however I found the questions I looked for, including the one linked, did not address specifics, such as where you expect an always positive value, I am happy for the focus to be on the third point though, as that is the most important.',)####(\"More use to ask candidates technical questions that naturally arise from discussion of their past experience & the job you'll be wanting them to do than to set them a quiz. You risk making the interview boring & putting them off working for you.\",)####('The data of $Y$ were logarithmized.',)####('My question is: how do i interpret the estimate b. Does this mean that a 1 percentage point increase in X, increases the growth rate by 0.07 percentage points?',)####('What\\'s \"CC\"? Do you mean \\'correlation coefficient\\'?',)####('Much clearer, thanks.',)####('Are these percentages continuous (like the percentage of cream in milk, for example), or discrete (like binomial proportions, a count in some category out of a total count)?',)####(\"Uhm... i don't get the difference. Aren't they both continuous?\\nAnyway I think the second describe better my data, since we are speaking about people out of at total.\",)####(\"Did you notice that $0.66^2 = 0.44$?  As far as your last remark goes, there's nothing strange about that because (presumably) both regressions involved an unnamed dependent variable I will call $Y$: the $0.68$ and $0.10$ values describe *only* how $A$ and $B$ are related to $Y$, not to each other. You can find [plenty of explanations of such phenomena](http://stats.stackexchange.com/search?tab=votes&q=regression%20significant) on our site.  [This thread on correlation](http://stats.stackexchange.com/questions/5747) also looks relevant.\",)####(\"@whuber I did, and thank you for the links.  There is no need then for a strong correlation between two variables to suggest that they produce the same 'strength' of effect when regressed on the same dependent variable.\",)####('I believe this question is answered at http://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model/9144#9144, which provides the correct formula for regression prediction intervals.  Implement that and let us know how it compares to the `R` output.',)####('The distribution of counts divided by counts is definitely discrete. Indeed, the numerator is usually modelled as a binomial, the denominator is conditioned on (treated as constant), so the ratio is usually treated as a scaled binomial. However, even if the denominator was alse a random variable, the ratio would still be discrete since its sample space is countable',)####('I wonder if the regression is using sampling weights (weights that denote the inverse of the probability that the observation is included because of the sampling design) and the t-test is using importance weights. That is what I get with Stata using your data.',)####(\"a correlation of 0.44 isn't particularly strong. It's perfectly possible to have three variables with $\\\\\\\\hat\\\\\\\\rho(y,a)=\\\\\\\\sqrt{0.68})$,$\\\\\\\\hat\\\\\\\\rho(y,b)=\\\\\\\\sqrt{0.10})$, and $\\\\\\\\hat\\\\\\\\rho(a,b)=\\\\\\\\sqrt{0.44})$ for example.\",)####('Because there are myriad ways to express PDFs, a good solution should aim for a representation that is suitable for your purpose: what, then, is the objective of this exercise?',)####('Ok, the main thing is that I need to be able to integrate the pdf * a gaussian (I added the expression above) analytically, uniquely for each pdf.',)####('Where do these PDFs come from and in what mathematical form are they currently represented?',)####(\"Yep... ok just did it numerically, it's not exactly one, but it does not get far away from one. For his starting values, at $x=10^{50}$, $y=1.033$.\",)####(\"It's a probability of magnification, they're currently just histograms (these are gaussian kde plots). Physically I think there is motivation to use a log-normal distribution, but that's hard to convolve with the gaussian.\",)####('So what you are really saying is that you have batches of univariate data: sets of *numbers,* not distributions! That makes all the difference because it opens up many more options for the solution. Please consider editing your question to include an explanation of these data and of why you will need to perform the convolutions.',)####('Your problem is the best fit will be obtained when `a` and `b` are huge and `c` = -0.5',)####(\"Yes, we are agreed that the starting values aren't much good. But that's not the main problem, since other starting values can be chosen easily, and that doesn't happen with better starting values. See what happens in your graph when a=50000, b=1000, and c=-0.5 --- yet he still has multiple problems.\",)####('Your function (given its starting values) is undefined for non-positive $x$ and is equivalent to $y=1$ for positive $x$.',)####('@Glen_b yup... at least in my graphing software for values of x up toward the order of $10^{1}$ on up to $10^{22}$. Or... Hmmm wait, that should be \"for negative\" and \"for non-negative\". My bad.',)####('What do you mean by range of errors? How those range of errors are computed?',)####('You will get better advice if you state why you want to constrain the coefficients. Because you can take any linear transformation to the variables and change the coefficients (but still have the same fit) the constraint is not always meaningful unless transforming the variables would also change the constraint.',)####('the range of errors was computed as follows : abs(min(actual-prediction)) + max (actual-prediction))\\nthe everage of errors was computed as follows: avg(actual - predicted)',)####('What do you mean that the \"models have different Y axes\"? Are they different variables?',)####(\"The mean of the errors should be zero in general, otherwise your model is biased. It would help a lot if you would say how precisely you calculate average mean (I've just realized that this is a bit of misnomer, since average and mean are for practical purposes synonyms) of errors, and how your errors are defined precisely.\",)####('@Glen_b I rephrased my question, I hope this makes it more clear.',)####('Yes, one set of comparison was for models having different response variable but same predictor. But I think it is not possible to do such a comparison.',)####(\"1. Your model omits an error term. If there's no error (observed data exactly equals your no-error RHS), significance can't even come into it. 2. Do you account for the serial correlation in your model?\",)####(\"Fixed. Autocorrelation is not required, I'm really more interested on how to practically blend the two models/predictors in one model.\",)####(\"If there's no time-correlation issue, how does fitting at 20 minute intervals instead of 10 minute intervals improve things?\",)####('The two $X$ variables are completely different. $X_2$ works better when applied to a longer term forecast because it uses a different set of data points unrelated to $X_1$ or $Y$.',)####(\"I don't see how that helps. If there's no time-correlation issue anywhere (everything is independent of everything at a different time to it), going to half the frequency simply halves the amount of data.\",)####(\"Why are you not concerned that **X2** is only significant at 20 minute intervals? Have you looked at other intervals and *not* found significance? NOTE: I'm not saying that you *should* be concerned--it's not a red flag... Pink? Maybe...\",)####(\"@SteveS Yes, that's the kind of answer I was looking for. I read about averaging models too, was wondering if there are other techniques. (I am choosing the Y sampling interval to better fit the models).\",)####('@SteveS I should have wrote $R^2$ rather than significant. Please check my question again, I edited and added an example.',)####(\"Well, I was just sort of throwing that out there... However, my advice would be to keep it simple (i.e. a single model incorporating both **X**'s). Remember: That higher R^2 could be purely due to chance--a result more of *data dredging* then anything else. Plus, recall that it's the out-of-sample **RMSE** that you should care about, not the **R^2**.\",)####(\"@SteveS I'm interested in the claim made there that the insight about model averaging comes from machine learning. Recently here (I think in a comment thread) I pointed out to someone that in statistics the practice (model averaging over potentially less complex models instead of selecting a single model) goes back many decades.\",)####('non-linear regression is a huge field! You should say something about your preferred application area, which will make it easier to give advice.  A similar question (with answers) http://stats.stackexchange.com/questions/25608/non-linear-regression-references        http://stats.stackexchange.com/questions/8570/references-on-numerical-optimization-for-statisticians/10166#10166',)####('Most of the time I really hate it when people go on and on about their datasets (\"*Just get to the damn question already!*\"). However, this may be one time where explaining a little bit about the type of data you\\'re working with (e.g. financial? biological? *Tweets*??) could really help sort out the best approach to take as far as choosing the right model & timescale goes...',)####(\"@Glen_b: Well, you'll have to take that up with the author (Hal Varian)... Actually, I think he's not trying to credit the *machine learning* crowd with *inventing* the technique, but rather to suggest that they're doing a great job of actually implementing this idea *in practice*. Also note: It's an Economics article; hence, I think he's saying that with economists in mind (and not trying to make a general statement)... In other words, you make a good point.\",)####(\"First, I'm not quite clear with about this point: Is the data *given* in 10 & 20 minute intervals or are you choosing these two intervals? Also, why are you running two separate regressions instead of a single regression with your two independent variables?\",)####(\"@Glen_b: My mistake for taking that quotation out of context... Anyway, unless you really can't stand Economics (you certainly would not be alone), you should definitely check it out--Varian is the chief economist over at Google (so the dude knows about Big Data). Plus, the particular journal is geared towards non-specialists so you won't find a more readable academic journal in Economics.\",)####('Last, I recently read [this](http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3) article (and really liked it!) on **Big Data**--your question made me think of this line: `\"An important insight from machine learning is that averaging over many small models tends to give better out-of-sample prediction than choosing a single model.\"` Granted, \"many\" could be *hundreds* (i.e. not 2). Still, I thought the line might be relevant...',)####('1. The two variables you gave in your data (Wavelength, aCDOM) do not match the two variables in your model (St0104, Wavelength). $\\\\\\\\,$ 2. the $Y$ variable in the data that you have supplied is discrete. Indeed, it appears to be integers between 31 and 35, all scaled by a constant. What is it?',)####('hi sorry I should have mentioned the name of the y variable is St0104 the values are actually the absorbance of CDOM at 440nm over the wavelength range of a sample called St0104.I only gave a small set of the full data as it was quite long. X values are wavelength Y are the a440 (absorbance at 440nm).',)####('maybe `glm` could be an option?',)####('How is absorbance measured? Why are the values all multiples of a constant?',)####('(1) The residuals will be awful because the $(x,y)$ data trace out a step function (having only five levels). (2) The narrow range of y-values (varying less than $\\\\\\\\pm 6\\\\\\\\%$ from their middle) indicates there is little difference between fitting $(x,y)$ or fitting $(x,\\\\\\\\log(y))$. (3) One would guess the $y$ values are either counts or rounded decimals that have been converted (via some kind of calibration information) to absorbance values. A maximum likelihood estimate suitable for *the raw data* ought to converge and do a good job of estimating $b$.',)####('The Y values were calculated by choosing a wavelength of interest (in this case 440nm) and using the formula a440 = 2.303(A440) where 2.303 is a constant and A440 is the (corrected) absorbance at that wavelength. I used a spec fluorometer to measure absorbance of the sample. I am trying to calculate the spectral slope (SCDOM) which is estimated from the slope of an exponential curve of a440 as a function of wavelength. Again I am new to R and stats so I find R a bit confusing. I tried this regression in Matlab and it works fine if I use  exp function with 2 terms but I cant figure it out in R.',)####('hmmmm not completely sure about that, maybe this can help? http://www.r-bloggers.com/linear-models-with-multiple-fixed-effects/',)####('glm has weights, but does it have anythiong similar to STATA\\'s \"absorb\" that allows me to include fixed effects but repress them in the output?',)####('See [Where do the assumptions for linear regression come from?](http://stats.stackexchange.com/q/55113/32036)',)####('A good place to start would be by researching \"Anscombe\\'s quartet,\" which is shown in the answer at http://stats.stackexchange.com/a/16131.',)####(\"See [here](http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression) and [here](http://stats.stackexchange.com/questions/17585/assumptions-needed-for-multiple-linear-regression) and a number of other posts (search is useful). However, note that if you're not doing any inference (testing, calculating standard errors/interval estimation), a number of the assumptions aren't required, though you may lose efficiency if some of the not-required ones don't hold.\",)####('If you are having difficulties with interpreting the output after excluding the intercept, then you probably do not have the statistical background that supports excluding the intercept in the first place.  I suggest this question belongs on CrossValidated.com',)####(\"Why would you want to exclude the intercept? That's a dodgy move in most circumstances...\",)####('Among other things, i want to simulate the Dickey-Fuller distribution by montecarlo methods in order to compute critical values.',)####('This is the subject matter of [circular statistics](http://stats.stackexchange.com/questions/105641/best-transformation-for-sinuous-data-sets). How to include these values depends on whether the aspect is explanatory or a response variable; presumably it is explanatory. In this case consider re-expressing aspect into a value more directly related to sun exposure, for which you say it is a proxy.  For instance, the pair (slope, aspect) can be converted into a rough numerical assessment of mean total daily insolation per unit area.',)####('You are partly right: The smaller the pairwise differences, the closer the points will scatter around the line through zero with slope 1. But your two analyses are not similar, for instance the t test only checks for *mean* differences, not for individual differences.',)####('**This is an irreproducible result:** when I run your code with your data and starting values, `nls` completes successfully with a reasonable set of estimates.',)####('What do you mean by \"regression weight\" there?',)####('The \"estimate\" output of the summary(lm()) function - is this correct?',)####(\"I'm not sure what you mean by correct. Are you asking if it's okay call the estimate of the coefficient a regression weight? Some people call them that, but the phrase 'regression weight' has more than one possible interpretation, which is why I asked for clarification of what you intended.\",)####(\"What's the dependent variable?\",)####('I understand, thank you. Then: Yes, I mean the estimates of the coefficient. I will change that in the original question.',)####('Both the dependent and the independent variable are chronological age measured in years',)####(\"Thanks Nick, your description of the dataset is correct, basically each subject has a certain mean reaction time (Y, expressed in ms) for each condition (X=1 through 9). Although I've never done regression in Statistica before, I think that a table like the one you described would probably allow me to get a mean regression slope for the dataset, but the problem is that (I thought) having repeated measures across rows is not statistically correct. Is this where I am wrong? This is the reason why I had each of the 9 means arranged as columns as opposed to making a dummy variable out of it.\",)####('On this site we can provide theory and related advice, but we are not a coding resource.  If the former is what you are interested in, then please edit this question to supply additional details of the situation.',)####(\"They don't test the same thing at all. The regression could indicate how much more sensitive the paired t-test would be compared to an independent one but the coefficient can be substantial with absolutely no paired t-test effect (and vice versa).\",)####('@Michael Mayer. Can I say that the two methods are testing against the same null hypothesis, so that interpreting the p-value (whether mean of difference is 0 and whether the coefficient of B is 1) should be consistent between the two methods? When interpreting the estimates of the two methods, they are different, because as you said, t test only checks for mean differences, not for individual differences.',)####('No, the null hypotheses are not equivalent (imagine a \"X\" shaped scatter plot: the mean difference will be around zero but the regression slope is 0, not 1). What I said is: If the individual differences are small, then the regression slope will be around 1. That\\'s the only relation between the two approaches. They investigate different aspects of the relation between X and Y.',)####('@John, thanks for the comments. Can you give me more insights about your comments? I am still bit confused.',)####(\"@Michael Mayer. Thanks. So the paired t-test is testing the intercept (against 0) of the linear model when fixing the slope at 1. The other method is testing the linear model slope (against 1) when fixing the intercept at 0. They are testing different aspects of the linear model. Therefore, to better illustrate the linear relationship, it's better to use the full model, y=kx+u, right?\",)####('Bear in mind that inferences on transform data cannot be validly applied to non-transformed data because $f(\\\\\\\\sigma^{2}_{X}) \\\\\\\\ne \\\\\\\\sigma^{2}_{f(X)}$. So if you find statistical evidence that $f(X) \\\\\\\\ne f(Y)$ you **did not** just find statistical evidence that $X \\\\\\\\ne Y$.',)####('I don\\'t get you. The only way you can run a paired sample t test with a regression in R is \"lm(X - Y ~ 1)\" and then check if the only parameter is zero or not.',)####('@Michael Mayer. Yes, in your formula X indicates the measurements, Y indicates the treatment factor (A or B). In my formula, X indicates the measurement from treatment A, and Y indicates the measurement from treatment B. Therefore, the linear relationship between the two treatment can be formed as $y_{i}=kx_{i}+\\\\\\\\mu+\\\\\\\\epsilon_{i}$. In terms of paired t-test, we fix $k=1$, then $y_{i}-x_{i}=\\\\\\\\mu+\\\\\\\\epsilon_{i}$.',)####(\"In my formula, X and Y are as in your formula. It's easy to run and check if it is similar to the t-test.\",)####('This question appears to be off-topic because it is about how to use MATLAB.',)####('Check the answer to [this](http://stats.stackexchange.com/questions/58107/conditional-expectation-of-r-squared) question.',)####('I think it depends on if the predictors are orthogonal. If they are not then more predictors can make the result better just by chance.',)####('Edited as per the comments. Thanks.',)####(\"Know when to stop what? Why not just use all $N$ variables? If you're worried about over-fitting you can use regularization with cross-validation to understand and maximize your model's predictive power. Please explain why you only want to use a few variables.\",)####('Do more variables always help? I thought there was a curse of dimensionality and adding more degrees of freedom is not always good. This question I have is can you know what the optimal result is beforehand, and then decide to stop say when you are close enough to optimal? Is there a global optimum?',)####(\"More predictors means fewer degrees of freedom, especially if you delete cases with missing data listwise (which hopefully you don't have to). I don't think there is a global optimum though, except in theory: $R^2\\\\\\\\le1$, AFAIK, and adjusted $R^2$ should be lower...\",)####(\"If the $X_i$ are really six orders of magnitude apart, it sounds like they are different things altogether or at least have been expressed in different units of measurement. Wouldn't that make the slopes automatically uncomparable? (2) Why are you not just doing a multiple regression of $Y$ versus $X_1$ and $X_2$? Is it (perhaps) because you have no data for which $X_1$ and $X_2$ are simultaneously available? (3) Exactly what F-test are you referring to?\",)####('Check out the `car` package.',)####(\"You should always look at the data before you run a regression, yes.  Doesn't have to be a histogram, though.  In the two-variable case you describe, I would plot a scatterplot (maybe with a histogram on the margins).  And look at it for a while.  Because maybe there's a crazy outlier observation.  Because maybe $y$ only takes on three values.  Because maybe the $y$ are all positive, or all between 0 and 1.  Because maybe the relationship between $y$ and $x$ looks less like a line and more like a hump or a circle or a hyperbola.  Or ...  Who knows?  If you don't look, you certainly won't know.\",)####('Which robust standard error do you mean? Please clarify.',)####('@QuantIbex does that dependence necessarily imply $E[\\\\\\\\varepsilon_i\\\\\\\\varepsilon_j]\\\\\\\\neq 0$?',)####(\"Can you clarify what you are asking about? I don't think this question is answerable in its current form. I am aware or robust  'sandwich' errors, eg, but those are for you betas, not for predicted y.\",)####(\"Are you looking for a way to calculate a standard error of the estimate that is robust (against what)? If so, I'm not aware of and haven't been able to find a method, but either way, this question really needs clarification. It does no good to post an answer in the negative to address only one possible interpretation...\",)####('If you mean \"heteroskedasticity-robust,\" then by definition it can\\'t be a single number.',)####('@kjetilbhalvorsen Thank you! I added a few details to the post. Since I have no experience with bootstrapping I would like to use the delta method. It looks like I would need the covariance between $\\\\\\\\bar{y}$ and $\\\\\\\\hat{\\\\\\\\beta_2}$ and $\\\\\\\\hat{\\\\\\\\beta_3}$, respectively. Do you have any suggestion how to compute this?',)####('The estimates are found as described at http://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re.  With these in hand, any test you want is obtained in a standard fashion.',)####('What is the nature of your items and response variable? Likert scale? If this is psychometric test are there correlated items (covariates)?',)####('You can read about the method here:  http://en.wikipedia.org/wiki/Delta_method',)####('Great, thank you!',)####(\"That's a difficult question to answer--but is best addressed at the other thread you started.\",)####('Variances of ratios can be tricky!  In particular, if $\\\\\\\\bar{y}$ can be zero there is obvious problems.  If the probability that $\\\\\\\\bar{y}$ is close to zero is so small that can be neglected, you could try bootstrapping (or simulation).  In that case you could also try the \"delta method\", that is, a linear taylor approximation and then calculate the variance in that linear approximation. But if $\\\\\\\\bar{y}$ can be close to zero, that can be very wrong.  For a better answer, we need more details.  Give us much more information about your data and model!',)####('Do you need to find the value of the deterministic parameter as well or do you have it avalilable?',)####('The variables are measured on likert scales, but there is not psyochometric scale underlying the variables. The covariates are correlated to some extent without explicit planing or knowledge about a scale.',)####('Thanks -- supose sample size n varies over cells of the variance-covariance matrix (for example due to item nonresponse and more particular due to the problem described in this question http://stats.stackexchange.com/questions/110559/estimate-linear-regression-using-items-randomly-selected-from-an-item-pool?noredirect=1#comment212379_110559). Should the relevant tests be adapted somehow?',)####('By the way, my current intuition is to multiply impute missing outcomes a large number of times. The fraction of missing information might be high, but given data are MCAR, estimates of paramters and their standard errors should be unbiased given large number of imputations.',)####('I figure that in this situation multiple imputation might be another approach to deal with the problem (see my comment on the other thread).',)####(\"Thank you for showing what you've tried so far & where you are stuck. Our policy is to provide hints to help get you unstuck & to the point where you can do this on your own. For more info, you may want to read the [wiki for the self-study tag](http://stats.stackexchange.com/tags/self-study/info).\",)####(\"Yes I need to find them. I'm trying to do something based on this stat.duke.edu/~fei/samsi/Readings/06TomaReich.pdf but just on a simple linear regression model with time series output and input variable. If it helps, you can assume that the range of the deterministic parameter is given. Please help me. I'll be grateful. Thanks. @davidhigh\",)####('Please check the first line of your quoted question. Is there something missing after $\\\\\\\\beta^T$?',)####(\"What's your estimate of $\\\\\\\\beta^Tx$? Can you use basic properties of variances or expectations to compute its variance?\",)####(\"How are you doing on this? In your attempts I see both $x^Tx$ and $X^TX$. But $x$ is a vector and $X$ is a matrix, so you've got a problem there. Also I think you want $Var(x^T\\\\\\\\beta)$, not $Var(x_i^T\\\\\\\\beta)$\",)####('This is a theoretical question. There are no concrete values. In my  post, I talked about computing $Var(\\\\\\\\Sigma_{j=1}^k\\\\\\\\beta_jx_{ij}|x_i)$ directly, but it is difficult because the $\\\\\\\\beta$ can be correlated which creates one big mess.',)####('I edited my post so that it specifies that $x_i$ are nx1 vectors',)####('The quotation is a summary of the detailed explanation that precedes it.  You could take the preceding paragraph as @Cardinal\\'s *definition* of \"strong mean offset.\"',)####('I like definitions by \"X is sth, that...\". Taking the paragraph does not change anything - I still don\\'t understand, that\\'s why I decided to ask a direct question.',)####('OK, so something more informed might be helpful. What is `variable`? Is is reasonable to take the `sqrt` of it? To `log` it? To do something like `I(variable/1e6)` maybe? Try to think easily interpretable transformations before doing something *heavy-handed* like a Box-Cox.',)####('Well, I have used scale() to make predictor variable with mean equal 0 and sd equal 0 - qqplot still looks the same, any other ideas for rescalling?',)####('This QQ-plot strongly suggests you have a heavy-tailed distribution. The \"most vanilla\" rescaling would be to first try to make your data zero-meaned and having std.deviation of 1; this is not a final solution just a first step. Try it and see if your model behaves better.',)####('Box-Cox gave me lambda close to 0 so this mean log transformation, still nothing close to normality in qqplots of residuals',)####(\"1) Don't confuse the ANOVA procedure with the ANOVA table; you can have ANOVA tables in more places than that. 2) ANOVA is simply a special case of regression in any case.\",)####('The error term is always included when fitting a regression. You just need to specify the non-random component of the model (b0 + b1*x1)',)####('Yes, but I want to model the error term in the regression. Therefore I want to include it. For example y = b0 + b1*x1 + 30*epsilon',)####(\"If $\\\\\\\\epsilon$ is fixed, e.g. to mean 0 variance 1, then the $b_2$ estimate is what `lm` gives you as 'standard error' in the summary output, ie the conditional variance.  If it's not fixed then $b_2$ is not separately identified in $b_2\\\\\\\\epsilon$ without more information.\",)####('Thanks, well on this particular dataset the linear post-treatment gives a higher cross-validated prediction error, but I was wondering if there was perhaps a more general answer.',)####('Have you cross-validated with and without it? What about holding a separate \"calibration\" set of data that learns the linear model from the $\\\\\\\\hat y_{held}$?',)####('Could you precise something about your data ? What is the dimension of the data ? What about the goodness of fit of the OLS regression ?',)####(\"There's not enough information here to discern what's happening. A reproducible minimal example (i.e. the smallest example that shows the problem you describe) might throw light on what's happening.\",)####('Your question seems rather vague. There may or may not be issues, depending on the circumstances.',)####('Could you tell us a little about your motivation for using $x_1/x_2$ in this model? For instance, is it suggested by theory, or did it emerge from a principled exploratory data analysis, or did it perhaps turn up after trying lots of different models?',)####('Gravitational force is inversely proportional to squared distance, but proportional to masses.  If your experiment was about gravitational force and your variables were mass and distance then you would find strong causal relationships between both, but their ratio would be more informative.',)####('In any event, be particularly careful if there are any small values of x2.',)####(\"Without some further diagnostics (eg. $\\\\\\\\beta$'s conf. intervals, plots of observed vs fitted, residual vs fitted, $R^2$, etc.) it is a bit hard to say. Please provide some additional diagnostics first. In general, have you thought of a GLM with Gamma or Inverse Gaussian?\",)####('I have no idea what those models are. (I have only taken stats 101). Are you saying that I cannot rely on intuition to make this decision? It feels reasonable to me that the shape of a log function would fit my variable. Therefore if I take the log of it, it will linearize my variable and be a better representation of reality. I am ok with discarding intuition in favor of more systematic diagnostics, but it is an awkward pill to swallow.',)####('Your intuition could be correct, I do not know. Nevertheless, given that you admit not having substantial experience on this field I urge you to follow standard procedures and not rely on your intuition immediately. Even in a `Stats101` course the importance of plotting your data should have been stressed anyway. The models I proposed are specifically designed to model dependent variables that are (any) positive numbers.',)####('Try then robust regression methods. In particular the functionality offered by the packages `lqmm` and `robustlmm` might can handy...',)####('You have \"repeated measures\" in your title, but don\\'t use packages nlme or lme4. Seek advice from a statistician.',)####(\"I don't know much about neural nets and random forests, but I'm surprised to see a fit like that. Hopefully someone can answer with an explanation of why something like this would happen.\",)####('These answers touche on the issue of the constant term, http://stats.stackexchange.com/questions/111544/removing-the-intercept-term-in-a-dynamic-regression-justified/111571#111571, and http://stats.stackexchange.com/questions/80790/deliberately-fitting-a-model-without-intercept/80798#80798although as @Glen_b notes, time series have their own aspects on the matter (such as differencing, that validly eliminates the constant).',)####(\"WHen the denominator df in an F go to infinity, it's exactly equivalent to a chi-square\",)####('Internally or externally studentized?',)####(\"This is a homework related question and it's not specified but I would assume internally.\",)####(\"the package `nlreg` (for heteroskedastic nonlinear regression) has a function (`nlreg.diag`) that can return a variety of such things including studentized residuals; I don't know much about it but it might be able to do ordinary nonlinear regression for you.\",)####(\"@whuber I think I've finally cracked it, but since this is the first time I've ever attempted to solve a problem like this, I'd appreciate your expert feedback. Thanks!\",)####('Note that the weights are not a complication at all, because they can be absorbed in the values of $b$ and $A$, leading to an ordinary least squares problem with a single linear constraint. That means you problem is solved as described at http://stats.stackexchange.com/questions/24193 .',)####('Very nearly the same question is asked--and answered in four different ways--at http://stats.stackexchange.com/questions/61733. It differs only in explicitly addressing the two dimensional case.',)####('The negatives are a result of the positive correlation between an observation and its fitted value, which reduces the variance of the difference.',)####('Is it possible that some \"$+$\" signs in the text are being mis-rendered (or misread) as \"$-$\" signs?',)####('I had thought this, but it happened twice in the text (2 different chapters) so I thought it to be unlikely.  Of course, a derivation of the formula would help! :)',)####('@user777 I am sorry about that: I was thinking that if these threads were close enough, we could merge your answer with the ones there.  I liked your explicit demonstration of `rstan`, which has no parallel in the answers within the other threads.',)####('@user777 I liked your solution as it was addressing this specific problem. If possible can you add it back? If anything it should be informative to someone else.',)####('@whuber Thanks, I was looking at other threads as well. The four different ways address a problem of a slope within the borders, not the sum of coefficients. The first link was helpful but I am still struggling to wrap my head around solve.QP or mgcv. I was hoping my question is generic enough to be useful for others and also sufficiently different from existing solutions in other threads.',)####('There is no essential mathematical difference between bounding the slope and bounding the sum of coefficients: both are bounds on linear combinations of the coefficients. The solutions offered to those questions apply with very little change to your slightly more general formulation, thereby immediately giving you access to a variety of approaches to choose from. However, I have not voted to close your question, because although it does appear to be answered elsewhere, evidently it does take a little mathematical manipulation to see that those answers can apply.',)####('@Glen Thanks for explaining why it turns out that the formula makes sense, along with your matrix derivation below.',)####('Why did you split your data into 3 subsets & fit 3 different models? What are you trying to do? Can you provide any information about your situation, your data & your goals? What do you mean by \"the anova code\"? Are you referring to the `anova()` function in R?',)####('See [here](http://i.stack.imgur.com/Xhilv.png). Also see [this](http://stats.stackexchange.com/questions/51061/does-the-normal-probability-plot-systematically-underestimate-the-mean) closely related question.',)####('Hope this helps: http://stats.stackexchange.com/questions/74334/question-about-prediction-bands-for-non-linear-regression-computation?lq=1',)####('Hope this helps: \\\\\\\\n\\\\\\\\nhttp://stats.stackexchange.com/questions/74334/question-about-prediction-bands-for-non-linear-regression-computation?lq=1',)",
        "base_pg_sql": "SELECT T1.Text FROM comments AS T1 INNER JOIN posts AS T2 ON T1.PostId = T2.Id WHERE T2.Title LIKE '%linear regression%'",
        "base_question": "Give the texts commented on the post about linear regression.",
        "base_evidence": "about linear regression refers to Title contains 'linear regression'",
        "gt": {
            "type": "delete",
            "table": "comments",
            "condition": "SELECT comments.id FROM comments, posts AS T2 WHERE comments.PostId = T2.Id AND T2.Title LIKE '%linear regression%' "
        }
    },
    {
        "question_id": 504,
        "db_id": "card_games",
        "question": "Delete all cards from the set 'World Championship Decks 2004' that have a converted mana cost of 3.",
        "evidence": "the set 'World Championship Decks 2004' refers to name = 'World Championship Decks 2004'",
        "SQL": "SELECT COUNT(id) FROM cards WHERE setCode IN ( SELECT code FROM sets WHERE name = 'World Championship Decks 2004' ) AND convertedManaCost = 3",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM cards WHERE setCode IN (SELECT code FROM sets WHERE name = 'World Championship Decks 2004') AND convertedManaCost = 3 ",
        "result_size": 1,
        "result": "(10,)",
        "base_pg_sql": "SELECT COUNT(id) FROM cards WHERE setCode IN (SELECT code FROM sets WHERE name = 'World Championship Decks 2004') AND convertedManaCost = 3",
        "base_question": "How many cards are there in the set 'World Championship Decks 2004' with the converted mana cost as '3'.",
        "base_evidence": "the set 'World Championship Decks 2004' refers to name = 'World Championship Decks 2004'",
        "gt": {
            "type": "delete",
            "table": "cards",
            "condition": "SELECT cards.id FROM cards WHERE setCode IN (SELECT code FROM sets WHERE name = 'World Championship Decks 2004') AND convertedManaCost = 3 "
        }
    },
    {
        "question_id": 551,
        "db_id": "codebase_community",
        "question": "Delete all badges that have been obtained by the user with the display name 'csgillespie'.",
        "evidence": "\"csgillespie\" is the DisplayName of user",
        "SQL": "SELECT COUNT(T1.Id) FROM badges AS T1 INNER JOIN users AS T2 ON T1.UserId = T2.Id WHERE T2.DisplayName = 'csgillespie'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM badges USING users AS T2 WHERE badges.UserId = T2.Id AND T2.DisplayName = 'csgillespie' ",
        "result_size": 1,
        "result": "(95,)",
        "base_pg_sql": "SELECT COUNT(T1.Id) FROM badges AS T1 INNER JOIN users AS T2 ON T1.UserId = T2.Id WHERE T2.DisplayName = 'csgillespie'",
        "base_question": "How many badges has the user csgillespie obtained?",
        "base_evidence": "\"csgillespie\" is the DisplayName of user",
        "gt": {
            "type": "delete",
            "table": "badges",
            "condition": "SELECT badges.id FROM badges, users AS T2 WHERE badges.UserId = T2.Id AND T2.DisplayName = 'csgillespie' "
        }
    },
    {
        "question_id": 1210,
        "db_id": "thrombosis_prediction",
        "question": "\"Delete all laboratory records for patients whose lactate dehydrogenase (LDH) levels fall within the normal range (less than 500).\"",
        "evidence": "average index of the lactate dehydrogenase (LDH) refers to AVG(LDH); (LDH) within the normal range refers to LDH < 500",
        "SQL": "SELECT AVG(LDH) FROM Laboratory WHERE LDH < 500",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM Laboratory WHERE LDH < 500 ",
        "result_size": 1,
        "result": "(Decimal('262.3600239616613419'),)",
        "base_pg_sql": "SELECT AVG(LDH) FROM Laboratory WHERE LDH < 500",
        "base_question": "What is the average index of the lactate dehydrogenase (LDH) for all patients with lactate dehydrogenase (LDH) within the normal range.",
        "base_evidence": "average index of the lactate dehydrogenase (LDH) refers to AVG(LDH); (LDH) within the normal range refers to LDH < 500",
        "gt": {
            "type": "delete",
            "table": "Laboratory",
            "condition": "SELECT Laboratory.id FROM Laboratory WHERE LDH < 500 "
        }
    },
    {
        "question_id": 241,
        "db_id": "toxicology",
        "question": "Remove all the molecules from the database that are not carcinogenic.",
        "evidence": "label = '-' means molecules are non-carcinogenic",
        "SQL": "SELECT COUNT(T.molecule_id) FROM molecule AS T WHERE T.label = '-'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM molecule WHERE molecule.label = '-' ",
        "result_size": 1,
        "result": "(191,)",
        "base_pg_sql": "SELECT COUNT(T.molecule_id) FROM molecule AS T WHERE T.label = '-'",
        "base_question": "How many of the molecules are not carcinogenic?",
        "base_evidence": "label = '-' means molecules are non-carcinogenic",
        "gt": {
            "type": "delete",
            "table": "molecule",
            "condition": "SELECT molecule.molecule_id FROM molecule WHERE molecule.label = '-' "
        }
    },
    {
        "question_id": 271,
        "db_id": "toxicology",
        "question": "\"Delete all connections for bond ID 'TR001_1_8' where the connected atom is a carbon atom (element type 'c' or 'c1').\"",
        "evidence": "chlorine refers to element = 'cl'; carbon refers to element = 'c'",
        "SQL": "SELECT T2.bond_id, T2.atom_id2, T1.element AS flag_have_CaCl FROM atom AS T1 INNER JOIN connected AS T2 ON T2.atom_id = T1.atom_id WHERE T2.bond_id = 'TR001_1_8' AND (T1.element = 'c1' OR T1.element = 'c')",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM connected USING atom AS T1 WHERE connected.atom_id = T1.atom_id AND connected.bond_id = 'TR001_1_8' AND (T1.element = 'c1' OR T1.element = 'c') ",
        "result_size": 2,
        "result": "('TR001_1_8', 'TR001_8', 'c')####('TR001_1_8', 'TR001_1', 'c')",
        "base_pg_sql": "SELECT T2.bond_id, T2.atom_id2, T1.element AS flag_have_CaCl FROM atom AS T1 INNER JOIN connected AS T2 ON T2.atom_id = T1.atom_id WHERE T2.bond_id = 'TR001_1_8' AND (T1.element = 'c1' OR T1.element = 'c')",
        "base_question": "Does bond id TR001_1_8 have both element of chlorine and carbon?",
        "base_evidence": "chlorine refers to element = 'cl'; carbon refers to element = 'c'",
        "gt": {
            "type": "delete",
            "table": "connected",
            "condition": "SELECT connected.atom_id FROM connected, atom AS T1 WHERE connected.atom_id = T1.atom_id AND connected.bond_id = 'TR001_1_8' AND (T1.element = 'c1' OR T1.element = 'c') "
        }
    },
    {
        "question_id": 401,
        "db_id": "card_games",
        "question": "Delete all cards that have the legendary frame effect from the database.",
        "evidence": "only available in online game variationsrefer to isOnlineOnly =1 ; legendary frame effect cards refer to frameEffects = 'legendary'; percentage refer to DIVIDE(COUNT(isOnlineOnly=1), COUNT(id)) from cards where frameEffects = 'legendary'",
        "SQL": "SELECT SUM(CASE WHEN isOnlineOnly = 1 THEN 1.0 ELSE 0 END) / COUNT(id) * 100 FROM cards WHERE frameEffects = 'legendary'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM cards WHERE frameEffects = 'legendary' ",
        "result_size": 1,
        "result": "(Decimal('1.08892921960072595300'),)",
        "base_pg_sql": "SELECT SUM(CASE WHEN isOnlineOnly = 1 THEN 1.0 ELSE 0 END) / NULLIF(COUNT(id), 0) * 100 FROM cards WHERE frameEffects = 'legendary'",
        "base_question": "What percentage of legendary frame effect cards that are only available in online game variations?",
        "base_evidence": "only available in online game variationsrefer to isOnlineOnly =1 ; legendary frame effect cards refer to frameEffects = 'legendary'; percentage refer to DIVIDE(COUNT(isOnlineOnly=1), COUNT(id)) from cards where frameEffects = 'legendary'",
        "gt": {
            "type": "delete",
            "table": "cards",
            "condition": "SELECT cards.id FROM cards WHERE frameEffects = 'legendary' "
        }
    },
    {
        "question_id": 544,
        "db_id": "codebase_community",
        "question": "Delete the user account of the person who last edited the post titled \"Examples for teaching: Correlation does not mean causation\".",
        "evidence": "\"Examples for teaching: Correlation does not mean causation\" is the Title of post; user who last edited refers to LastEditorUserId",
        "SQL": "SELECT T2.DisplayName FROM posts AS T1 INNER JOIN users AS T2 ON T1.LastEditorUserId = T2.Id WHERE T1.Title = 'Examples for teaching: Correlation does not mean causation'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM users USING posts AS T1 WHERE T1.LastEditorUserId = users.Id AND T1.Title = 'Examples for teaching: Correlation does not mean causation' ",
        "result_size": 1,
        "result": "('Abhilash',)",
        "base_pg_sql": "SELECT T2.DisplayName FROM posts AS T1 INNER JOIN users AS T2 ON T1.LastEditorUserId = T2.Id WHERE T1.Title = 'Examples for teaching: Correlation does not mean causation'",
        "base_question": "What is the display name of the user who last edited the post \"Examples for teaching: Correlation does not mean causation\"?",
        "base_evidence": "\"Examples for teaching: Correlation does not mean causation\" is the Title of post; user who last edited refers to LastEditorUserId",
        "gt": {
            "type": "delete",
            "table": "users",
            "condition": "SELECT users.id FROM users, posts AS T1 WHERE T1.LastEditorUserId = users.Id AND T1.Title = 'Examples for teaching: Correlation does not mean causation' "
        }
    },
    {
        "question_id": 1523,
        "db_id": "debit_card_specializing",
        "question": "Delete all gas stations where card with ID '667467' was used for transactions.",
        "evidence": "",
        "SQL": "SELECT T2.Country FROM transactions_1k AS T1 INNER JOIN gasstations AS T2 ON T1.GasStationID = T2.GasStationID WHERE T1.CardID = '667467'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM gasstations USING transactions_1k AS T1 WHERE T1.GasStationID = gasstations.GasStationID AND T1.CardID = '667467' ",
        "result_size": 1,
        "result": "('CZE',)",
        "base_pg_sql": "SELECT T2.Country FROM transactions_1k AS T1 INNER JOIN gasstations AS T2 ON T1.GasStationID = T2.GasStationID WHERE T1.CardID = '667467'",
        "base_question": "Which country was the card owner of No.667467 in?",
        "base_evidence": "",
        "gt": {
            "type": "delete",
            "table": "gasstations",
            "condition": "SELECT gasstations.gasstationid FROM gasstations, transactions_1k AS T1 WHERE T1.GasStationID = gasstations.GasStationID AND T1.CardID = '667467' "
        }
    },
    {
        "question_id": 650,
        "db_id": "codebase_community",
        "question": "\"Delete all users who have received the 'outliers' badge.\"",
        "evidence": "Outliers is the name of the badge;",
        "SQL": "SELECT T1.LastAccessDate, T1.Location FROM users AS T1 INNER JOIN badges AS T2 ON T1.Id = T2.UserId WHERE T2.Name = 'outliers'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM users USING badges AS T2 WHERE users.Id = T2.UserId AND T2.Name = 'outliers' ",
        "result_size": 1,
        "result": "(datetime.datetime(2014, 9, 12, 9, 6, 49, tzinfo=datetime.timezone.utc), 'Leuven, Belgium')",
        "base_pg_sql": "SELECT T1.LastAccessDate, T1.Location FROM users AS T1 INNER JOIN badges AS T2 ON T1.Id = T2.UserId WHERE T2.Name = 'outliers'",
        "base_question": "Describe the last accessed date and location of the users who received the outliers badge.",
        "base_evidence": "Outliers is the name of the badge;",
        "gt": {
            "type": "delete",
            "table": "users",
            "condition": "SELECT users.id FROM users, badges AS T2 WHERE users.Id = T2.UserId AND T2.Name = 'outliers' "
        }
    },
    {
        "question_id": 1412,
        "db_id": "student_club",
        "question": "Delete all expenses associated with members who wear X-Large t-shirts.",
        "evidence": "kind of expenses refers to expense_description; t_shirt_size = 'X-Large'",
        "SQL": "SELECT T2.expense_description FROM member AS T1 INNER JOIN expense AS T2 ON T1.member_id = T2.link_to_member WHERE T1.t_shirt_size = 'X-Large'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM expense USING member AS T1 WHERE T1.member_id = expense.link_to_member AND T1.t_shirt_size = 'X-Large' ",
        "result_size": 20,
        "result": "('Water, Cookies',)####('Pizza',)####('Parking',)####('Water, Cookies',)####('Pizza',)####('Pizza',)####('Posters',)####('Posters',)####('Pizza',)####('Parking',)####('Water, chips, cookies',)####('Club shirts',)####('Posters',)####('Water, chips, cookies',)####('Pizza',)####('Water, Veggie tray, supplies',)####('Pizza',)####('Water, Cookies',)####('Parking',)####('Water, cookies, chips',)",
        "base_pg_sql": "SELECT T2.expense_description FROM member AS T1 INNER JOIN expense AS T2 ON T1.member_id = T2.link_to_member WHERE T1.t_shirt_size = 'X-Large'",
        "base_question": "What kind of expenses incurred by members who have X-Large in size of tee shirt?",
        "base_evidence": "kind of expenses refers to expense_description; t_shirt_size = 'X-Large'",
        "gt": {
            "type": "delete",
            "table": "expense",
            "condition": "SELECT expense.expense_id FROM expense, member AS T1 WHERE T1.member_id = expense.link_to_member AND T1.t_shirt_size = 'X-Large' "
        }
    },
    {
        "question_id": 313,
        "db_id": "toxicology",
        "question": "Delete all atoms that belong to the molecule with ID 'TR001'.",
        "evidence": "",
        "SQL": "SELECT COUNT(T.atom_id) FROM atom AS T WHERE T.molecule_id = 'TR001'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM atom WHERE atom.molecule_id = 'TR001' ",
        "result_size": 1,
        "result": "(21,)",
        "base_pg_sql": "SELECT COUNT(T.atom_id) FROM atom AS T WHERE T.molecule_id = 'TR001'",
        "base_question": "How many atoms belong to molecule id TR001?",
        "base_evidence": "",
        "gt": {
            "type": "delete",
            "table": "atom",
            "condition": "SELECT atom.atom_id FROM atom WHERE atom.molecule_id = 'TR001' "
        }
    },
    {
        "question_id": 442,
        "db_id": "card_games",
        "question": "Delete all sets from the 'sets' table that belong to the blocks named 'Masques' or 'Mirage'.",
        "evidence": "",
        "SQL": "SELECT DISTINCT T1.baseSetSize, T2.setCode FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T1.block IN ('Masques', 'Mirage')",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM sets USING set_translations AS T2 WHERE T2.setCode = sets.code AND sets.block IN ('Masques', 'Mirage') ",
        "result_size": 3,
        "result": "(144, 'PCY')####(167, 'WTH')####(350, 'MMQ')",
        "base_pg_sql": "SELECT DISTINCT T1.baseSetSize, T2.setCode FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T1.block IN ('Masques', 'Mirage')",
        "base_question": "Mention the base set size and set code of the set that was in block named \"Masques\" and \"Mirage\".",
        "base_evidence": "",
        "gt": {
            "type": "delete",
            "table": "sets",
            "condition": "SELECT sets.id FROM sets, set_translations AS T2 WHERE T2.setCode = sets.code AND sets.block IN ('Masques', 'Mirage') "
        }
    },
    {
        "question_id": 584,
        "db_id": "codebase_community",
        "question": "Delete all revision history comments for the post titled 'Why square the difference instead of taking the absolute value in standard deviation?'",
        "evidence": "\"Why square the difference instead of taking the absolute value in standard deviation?\" is the Title of post;",
        "SQL": "SELECT T2.Comment FROM posts AS T1 INNER JOIN postHistory AS T2 ON T1.Id = T2.PostId WHERE T1.Title = 'Why square the difference instead of taking the absolute value in standard deviation?'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM postHistory USING posts AS T1 WHERE T1.Id = postHistory.PostId AND T1.Title = 'Why square the difference instead of taking the absolute value in standard deviation?' ",
        "result_size": 8,
        "result": "('edited title',)####('',)####('',)####('',)####('TeXified the formulas.',)####('deleted 5 characters in body; edited title',)####('deleted 5 characters in body; edited title',)####('edited title',)",
        "base_pg_sql": "SELECT T2.Comment FROM posts AS T1 INNER JOIN postHistory AS T2 ON T1.Id = T2.PostId WHERE T1.Title = 'Why square the difference instead of taking the absolute value in standard deviation?'",
        "base_question": "Write all the comments left by users who edited the post titled 'Why square the difference instead of taking the absolute value in standard deviation?'",
        "base_evidence": "\"Why square the difference instead of taking the absolute value in standard deviation?\" is the Title of post;",
        "gt": {
            "type": "delete",
            "table": "postHistory",
            "condition": "SELECT postHistory.id FROM postHistory, posts AS T1 WHERE T1.Id = postHistory.PostId AND T1.Title = 'Why square the difference instead of taking the absolute value in standard deviation?' "
        }
    },
    {
        "question_id": 699,
        "db_id": "codebase_community",
        "question": "Delete the user known as 'IrishStat' from the database.",
        "evidence": "",
        "SQL": "SELECT CreationDate FROM users WHERE DisplayName = 'IrishStat'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM users WHERE DisplayName = 'IrishStat' ",
        "result_size": 1,
        "result": "(datetime.datetime(2011, 2, 23, 5, 29, 56, tzinfo=datetime.timezone.utc),)",
        "base_pg_sql": "SELECT CreationDate FROM users WHERE DisplayName = 'IrishStat'",
        "base_question": "When did the user known as 'IrishStat' create his or her account?",
        "base_evidence": "DisplayName = 'IrishStat'; when create his or her account refers to CreationDate;",
        "gt": {
            "type": "delete",
            "table": "users",
            "condition": "SELECT users.id FROM users WHERE DisplayName = 'IrishStat' "
        }
    },
    {
        "question_id": 24,
        "db_id": "california_schools",
        "question": "Delete records from the FRPM database for schools where more than 10% of K-12 students are eligible for free meals and have at least one student who scored 1500 or higher on the SAT.",
        "evidence": "Percent eligible for free meals = Free Meal Count (K-12) / Total (Enrollment (K-12)",
        "SQL": "SELECT T2.`School Name` FROM satscores AS T1 INNER JOIN frpm AS T2 ON T1.cds = T2.CDSCode WHERE CAST(T2.`Free Meal Count (K-12)` AS REAL) / T2.`Enrollment (K-12)` > 0.1 AND T1.NumGE1500 > 0",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM frpm USING satscores AS T1 WHERE T1.cds = frpm.CDSCode AND CAST(frpm.\"Free Meal Count (K-12)\" AS REAL) / NULLIF(frpm.\"Enrollment (K-12)\", 0) > 0.1 AND T1.NumGE1500 > 0 ",
        "result_size": 1070,
        "result": "('Clovis North High',)####('Clovis West High',)####('Buchanan High',)####('Clovis East High',)####('Clovis High',)####('Coalinga High',)####('Fowler High',)####('Design Science Early College High',)####('Bullard High',)####('Erma Duncan Polytechnical High',)####('Sunnyside High',)####('Edison High',)####('Fresno High',)####('Herbert Hoover High',)####('McLane High',)####('Roosevelt High',)####('Kingsburg High',)####('Orange Cove High',)####('Reedley High',)####('Parlier High',)####('Hallmark Charter',)####('Sanger High',)####('Selma High',)####('Firebaugh High',)####('Central High East Campus',)####('Kerman High',)####('Mendota High',)####('Tranquillity High',)####('Sierra High',)####('Riverdale High',)####('Caruthers High',)####('Washington High',)####('Willows High',)####('Orland High',)####('Hamilton High',)####('Frontier High',)####('Northcoast Preparatory and Performing Arts Academy',)####('Six Rivers Charter High',)####('Arcata High',)####('McKinleyville High',)####('Academy of the Redwoods',)####('Fortuna Union High',)####('Hoopa Valley High',)####('South Fork Junior - Senior High',)####('Ferndale High',)####('District Office',)####('Eureka Senior High',)####('Brawley High',)####('Calexico High',)####('Calipatria High',)####('Southwest High',)####('Central Union High',)####('Holtville High',)####('Imperial High',)####('Bishop Union High',)####('Paramount Academy',)####('Valley Oaks Charter',)####('Cesar E. Chavez High',)####('Robert F. Kennedy High',)####('Delano High',)####('Golden Valley High',)####('Independence High',)####('Mira Monte High',)####('Arvin High',)####('Stockdale High',)####('Centennial High',)####('Ridgeview High',)####('Liberty High',)####('Bakersfield High',)####('East Bakersfield High',)####('Foothill High',)####('Highland High',)####('Kern Valley High',)####('North High',)####('Shafter High',)####('South High',)####('West High',)####('California City High',)####('Rosamond High',)####('Taft Union High',)####('Tehachapi High',)####('Wasco High',)####('Burroughs High',)####('McFarland High',)####('Frazier Mountain High',)####('Corcoran High',)####('Sierra Pacific High',)####('Hanford West High',)####('Hanford High',)####('Lemoore Middle College High',)####('Lemoore High',)####('Avenal High',)####('Kelseyville High',)####('Lower Lake High',)####('Clear Lake High',)####('Middletown High',)####('Lassen High',)####('Los Angeles International Charter High',)####('International Polytechnic High',)####('District Office',)####('Cerritos High',)####('Artesia High',)####('Whitney (Gretchen) High',)####('Gahr (Richard) High',)####('William J. (Pete) Knight High',)####('Eastside High',)####('SOAR High (Students On Academic Rise)',)####('Antelope Valley High',)####('Palmdale High',)####('Quartz Hill High',)####('Highland High',)####('Littlerock High',)####('Lancaster High',)####('Arcadia High',)####('Azusa High',)####('Gladstone High',)####('District Office',)####('Opportunities For Learning - Baldwin Park II',)####('Baldwin Park High',)####('Sierra Vista High',)####('Opportunities for Learning - Baldwin Park',)####('Bassett Senior High',)####('Bellflower High',)####('Mayfair High',)####('Bonita High',)####('San Dimas High',)####('Burbank High',)####('Burroughs High',)####('Options for Youth-Burbank Charter',)####('Hawthorne High',)####('Lawndale High',)####('Leuzinger High',)####('Charter Oak High',)####('Claremont High',)####('Covina High',)####('Northview High',)####('South Hills High',)####('Culver City High',)####('Downey High',)####('Warren High',)####('Duarte High',)####('Arroyo High',)####('El Monte High',)####('Mountain View High',)####('Rosemead High',)####('South El Monte High',)####('District Office',)####('El Rancho High',)####('Crescenta Valley High',)####('Glendale High',)####('Herbert Hoover High',)####('Anderson W. Clark Magnet High',)####('Glendora High',)####('Gorman Learning Center',)####('Hawthorne Math and Science Academy',)####('Inglewood High',)####('Morningside High',)####('City Honors College Preparatory Academy',)####('Animo Inglewood Charter High',)####('Environmental Charter High',)####('Lennox Mathematics, Science and Technology Academy',)####('Animo Leadership High',)####('District Office',)####('Avalon K-12',)####('Jordan High',)####('Lakewood High',)####('Millikan High',)####('Polytechnic High',)####('Wilson High',)####('California Academy of Mathematics and Science',)####('Cabrillo High',)####('Renaissance High School for the Arts',)####('High Tech LA',)####('Wallis Annenberg High',)####('Central City Value',)####('View Park Preparatory Accelerated High',)####('Crenshaw Arts-Technology Charter High',)####('Oscar De La Hoya Animo Charter High',)####('Renaissance Arts Academy',)####('Animo South Los Angeles Charter',)####('New Designs Charter',)####('Middle College High',)####('Harbor Teacher Preparation Academy',)####('Ivy Academia',)####('Camino Nuevo Charter High',)####('Animo Venice Charter High',)####('Animo Pat Brown',)####('Alliance Gertz-Ressler Richard Merkin 6-12 Complex',)####('Orthopaedic Hospital',)####('Northridge Academy High',)####('International Studies Learning Center at Legacy High School Complex',)####('Port of Los Angeles High',)####('CHAMPS - Charter HS of Arts-Multimedia & Performing',)####('Alliance Judy Ivie Burton Technology Academy High',)####('Alliance Collins Family College-Ready High',)####('Santee Education Complex',)####('South East High',)####('Maywood Academy High',)####('PUC CA Academy for Liberal Studies Early College High',)####('Los Angeles Academy of Arts & Enterprise Charter',)####('Alliance Patti And Peter Neuwirth Leadership Academy',)####('Alliance Dr. Olga Mohan High',)####('Animo Ralph Bunche High',)####('Animo Jackie Robinson High',)####('Animo Watts College Preparatory Academy',)####(\"Alliance Ouchi-O'Donovan 6-12 Complex\",)####('Alliance Marc & Eva Stern Math and Science',)####('School of Business and Tourism at Contreras Learning Complex',)####('East Valley Senior High',)####('Arleta High',)####('Panorama High',)####('Bright Star Secondary Charter Academy',)####('Frederick Douglass Academy High',)####('Student Empowerment Academy',)####('Contreras Learning Center-Los Angeles School of Global Studies',)####('West Adams Preparatory High',)####('Magnolia Science Academy 3',)####('Magnolia Science Academy 2',)####('Discovery Charter Preparatory No. 2',)####('Alliance Media Arts and Entertainment Design High',)####('Edward R. Roybal Learning Center',)####('Helen Bernstein High',)####('APEX Academy',)####('Alliance Health Services Academy High',)####('Alliance Environmental Science and Technology High',)####('Magnolia Science Academy 4',)####('Contreras Learning Center-Academic Leadership Community',)####('RFK Community Schools-Los Angeles High School of the Arts',)####('Belmont SH-LA Teacher Preparatory Academy',)####('RFK Community Schools- for the Visual Arts and Humanities',)####('New Millennium Secondary',)####('Alain Leroy Locke College Prep Academy',)####('Sun Valley High',)####('RFK Community Schools-New Open World Academy K-12',)####('RFK Community Schools-UCLA Community K-12',)####('Ramon C. Cortines School of Visual and Performing Arts',)####('Felicitas and Gonzalo Mendez High',)####('Daniel Pearl Journalism & Communications Magnet',)####('RFK Community Schools-Ambassador-Global Leadership',)####('Alliance Cindy and Bill Simon Technology Academy High',)####('Alliance Tennenbaum Family Technology High',)####('Academy of Environmental & Social Policy (ESP) at Roosevelt High',)####('Math, Science, & Technology Magnet Academy at Roosevelt High',)####('East Los Angeles Performing Arts Academy at Esteban E. Torres High No. 1',)####('Humanitas Academy of Art and Technology at Esteban E. Torres High No. 4',)####('Social Justice Leadership Academy at Esteban E. Torres High No. 5',)####('East Los Angeles Renaissance Academy at Esteban E. Torres High No. 2',)####('Engineering and Technology Academy at Esteban E. Torres High No. 3',)####('PUC Lakeview Charter High',)####('Aspire Pacific Academy',)####('Alliance College-Ready Academy High 16',)####('Valley Academy of Arts and Sciences',)####('Cesar E. Chavez Learning Academies-Arts,Theatre, Entertainment (ArTES)',)####('Cesar E. Chavez Learning Academies-Social Justice Humanitas Academy',)####('Cesar E. Chavez Learning Academies-Academy of Scientific Exploration (ASE)',)####('Cesar E. Chavez Learning Academies-Teacher Preparation Academy',)####('Los Angeles River at Sonia Sotomayor Learning Academies',)####('School of History and Dramatic Arts at Sonia Sotomayor Learning Academies',)####('Dr. Maya Angelou Community High',)####('Public Service Community at Diego Rivera Learning Complex',)####('Communication and Technology at Diego Rivera Learning Complex',)####('Green Design at Diego Rivera Learning Complex',)####('Performing Arts Community at Diego Rivera Learning Complex',)####('Rancho Dominguez Preparatory',)####('Synergy Quantum Academy',)####('Animo College Preparatory Academy',)####('Alliance Renee and Meyer Luskin Academy High',)####('PUC Early College Academy for Leaders and Scholars (ECALS)',)####('STEM Academy at Bernstein High',)####('Augustus F. Hawkins High B Community Health Advocates',)####('Augustus F. Hawkins High C Responsible Indigenous Social Entrepreneurship',)####('Linda Esperanza Marquez High A Huntington Park Institute of Applied Medicine',)####('Linda Esperanza Marquez High B LIBRA Academy',)####('Linda Esperanza Marquez High C School of Social Justice',)####('Science, Technology, Engineering, Arts and Mathematics at Legacy High School Complex',)####('Visual and Performing Arts at Legacy High School Complex',)####('Academies of Education and Empowerment at Carson High',)####('Academy of Medical Arts at Carson High',)####('Humanities and Arts (HARTS) Academy of Los Angeles',)####('Contreras Learning Center-School of Social Justice',)####('Phineas Banning Senior High',)####('Bell Senior High',)####('Belmont Senior High',)####('Birmingham Community Charter High',)####('Arnold O. Beckman High',)####('Canoga Park Senior High',)####('Carson Senior High',)####('Chatsworth Charter High',)####('Valley Alternative Magnet',)####('Grover Cleveland Charter High',)####('Crenshaw Science, Technology, Engineering, Math and Medicine Magnet',)####('Susan Miller Dorsey Senior High',)####('Eagle Rock High',)####('El Camino Real Charter High',)####('Los Angeles Center for Enriched Studies',)####('Downtown Business High',)####('Fairfax Senior High',)####('John H. Francis Polytechnic',)####('King/Drew Medical Magnet High',)####('Benjamin Franklin Senior High',)####('John C. Fremont Senior High',)####('Sherman Oaks Center for Enriched Studies',)####('Gardena Senior High',)####('James A. Garfield Senior High',)####('Granada Hills Charter High',)####('Ulysses S. Grant Senior High',)####('Alexander Hamilton Senior High',)####('Hollywood Senior High',)####('Huntington Park Senior High',)####('Thomas Jefferson Senior High',)####('David Starr Jordan Senior High',)####('Abraham Lincoln Senior High',)####('Los Angeles Senior High',)####('Manual Arts Senior High',)####('John Marshall Senior High',)####('James Monroe High',)####('Nathaniel Narbonne Senior High',)####('North Hollywood Senior High',)####('Reseda Senior High',)####('Theodore Roosevelt Senior High',)####('San Fernando Senior High',)####('San Pedro Senior High',)####('South Gate Senior High',)####('Sylmar Senior High',)####('William Howard Taft Charter High',)####('University Senior High',)####('Van Nuys Senior High',)####('Venice Senior High',)####('Verdugo Hills Senior High',)####('George Washington Preparatory High',)####('WESM Health/Sports Medicine',)####('Woodrow Wilson Senior High',)####('John F. Kennedy High',)####('Francisco Bravo Medical Magnet High',)####('Palisades Charter High',)####('City of Angels',)####('Los Angeles Leadership Academy',)####('Elizabeth Learning Center',)####('Paramount High',)####('Thirty-Second Street USC Performing Arts',)####('Vaughn Next Century Learning Center',)####('Robert Fulton College Preparatory',)####('Foshay Learning Center',)####('Magnolia Science Academy',)####('Marco Antonio Firebaugh High',)####('Lynwood High',)####('Monrovia High',)####('Mountain Park',)####('Applied Technology Center',)####('Schurr High',)####('Bell Gardens High',)####('Montebello High',)####('John H. Glenn High',)####('La Mirada High',)####('Norwalk High',)####('Guidance Charter',)####('District Office',)####('Blair High',)####('Marshall Fundamental',)####('John Muir High',)####('Pasadena High',)####('Ganesha High',)####('Garey High',)####('Pomona High',)####('Diamond Ranch High',)####('Village Academy High School at Indian Hill',)####('Fremont Academy of Engineering and Design',)####('Palomares Academy of Health Science',)####('Santa Monica High',)####('South Pasadena Senior High',)####('Temple City High',)####('North High',)####('South High',)####('Torrance High',)####('West High',)####('California Virtual Academy @ Los Angeles',)####('Edgewood High',)####('Insight @ Los Angeles',)####('West Covina High',)####('California High',)####('La Serna High',)####('Pioneer High',)####('Santa Fe High',)####('Whittier High',)####('Golden Valley High',)####('Santa Clarita Valley International',)####('Canyon High',)####('William S. Hart High',)####('Opportunities for Learning - Santa Clarita',)####('Centennial High',)####('Compton High',)####('Dominguez High',)####('La Puente High',)####('Los Altos High',)####('Glen A. Wilson High',)####('William Workman High',)####('Nogales High',)####('John A. Rowland High',)####('Gabrielino High',)####('Options for Youth San Gabriel',)####('Vasquez High',)####('Redondo Union High',)####('School of Arts and Enterprise',)####('Alhambra High',)####('Mark Keppel High',)####('San Gabriel High',)####('Da Vinci Science',)####('Da Vinci Design',)####('Chowchilla Union High',)####('Madera South High',)####('Madera High',)####('Liberty High',)####('Minarets High',)####('Minarets Charter High',)####('Yosemite High',)####('Novato High',)####('San Marin High',)####('San Rafael High',)####('Terra Linda High',)####('Tomales High',)####('Mariposa County High',)####('Anderson Valley Junior-Senior High',)####('Fort Bragg High',)####('Mendocino High',)####('Point Arena High',)####('Redwood Academy of Ukiah',)####('Ukiah High',)####('Willits High',)####('Potter Valley High',)####('Hilmar High',)####('Le Grand High',)####('Pacheco High',)####('Los Banos High',)####('Golden Valley High',)####('Buhach Colony High',)####('Atwater High',)####('Livingston High',)####('Merced High',)####('Gustine High',)####('Dos Palos High',)####('Delhi High',)####('Modoc High',)####('Mammoth High',)####('Greenfield High',)####('King City High',)####('Marina High',)####('Monterey High',)####('Seaside High',)####('Pacific Grove High',)####('Alisal High',)####('Everett Alvarez High',)####('North Salinas High',)####('Salinas High',)####('North Monterey County High',)####('Soledad High',)####('Gonzales High',)####('Calistoga Junior-Senior High',)####('American Canyon High',)####('Vintage High',)####('New Technology High',)####('Napa High',)####('Saint Helena High',)####('Bitney College Preparatory High',)####('Forest Charter',)####('William & Marian Ghidotti High',)####('Bear River High',)####('Nevada Union High',)####('OCCS:CHEP/PCHS',)####('Cypress High',)####('Anaheim High',)####('Oxford Academy',)####('Katella High',)####('John F. Kennedy High',)####('Loara High',)####('Magnolia High',)####('Savanna High',)####('Western High',)####('Brea-Olinda High',)####('Capistrano Connections Academy',)####('San Juan Hills High',)####('Capistrano Valley High',)####('San Clemente High',)####('Dana Hills High',)####('Buena Park High',)####('Fullerton Union High',)####('La Habra High',)####('Sonora High',)####('Sunny Hills High',)####('Troy High',)####('Bolsa Grande High',)####('Garden Grove High',)####('La Quinta High',)####('Los Amigos High',)####('Pacifica High',)####('Rancho Alamitos High',)####('Santiago High',)####('Ocean View High',)####('Fountain Valley High',)####('Huntington Beach High',)####('Marina High',)####('Westminster High',)####('Early College High',)####('Costa Mesa High',)####('Estancia High',)####('Newport Harbor High',)####('El Modena High',)####('Orange High',)####('Villa Park High',)####('Esperanza High',)####('El Dorado High',)####('Valencia High',)####('Nova Academy',)####('Segerstrom High',)####('Hector G. Godinez',)####('Century High',)####('Middle College High',)####('OCSA',)####('Saddleback High',)####('Santa Ana High',)####('Valley High',)####('El Toro High',)####('Laguna Hills High',)####('Mission Viejo High',)####('Foothill High',)####('Tustin High',)####('Irvine High',)####('Woodbridge High',)####('University High',)####('Foresthill High',)####('Colfax High',)####('Placer High',)####('Antelope High',)####('Woodcreek High',)####('Oakmont High',)####('Roseville High',)####('North Tahoe High',)####('Tahoe Truckee High',)####('Horizon Charter',)####('Lincoln High',)####('Whitney High',)####('Chester Junior/Senior High',)####('Portola Junior/Senior High',)####('Quincy Junior/Senior High',)####('River Springs Charter',)####('La Sierra High',)####('Norte Vista High',)####('Rancho Verde High',)####('Banning High',)####('Beaumont Senior High',)####('Eleanor Roosevelt High',)####('John F. Kennedy High',)####('Centennial High',)####('Santiago High',)####('Corona High',)####('Norco High',)####('Shadow Hills High',)####('Palm Desert High',)####('La Quinta High',)####('Indio High',)####('Hamilton High',)####('Tahquitz High',)####('College Prep High',)####('West Valley High',)####('Hemet High',)####('Patriot High',)####('Jurupa Valley High',)####('Rubidoux High',)####('Santa Rosa Academy',)####('Canyon Springs High',)####('Valley View High',)####('Vista del Lago High',)####('Moreno Valley High',)####('Nuview Bridge Early College High',)####('District Office',)####('Cathedral City High',)####('Desert Hot Springs High',)####('Palm Springs High',)####('Palo Verde High',)####('California Military Institute',)####('Heritage High',)####('Paloma Valley High',)####('Perris High',)####('Arlington High',)####('Martin Luther King Jr. High',)####('John W. North High',)####('Polytechnic High',)####('Ramona High',)####('San Jacinto High',)####('San Jacinto Valley Academy',)####('District Office',)####('Desert Mirage High',)####('NOVA Academy - Coachella',)####('West Shores High',)####('Coachella Valley High',)####('Lakeside High',)####('Temescal Canyon High',)####('Elsinore High',)####('Monte Vista High',)####('Temecula Valley High',)####('Chaparral High',)####('Vista Murrieta High',)####('Murrieta Mesa High',)####('Murrieta Valley High',)####('Citrus Hill High',)####('Monterey Trail High',)####('Pleasant Grove High',)####('Cosumnes Oaks High',)####('Valley High',)####('Florin High',)####('Laguna Creek High',)####('Sheldon High',)####('Franklin High',)####('Elk Grove High',)####('Cordova High',)####('Liberty Ranch High',)####('Galt High',)####('Rio Vista High',)####('Delta High',)####('District Office',)####('New Technology High',)####('George Washington Carver School of Arts and Science',)####('The MET',)####('Rosemont High',)####('Sacramento Charter High',)####('Arthur A. Benjamin Health Professions High',)####('School of Engineering & Sciences',)####('West Campus',)####('Luther Burbank High',)####('Hiram W. Johnson High',)####('John F. Kennedy High',)####('C. K. McClatchy High',)####('District Office',)####('Aspire Alexander Twilight Secondary Academy',)####('Mesa Verde High',)####('Bella Vista High',)####('Options for Youth-San Juan',)####('Visions In Education',)####('Casa Roble Fundamental High',)####('Del Campo High',)####('El Camino Fundamental High',)####('Encina Preparatory High',)####('Mira Loma High',)####('Rio Americano High',)####('San Juan High',)####('Center High',)####('Inderkum High',)####('Natomas Pacific Pathways Prep',)####('Natomas High',)####('Natomas Charter',)####('Futures High',)####('Heritage Peak Charter',)####('Foothill High',)####('Grant Union High',)####('Highlands High',)####('Rio Linda High',)####('San Benito High',)####('Anzar High',)####('Barstow High',)####('Big Bear High',)####('Mount Miguel High',)####('Alta Loma High',)####('Etiwanda High',)####('Rancho Cucamonga High',)####('Los Osos High',)####('Colony High',)####('Chaffey High',)####('Montclair High',)####('Ontario High',)####('Don Antonio Lugo High',)####('Ruben S. Ayala High',)####('Chino Hills High',)####('Chino High',)####('Santana High',)####('District Office',)####('Grand Terrace High School at the Ray Abril Jr. Educational Complex',)####('Bloomington High',)####('Colton High',)####('Summit High',)####('Jurupa Hills High',)####('Fontana A. B. Miller High',)####('Henry J. Kaiser High',)####('Fontana High',)####('Academy of Careers and Exploration',)####('Twentynine Palms High',)####('Yucca Valley High',)####('Needles High',)####('District Office',)####('Riverside Preparatory',)####('District Office',)####('Citrus Valley High',)####('Redlands East Valley High',)####('Grove',)####('Redlands Senior High',)####('Wilmer Amina Carter High',)####('Rialto High',)####('Eisenhower Senior High',)####('Rim of the World Senior High',)####('District Office',)####('Options for Youth-San Bernardino',)####('Indian Springs High',)####('Arroyo Valley High',)####('Middle College High',)####('Cajon High',)####('Pacific High',)####('San Bernardino High',)####('San Gorgonio High',)####('University Preparatory',)####('Adelanto High',)####('Options for Youth-Victorville Charter',)####('Excelsior Charter',)####('Silverado High',)####('Victor Valley High',)####('Yucaipa High',)####('Silver Valley High',)####('Serrano High',)####('Encore Jr./Sr. High School for the Performing and Visual Arts',)####('Oak Hills High',)####('Hesperia High',)####('Sultana High',)####('Upland High',)####('Apple Valley High',)####('Granite Hills High',)####('Academy for Academic Excellence',)####('San Pasqual Academy',)####('Borrego Springs High',)####('Dehesa Charter',)####('San Pasqual High',)####('Escondido Charter High',)####('Escondido High',)####('Orange Glen High',)####('Fallbrook High',)####('Valhalla High',)####('West Hills High',)####('Steele Canyon High',)####('Grossmont Middle College High',)####('El Cajon Valley High',)####('El Capitan High',)####('Granite Hills High',)####('Grossmont High',)####('Helix High',)####('Julian Charter',)####('Mountain Empire High',)####('District Office',)####('Mt. Carmel High',)####('Poway High',)####('Ramona High',)####('High Tech High International',)####('San Diego International Studies',)####('San Diego Business/Leadership',)####('San Diego MVP Arts',)####('Kearny Digital Media & Design',)####('Kearny SCT',)####('Kearny International Business',)####('Kearny Eng, Innov & Design',)####('Crawford High',)####('San Diego Science and Technology',)####('San Diego Metro Career and Tech',)####('High Tech High Media Arts',)####('Lincoln High',)####('Health Sciences High',)####('San Diego Early/Middle College',)####('King-Chavez Community High',)####('Gompers Preparatory Academy',)####('Coleman Tech Charter High',)####('Serra High',)####('Mira Mesa High',)####('University City High',)####('San Diego SCPA',)####('Mt. Everest Academy',)####('Scripps Ranch High',)####('Charter School of San Diego',)####('Preuss School UCSD',)####('Clairemont High',)####('High Tech High',)####('Audeo Charter',)####('Henry High',)####('Hoover High',)####('La Jolla High',)####('Madison High',)####('Mission Bay High',)####('Morse High',)####('Point Loma High',)####('Montgomery Senior High',)####('California Virtual Academy @ San Diego',)####('District Office',)####('Olympian High',)####('Southwest Senior High',)####('Bonita Vista Senior High',)####('Castle Park Senior High',)####('Eastlake High',)####('Chula Vista Senior High',)####('San Ysidro High',)####('Otay Ranch Senior High',)####('Hilltop Senior High',)####('Mar Vista Senior High',)####('Sweetwater High',)####('Mission Vista High',)####('Rancho Buena Vista High',)####('Guajome Park Academy Charter',)####('Vista High',)####('Carlsbad High',)####('Oceanside High',)####('El Camino High',)####('Mission Hills High',)####('San Marcos High',)####('Valley Center High',)####('High Tech High Chula Vista',)####('High Tech High North County',)####('District Office',)####('City Arts and Tech High',)####('S.F. International High',)####('Academy of Arts and Sciences',)####('Independence High',)####('Wallenberg (Raoul) Traditional High',)####('Burton (Phillip and Sala) Academic High',)####('Balboa High',)####('International Studies Academy',)####('Asawa (Ruth) San Francisco School of the Arts, A Public School.',)####('Marshall (Thurgood) High',)####('Leadership High',)####('Gateway High',)####('Galileo High',)####('Lincoln (Abraham) High',)####('Lowell High',)####('Mission High',)####(\"O'Connell (John) High\",)####('Washington (George) High',)####('Aptos High',)####('San Francisco Flex Academy',)####('Venture Academy',)####('Escalon High',)####('Lincoln High',)####('Linden High',)####('Aspire Benjamin Holt College Preparatory Academy',)####('Ronald E. McNair High',)####('Bear Creek High',)####('Middle College High',)####('Tokay High',)####('Lodi High',)####('Weston Ranch High',)####('Lathrop High',)####('Sierra High',)####('East Union High',)####('Manteca High',)####('Humphreys College Academy of Business, Law and Education',)####('Delta Charter',)####('California Connections Academy @ Ripon',)####('Ripon High',)####('Etna Union High',)####('Cesar Chavez High',)####('Aspire Langston Hughes Academy',)####('Stockton Unified Early College Academy',)####('Stockton Collegiate International Secondary',)####('Health Careers Academy',)####('Weber Institute',)####('Edison High',)####('Franklin High',)####('Stagg Senior High',)####('Millennium Charter',)####('John C. Kimball High',)####('Merrill F. West High',)####('Tracy High',)####('Atascadero High',)####('Nipomo High',)####('Arroyo Grande High',)####('Morro Bay High',)####('San Luis Obispo High',)####('Paso Robles High',)####('Coast Union High',)####('Half Moon Bay High',)####('Jefferson High',)####('Oceana High',)####('Terra Nova High',)####('Westmoor High',)####('Aragon High',)####('Capuchino High',)####('Hillsdale High',)####('Mills High',)####('San Mateo High',)####('Summit Preparatory Charter High',)####('Everest Public High',)####('East Palo Alto Academy',)####('Carlmont High',)####('Menlo-Atherton High',)####('Sequoia High',)####('Woodside High',)####('El Camino High',)####('South San Francisco High',)####('Carpinteria Senior High',)####('Cabrillo High',)####('Lompoc High',)####('Piner High',)####('Orcutt Academy Charter',)####('Pioneer Valley High',)####('Ernest Righetti High',)####('Santa Maria High',)####('Santa Ynez Valley Union High',)####('Alta Vista Alternative High',)####('Dos Pueblos Senior High',)####('San Marcos Senior High',)####('Santa Barbara Senior High',)####('Leadership Public Schools - San Jose',)####('University Preparatory Academy Charter',)####('Summit Public School: Tahoma',)####('Branham High',)####('Del Mar High',)####('Prospect High',)####('Westmont High',)####('KIPP San Jose Collegiate',)####('Summit Public School: Rainier',)####('Yerba Buena High',)####('Santa Teresa High',)####('Independence High',)####('Evergreen Valley High',)####('Andrew P. Hill High',)####('James Lick High',)####('Mount Pleasant High',)####('Oak Grove High',)####('William C. Overfelt High',)####('Piedmont Hills High',)####('Silver Creek High',)####('Fremont High',)####('Homestead High',)####('Dr. T. J. Owens Gilroy Early College Academy',)####('Christopher High',)####('Gilroy High',)####('Ann Sobrato High',)####('Live Oak High',)####('Los Altos High',)####('Mountain View High',)####('Gunderson High',)####('Abraham Lincoln High',)####('Pioneer High',)####('San Jos\u00e9 High',)####('Willow Glen High',)####('Santa Clara High',)####('Adrian Wilcox High',)####('Milpitas High',)####('District Office',)####('Pajaro Valley High',)####('Watsonville High',)####('Ocean Grove Charter',)####('San Lorenzo Valley High',)####('Harbor High',)####('Santa Cruz High',)####('Soquel High',)####('West Valley High',)####('Anderson High',)####('Fall River Junior-Senior High',)####('University Preparatory',)####('Foothill High',)####('Enterprise High',)####('Shasta High',)####('Central Valley High',)####('Mt. Shasta High',)####('Weed High',)####('Yreka High',)####('Benicia High',)####('Dixon High',)####('Angelo Rodriguez High',)####('Armijo High',)####('Fairfield High',)####('Vanden High',)####('Will C. Wood High',)####('Elise P. Buckingham Charter Magnet High',)####('Vacaville High',)####('Jesse M. Bethel High',)####('Vallejo High',)####('Analy High',)####('El Molino High',)####('Cloverdale High',)####('Casa Grande High',)####('Petaluma High',)####('Roseland Charter',)####('Elsie Allen High',)####('Maria Carrillo High',)####('Montgomery High',)####('Santa Rosa High',)####('Sonoma Valley High',)####('Rancho Cotate High',)####('Windsor High',)####('Healdsburg High',)####('Valley Charter High',)####('Whitmore Charter High',)####('Central Valley High',)####('Ceres High',)####('Denair High',)####('James C. Enochs High',)####('Joseph A. Gregori High',)####('Fred C. Beyer High',)####('Peter Johansen High',)####('Grace M. Davis High',)####('Thomas Downey High',)####('Modesto High',)####('Patterson High',)####('Orestimba High',)####('Hughson High',)####('Riverbank High',)####('Oakdale High',)####('Waterford High',)####('Connecting Waters Charter',)####('John H. Pitman High',)####('Turlock High',)####('East Nicolaus High',)####('Live Oak High',)####('South Sutter Charter',)####('Sutter High',)####('River Valley High',)####('Yuba City High',)####('Corning High',)####('Los Molinos High',)####('Red Bluff High',)####('Trinity High',)####('University Preparatory High',)####('Orosi High',)####('Lindsay Senior High',)####('Mission Oak High',)####('Tulare Union High',)####('Tulare Western High',)####('Visalia Charter Independent Study',)####('Golden West High',)####('El Diamante High',)####('Mt. Whitney High',)####('Redwood High',)####('Farmersville High',)####('Harmony Magnet Academy',)####('Granite Hills High',)####('Monache High',)####('Porterville High',)####('Strathmore High',)####('Dinuba High',)####('Woodlake High',)####('Exeter Union High',)####('District Office',)####('Sonora High',)####('Summerville High',)####('Fillmore Senior High',)####('Nordhoff High',)####('Pacifica High',)####('Adolfo Camarillo High',)####('Channel Islands High',)####('Hueneme High',)####('Oxnard High',)####('Rio Mesa High',)####('Santa Susana High',)####('Royal High',)####('Simi Valley High',)####('El Camino High',)####('Foothill Technology High',)####('Buena High',)####('Ventura High',)####('District Office',)####('Westlake High',)####('Newbury Park High',)####('Thousand Oaks High',)####('The High School at Moorpark College',)####('Moorpark High',)####('District Office',)####('Santa Paula High',)####('Davis Senior High',)####('Esparto High',)####('River City High',)####('Winters High',)####('District Office',)####('Pioneer High',)####('Woodland Senior High',)####('Lindhurst High',)####('Marysville Charter Academy for the Arts',)####('Marysville High',)####('Wheatland Union High',)",
        "base_pg_sql": "SELECT T2.\"School Name\" FROM satscores AS T1 INNER JOIN frpm AS T2 ON T1.cds = T2.CDSCode WHERE CAST(T2.\"Free Meal Count (K-12)\" AS REAL) / NULLIF(T2.\"Enrollment (K-12)\", 0) > 0.1 AND T1.NumGE1500 > 0",
        "base_question": "Give the names of the schools with the percent eligible for free meals in K-12 is more than 0.1 and test takers whose test score is greater than or equal to 1500?",
        "base_evidence": "Percent eligible for free meals = Free Meal Count (K-12) / Total (Enrollment (K-12)",
        "gt": {
            "type": "delete",
            "table": "frpm",
            "condition": "SELECT frpm.cdscode FROM frpm, satscores AS T1 WHERE T1.cds = frpm.CDSCode AND CAST(frpm.\"Free Meal Count (K-12)\" AS REAL) / NULLIF(frpm.\"Enrollment (K-12)\", 0) > 0.1 AND T1.NumGE1500 > 0 "
        }
    },
    {
        "question_id": 462,
        "db_id": "card_games",
        "question": "Remove the Italian translation for the set of cards that includes 'Ancestor's Chosen'.",
        "evidence": "Italian is a language which refers to language = 'Italian'; with \"Ancestor's Chosen\" in the card set refers to name = 'Ancestor''s Chosen'",
        "SQL": "SELECT translation FROM set_translations WHERE setCode IN ( SELECT setCode FROM cards WHERE name = 'Ancestor''s Chosen' ) AND language = 'Italian'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM set_translations WHERE setCode IN (SELECT setCode FROM cards WHERE name = 'Ancestor''s Chosen') AND language = 'Italian' ",
        "result_size": 2,
        "result": "('Set Base Decima Edizione',)####('Sentenza',)",
        "base_pg_sql": "SELECT translation FROM set_translations WHERE setCode IN (SELECT setCode FROM cards WHERE name = 'Ancestor''s Chosen') AND language = 'Italian'",
        "base_question": "What's the Italian name of the set of cards with \"Ancestor's Chosen\" is in?",
        "base_evidence": "Italian is a language which refers to language = 'Italian'; with \"Ancestor's Chosen\" in the card set refers to name = 'Ancestor''s Chosen'",
        "gt": {
            "type": "delete",
            "table": "set_translations",
            "condition": "SELECT set_translations.id FROM set_translations WHERE setCode IN (SELECT setCode FROM cards WHERE name = 'Ancestor''s Chosen') AND language = 'Italian' "
        }
    },
    {
        "question_id": 1307,
        "db_id": "thrombosis_prediction",
        "question": "Delete the records of patients who have an abnormal level of red blood cells and are being followed at the outpatient clinic.",
        "evidence": "RBC < = 3.5 or RBC > = 6.0 means the patient has an abnormal level of red blood cell; 3.5 < RBC < 6.0 means the patient has a normal level of red blood cell; followed at the outpatient clinic refers to Admission = '-';",
        "SQL": "SELECT DISTINCT T1.ID FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE (T2.RBC <= 3.5 OR T2.RBC >= 6) AND T1.Admission = '-'",
        "difficulty": "challenging",
        "pg_sql": "DELETE FROM Patient USING Laboratory AS T2 WHERE Patient.ID = T2.ID AND (T2.RBC <= 3.5 OR T2.RBC >= 6) AND Patient.Admission = '-' ",
        "result_size": 35,
        "result": "(4417978,)####(4862013,)####(58139,)####(2256652,)####(3561498,)####(4416959,)####(3182521,)####(3362815,)####(4632825,)####(1043570,)####(5303768,)####(3310582,)####(2276582,)####(1557464,)####(4632519,)####(1124385,)####(4466840,)####(5117606,)####(4840422,)####(1711181,)####(2931207,)####(4915498,)####(5063068,)####(2307640,)####(528900,)####(2355809,)####(5122312,)####(71417,)####(30609,)####(1180510,)####(1755350,)####(3545964,)####(619178,)####(525998,)####(4471732,)",
        "base_pg_sql": "SELECT DISTINCT T1.ID FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE (T2.RBC <= 3.5 OR T2.RBC >= 6) AND T1.Admission = '-'",
        "base_question": "Please list the patient's ID if he or she has an abnormal level of red blood cell and is followed at the outpatient clinic.",
        "base_evidence": "RBC < = 3.5 or RBC > = 6.0 means the patient has an abnormal level of red blood cell; 3.5 < RBC < 6.0 means the patient has a normal level of red blood cell; followed at the outpatient clinic refers to Admission = '-';",
        "gt": {
            "type": "delete",
            "table": "Patient",
            "condition": "SELECT Patient.id FROM Patient, Laboratory AS T2 WHERE Patient.ID = T2.ID AND (T2.RBC <= 3.5 OR T2.RBC >= 6) AND Patient.Admission = '-' "
        }
    },
    {
        "question_id": 337,
        "db_id": "toxicology",
        "question": "Delete all atoms that belong to molecule TR002 that are associated with bonds.",
        "evidence": "TR002 is the molecule id",
        "SQL": "SELECT DISTINCT T1.element, T2.bond_type FROM atom AS T1 INNER JOIN bond AS T2 ON T1.molecule_id = T2.molecule_id WHERE T1.molecule_id = 'TR002'",
        "difficulty": "challenging",
        "pg_sql": "DELETE FROM atom USING bond AS T2 WHERE atom.molecule_id = T2.molecule_id AND atom.molecule_id = 'TR002' ",
        "result_size": 6,
        "result": "('h', '=')####('cl', '=')####('cl', '-')####('h', '-')####('c', '=')####('c', '-')",
        "base_pg_sql": "SELECT DISTINCT T1.element, T2.bond_type FROM atom AS T1 INNER JOIN bond AS T2 ON T1.molecule_id = T2.molecule_id WHERE T1.molecule_id = 'TR002'",
        "base_question": "List the element and bond type included in the molecule with molecule ID of TR002.",
        "base_evidence": "TR002 is the molecule id",
        "gt": {
            "type": "delete",
            "table": "atom",
            "condition": "SELECT atom.atom_id FROM atom, bond AS T2 WHERE atom.molecule_id = T2.molecule_id AND atom.molecule_id = 'TR002' "
        }
    },
    {
        "question_id": 363,
        "db_id": "card_games",
        "question": "\"Delete all cards that are in a starter deck and have a legality status of 'Restricted'.\"",
        "evidence": "restricted refers to status = 'restricted'; found in the starter deck refers to isStarter = 1;",
        "SQL": "SELECT COUNT(DISTINCT T1.id) FROM cards AS T1 INNER JOIN legalities AS T2 ON T1.uuid = T2.uuid WHERE T2.status = 'Restricted' AND T1.isStarter = 1",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM cards USING legalities AS T2 WHERE cards.uuid = T2.uuid AND T2.status = 'Restricted' AND cards.isStarter = 1 ",
        "result_size": 1,
        "result": "(189,)",
        "base_pg_sql": "SELECT COUNT(DISTINCT T1.id) FROM cards AS T1 INNER JOIN legalities AS T2 ON T1.uuid = T2.uuid WHERE T2.status = 'Restricted' AND T1.isStarter = 1",
        "base_question": "How many cards of legalities whose status is restricted are found in a starter deck?",
        "base_evidence": "restricted refers to status = 'restricted'; found in the starter deck refers to isStarter = 1;",
        "gt": {
            "type": "delete",
            "table": "cards",
            "condition": "SELECT cards.id FROM cards, legalities AS T2 WHERE cards.uuid = T2.uuid AND T2.status = 'Restricted' AND cards.isStarter = 1 "
        }
    },
    {
        "question_id": 705,
        "db_id": "codebase_community",
        "question": "Delete the user who commented \"fine, you win :)\".",
        "evidence": "Text = 'fine, you win :)';",
        "SQL": "SELECT T2.Reputation, T2.UpVotes FROM comments AS T1 INNER JOIN users AS T2 ON T1.UserId = T2.Id WHERE T1.Text = 'fine, you win :)'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM users USING comments AS T1 WHERE T1.UserId = users.Id AND T1.Text = 'fine, you win :)' ",
        "result_size": 1,
        "result": "(2316, 74)",
        "base_pg_sql": "SELECT T2.Reputation, T2.UpVotes FROM comments AS T1 INNER JOIN users AS T2 ON T1.UserId = T2.Id WHERE T1.Text = 'fine, you win :)'",
        "base_question": "Give the user's reputation and up vote number of the user that commented \"fine, you win :)\".",
        "base_evidence": "Text = 'fine, you win :)';",
        "gt": {
            "type": "delete",
            "table": "users",
            "condition": "SELECT users.id FROM users, comments AS T1 WHERE T1.UserId = users.Id AND T1.Text = 'fine, you win :)' "
        }
    },
    {
        "question_id": 357,
        "db_id": "card_games",
        "question": "Delete the card named 'Duress' that has at least one type of promotion listed.",
        "evidence": "card Duress refers to name = 'Duress'; type of promotion refers to promoTypes;",
        "SQL": "SELECT promoTypes FROM cards WHERE name = 'Duress' AND promoTypes IS NOT NULL",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM cards WHERE name = 'Duress' AND NOT promoTypes IS NULL ",
        "result_size": 4,
        "result": "('tourney,fnm',)####('arenaleague',)####('mediainsert',)####('mediainsert',)",
        "base_pg_sql": "SELECT promoTypes FROM cards WHERE name = 'Duress' AND NOT promoTypes IS NULL",
        "base_question": "What type of promotion is of card 'Duress'?",
        "base_evidence": "card Duress refers to name = 'Duress'; type of promotion refers to promoTypes;",
        "gt": {
            "type": "delete",
            "table": "cards",
            "condition": "SELECT cards.id FROM cards WHERE name = 'Duress' AND NOT promoTypes IS NULL "
        }
    },
    {
        "question_id": 1207,
        "db_id": "thrombosis_prediction",
        "question": "Delete all patient records whose AST glutamic oxaloacetic transaminase (GOT) index is within normal range (GOT < 60) based on laboratory examinations conducted in 1994.",
        "evidence": "AST glutamic oxaloacetic transaminase (GOT) index is within normal range refers to GOT < 60; examination in 1994 refers to year(Date) = 1994",
        "SQL": "SELECT DISTINCT T1.SEX, T1.Birthday FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.GOT < 60 AND STRFTIME('%Y', T2.Date) = '1994'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM Patient USING Laboratory AS T2 WHERE Patient.ID = T2.ID AND T2.GOT < 60 AND TO_CHAR(CAST(T2.Date AS TIMESTAMP), 'YYYY') = '1994' ",
        "result_size": 97,
        "result": "('', datetime.date(1931, 4, 4))####('F', datetime.date(1925, 3, 25))####('F', datetime.date(1926, 1, 2))####('F', datetime.date(1927, 1, 25))####('F', datetime.date(1929, 3, 22))####('F', datetime.date(1930, 2, 15))####('F', datetime.date(1931, 5, 27))####('F', datetime.date(1932, 12, 5))####('F', datetime.date(1935, 6, 3))####('F', datetime.date(1936, 5, 22))####('F', datetime.date(1937, 12, 18))####('F', datetime.date(1938, 1, 1))####('F', datetime.date(1938, 3, 23))####('F', datetime.date(1938, 4, 8))####('F', datetime.date(1939, 8, 10))####('F', datetime.date(1941, 10, 6))####('F', datetime.date(1941, 11, 21))####('F', datetime.date(1942, 6, 6))####('F', datetime.date(1942, 10, 28))####('F', datetime.date(1943, 7, 6))####('F', datetime.date(1943, 11, 28))####('F', datetime.date(1945, 12, 4))####('F', datetime.date(1946, 4, 6))####('F', datetime.date(1948, 4, 26))####('F', datetime.date(1948, 7, 25))####('F', datetime.date(1948, 11, 10))####('F', datetime.date(1949, 4, 21))####('F', datetime.date(1949, 5, 11))####('F', datetime.date(1949, 5, 13))####('F', datetime.date(1949, 7, 15))####('F', datetime.date(1949, 7, 16))####('F', datetime.date(1950, 8, 3))####('F', datetime.date(1950, 9, 4))####('F', datetime.date(1950, 10, 19))####('F', datetime.date(1952, 3, 3))####('F', datetime.date(1953, 7, 12))####('F', datetime.date(1954, 12, 23))####('F', datetime.date(1955, 1, 7))####('F', datetime.date(1955, 3, 7))####('F', datetime.date(1956, 1, 27))####('F', datetime.date(1956, 12, 7))####('F', datetime.date(1957, 1, 19))####('F', datetime.date(1958, 3, 4))####('F', datetime.date(1958, 11, 24))####('F', datetime.date(1959, 1, 22))####('F', datetime.date(1959, 3, 12))####('F', datetime.date(1960, 6, 6))####('F', datetime.date(1960, 8, 24))####('F', datetime.date(1961, 2, 2))####('F', datetime.date(1962, 1, 18))####('F', datetime.date(1963, 7, 21))####('F', datetime.date(1963, 12, 13))####('F', datetime.date(1964, 1, 1))####('F', datetime.date(1964, 1, 29))####('F', datetime.date(1964, 10, 17))####('F', datetime.date(1965, 3, 19))####('F', datetime.date(1965, 10, 1))####('F', datetime.date(1965, 11, 18))####('F', datetime.date(1966, 2, 7))####('F', datetime.date(1967, 11, 11))####('F', datetime.date(1968, 9, 25))####('F', datetime.date(1969, 4, 5))####('F', datetime.date(1969, 7, 14))####('F', datetime.date(1969, 11, 30))####('F', datetime.date(1971, 5, 12))####('F', datetime.date(1971, 5, 22))####('F', datetime.date(1971, 9, 20))####('F', datetime.date(1971, 11, 16))####('F', datetime.date(1971, 12, 14))####('F', datetime.date(1973, 3, 27))####('F', datetime.date(1974, 11, 25))####('F', datetime.date(1975, 1, 31))####('F', datetime.date(1975, 2, 17))####('F', datetime.date(1975, 4, 22))####('F', datetime.date(1976, 4, 3))####('F', datetime.date(1976, 4, 5))####('F', datetime.date(1976, 12, 15))####('F', datetime.date(1977, 2, 4))####('F', datetime.date(1977, 2, 26))####('F', datetime.date(1984, 2, 11))####('F', datetime.date(1989, 8, 28))####('M', datetime.date(1923, 7, 25))####('M', datetime.date(1931, 2, 11))####('M', datetime.date(1937, 11, 24))####('M', datetime.date(1938, 11, 7))####('M', datetime.date(1942, 3, 4))####('M', datetime.date(1950, 4, 4))####('M', datetime.date(1953, 7, 15))####('M', datetime.date(1961, 3, 18))####('M', datetime.date(1961, 12, 12))####('M', datetime.date(1963, 7, 9))####('M', datetime.date(1965, 1, 9))####('M', datetime.date(1966, 12, 1))####('M', datetime.date(1967, 10, 20))####('M', datetime.date(1968, 5, 21))####('M', datetime.date(1974, 6, 3))####('M', datetime.date(1974, 11, 8))",
        "base_pg_sql": "SELECT DISTINCT T1.SEX, T1.Birthday FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.GOT < 60 AND TO_CHAR(CAST(T2.Date AS TIMESTAMP), 'YYYY') = '1994'",
        "base_question": "List all patients with their sex and date of birthday, whose AST glutamic oxaloacetic transaminase (GOT) index is within normal range for loboratory examination in 1994.",
        "base_evidence": "AST glutamic oxaloacetic transaminase (GOT) index is within normal range refers to GOT < 60; examination in 1994 refers to year(Date) = 1994",
        "gt": {
            "type": "delete",
            "table": "Patient",
            "condition": "SELECT Patient.id FROM Patient, Laboratory AS T2 WHERE Patient.ID = T2.ID AND T2.GOT < 60 AND TO_CHAR(CAST(T2.Date AS TIMESTAMP), 'YYYY') = '1994' "
        }
    },
    {
        "question_id": 374,
        "db_id": "card_games",
        "question": "\"Delete all black border cards that are only available on mtgo.\"",
        "evidence": "black border card refers to borderColor = black; available on mtgo refers to availability = mtgo;\n\nadd quotes for string = 'black' and = 'mtgo'",
        "SQL": "SELECT COUNT(id) FROM cards WHERE availability = 'mtgo' AND borderColor = 'black'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM cards WHERE availability = 'mtgo' AND borderColor = 'black' ",
        "result_size": 1,
        "result": "(3697,)",
        "base_pg_sql": "SELECT COUNT(id) FROM cards WHERE availability = 'mtgo' AND borderColor = 'black'",
        "base_question": "How many black border cards are only available on mtgo?",
        "base_evidence": "black border card refers to borderColor = black; available on mtgo refers to availability = mtgo;\n\nadd quotes for string = 'black' and = 'mtgo'",
        "gt": {
            "type": "delete",
            "table": "cards",
            "condition": "SELECT cards.id FROM cards WHERE availability = 'mtgo' AND borderColor = 'black' "
        }
    },
    {
        "question_id": 202,
        "db_id": "toxicology",
        "question": "Delete all triple type bonds from the database.",
        "evidence": "triple type bonds refers to bond_type = '#'",
        "SQL": "SELECT COUNT(T.bond_id) FROM bond AS T WHERE T.bond_type = '#'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM bond WHERE bond.bond_type = '#' ",
        "result_size": 1,
        "result": "(6,)",
        "base_pg_sql": "SELECT COUNT(T.bond_id) FROM bond AS T WHERE T.bond_type = '#'",
        "base_question": "How many triple type bonds are there?",
        "base_evidence": "triple type bonds refers to bond_type = '#'",
        "gt": {
            "type": "delete",
            "table": "bond",
            "condition": "SELECT bond.bond_id FROM bond WHERE bond.bond_type = '#' "
        }
    },
    {
        "question_id": 502,
        "db_id": "card_games",
        "question": "\"Delete the set that has the translated name 'Ola de fr\u00edo'.\"",
        "evidence": "release date is the date of card set being released; set \"Ola de fr\u00edo\" refers to translation = 'Ola de fr\u00edo'",
        "SQL": "SELECT T1.releaseDate FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T2.translation = 'Ola de fr\u00edo'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM sets USING set_translations AS T2 WHERE T2.setCode = sets.code AND T2.translation = 'Ola de fr\u00edo' ",
        "result_size": 1,
        "result": "(datetime.date(2006, 7, 21),)",
        "base_pg_sql": "SELECT T1.releaseDate FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T2.translation = 'Ola de fr\u00edo'",
        "base_question": "What is the release date for the set \"Ola de fr\u00edo\"?",
        "base_evidence": "release date is the date of card set being released; set \"Ola de fr\u00edo\" refers to translation = 'Ola de fr\u00edo'",
        "gt": {
            "type": "delete",
            "table": "sets",
            "condition": "SELECT sets.id FROM sets, set_translations AS T2 WHERE T2.setCode = sets.code AND T2.translation = 'Ola de fr\u00edo' "
        }
    },
    {
        "question_id": 295,
        "db_id": "toxicology",
        "question": "\"Delete all hydrogen atoms that belong to molecules labeled as carcinogenic compounds.\"",
        "evidence": "label = '+' mean molecules are carcinogenic; hydrogen refers to element = h'",
        "SQL": "SELECT COUNT(T1.atom_id) AS atomnums_h FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id WHERE T2.label = '+' AND T1.element = 'h'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM atom USING molecule AS T2 WHERE atom.molecule_id = T2.molecule_id AND T2.label = '+' AND atom.element = 'h' ",
        "result_size": 1,
        "result": "(1569,)",
        "base_pg_sql": "SELECT COUNT(T1.atom_id) AS atomnums_h FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id WHERE T2.label = '+' AND T1.element = 'h'",
        "base_question": "How many atoms belong to the molecule that element is hydrogen and labeled with carcinogenic compound?",
        "base_evidence": "label = '+' mean molecules are carcinogenic; hydrogen refers to element = h'",
        "gt": {
            "type": "delete",
            "table": "atom",
            "condition": "SELECT atom.atom_id FROM atom, molecule AS T2 WHERE atom.molecule_id = T2.molecule_id AND T2.label = '+' AND atom.element = 'h' "
        }
    },
    {
        "question_id": 519,
        "db_id": "card_games",
        "question": "\"Delete all translations for the 'Battlebond' set.\"",
        "evidence": "\"Battlebond\" set refers to name = 'Battlebond'",
        "SQL": "SELECT language FROM set_translations WHERE id IN ( SELECT id FROM sets WHERE name = 'Battlebond' )",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM set_translations WHERE id IN (SELECT id FROM sets WHERE name = 'Battlebond') ",
        "result_size": 1,
        "result": "('Chinese Simplified',)",
        "base_pg_sql": "SELECT language FROM set_translations WHERE id IN (SELECT id FROM sets WHERE name = 'Battlebond')",
        "base_question": "What is the language of the \"Battlebond\" set?",
        "base_evidence": "\"Battlebond\" set refers to name = 'Battlebond'",
        "gt": {
            "type": "delete",
            "table": "set_translations",
            "condition": "SELECT set_translations.id FROM set_translations WHERE id IN (SELECT id FROM sets WHERE name = 'Battlebond') "
        }
    },
    {
        "question_id": 97,
        "db_id": "financial",
        "question": "Delete all disposition records for clients who are disponents and are associated with accounts that have the 'issuance after transaction' frequency.",
        "evidence": "'POPLATEK PO OBRATU' stands for issuance after transaction",
        "SQL": "SELECT T2.client_id FROM account AS T1 INNER JOIN disp AS T2 ON T1.account_id = T2.account_id WHERE T1.frequency = 'POPLATEK PO OBRATU' AND T2.type = 'DISPONENT'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM disp USING account AS T1 WHERE T1.account_id = disp.account_id AND T1.frequency = 'POPLATEK PO OBRATU' AND disp.type = 'DISPONENT' ",
        "result_size": 14,
        "result": "(159,)####(384,)####(1261,)####(1706,)####(2304,)####(3526,)####(3609,)####(4035,)####(4133,)####(4334,)####(4625,)####(5580,)####(6318,)####(11979,)",
        "base_pg_sql": "SELECT T2.client_id FROM account AS T1 INNER JOIN disp AS T2 ON T1.account_id = T2.account_id WHERE T1.frequency = 'POPLATEK PO OBRATU' AND T2.type = 'DISPONENT'",
        "base_question": "List out the id number of client who choose statement of issuance after transaction are Disponent?",
        "base_evidence": "'POPLATEK PO OBRATU' stands for issuance after transaction",
        "gt": {
            "type": "delete",
            "table": "disp",
            "condition": "SELECT disp.disp_id FROM disp, account AS T1 WHERE T1.account_id = disp.account_id AND T1.frequency = 'POPLATEK PO OBRATU' AND disp.type = 'DISPONENT' "
        }
    },
    {
        "question_id": 703,
        "db_id": "codebase_community",
        "question": "Delete all tags with tag ID below 15 that have 20 or fewer posts associated with them.",
        "evidence": "ID below 15 refers to Id < 15; have 20 count of posts and below refers to Count < = 20;",
        "SQL": "SELECT COUNT(id) FROM tags WHERE Count <= 20 AND Id < 15",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM tags WHERE Count <= 20 AND Id < 15 ",
        "result_size": 1,
        "result": "(2,)",
        "base_pg_sql": "SELECT COUNT(id) FROM tags WHERE Count <= 20 AND Id < 15",
        "base_question": "Among the tags with tag ID below 15, how many of them have 20 count of posts and below?",
        "base_evidence": "ID below 15 refers to Id < 15; have 20 count of posts and below refers to Count < = 20;",
        "gt": {
            "type": "delete",
            "table": "tags",
            "condition": "SELECT tags.id FROM tags WHERE Count <= 20 AND Id < 15 "
        }
    },
    {
        "question_id": 405,
        "db_id": "card_games",
        "question": "Remove all sets from the Commander block that have been translated into Brazilian Portuguese.",
        "evidence": "Commander block refer to block = 'Commander'; sets refer to code = setCode; Portuguese refer to language = 'Portuguese (Brasil)'",
        "SQL": "SELECT COUNT(T1.id) FROM sets AS T1 INNER JOIN set_translations AS T2 ON T1.code = T2.setCode WHERE T2.language = 'Portuguese (Brazil)' AND T1.block = 'Commander'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM sets USING set_translations AS T2 WHERE sets.code = T2.setCode AND T2.language = 'Portuguese (Brazil)' AND sets.block = 'Commander' ",
        "result_size": 1,
        "result": "(7,)",
        "base_pg_sql": "SELECT COUNT(T1.id) FROM sets AS T1 INNER JOIN set_translations AS T2 ON T1.code = T2.setCode WHERE T2.language = 'Portuguese (Brazil)' AND T1.block = 'Commander'",
        "base_question": "How many Brazilian Portuguese translated sets are inside the Commander block?",
        "base_evidence": "Commander block refer to block = 'Commander'; sets refer to code = setCode; Portuguese refer to language = 'Portuguese (Brasil)'",
        "gt": {
            "type": "delete",
            "table": "sets",
            "condition": "SELECT sets.id FROM sets, set_translations AS T2 WHERE sets.code = T2.setCode AND T2.language = 'Portuguese (Brazil)' AND sets.block = 'Commander' "
        }
    },
    {
        "question_id": 675,
        "db_id": "codebase_community",
        "question": "Delete all users whose reputations are higher than 2000 and the number of views is higher than 1000.",
        "evidence": "reputations are higher than 2000 refer to Reputation > 2000; number of views is higher than 1000 refers to Views > 1000;",
        "SQL": "SELECT COUNT(id) FROM users WHERE Reputation > 2000 AND Views > 1000",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM users WHERE Reputation > 2000 AND Views > 1000 ",
        "result_size": 1,
        "result": "(44,)",
        "base_pg_sql": "SELECT COUNT(id) FROM users WHERE Reputation > 2000 AND Views > 1000",
        "base_question": "How many users whose reputations are higher than 2000 and the number of views is higher than 1000?",
        "base_evidence": "reputations are higher than 2000 refer to Reputation > 2000; number of views is higher than 1000 refers to Views > 1000;",
        "gt": {
            "type": "delete",
            "table": "users",
            "condition": "SELECT users.id FROM users WHERE Reputation > 2000 AND Views > 1000 "
        }
    },
    {
        "question_id": 256,
        "db_id": "toxicology",
        "question": "Delete all atoms from the database that consist of either the element carbon or the element hydrogen.",
        "evidence": "consisting of element carbon and hydrogen refers to element in('c', 'h')",
        "SQL": "SELECT COUNT(T.atom_id) FROM atom AS T WHERE T.element = 'c' OR T.element = 'h'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM atom WHERE atom.element = 'c' OR atom.element = 'h' ",
        "result_size": 1,
        "result": "(10123,)",
        "base_pg_sql": "SELECT COUNT(T.atom_id) FROM atom AS T WHERE T.element = 'c' OR T.element = 'h'",
        "base_question": "Calculate the total atoms consisting of the element carbon and hydrogen.",
        "base_evidence": "consisting of element carbon and hydrogen refers to element in('c', 'h')",
        "gt": {
            "type": "delete",
            "table": "atom",
            "condition": "SELECT atom.atom_id FROM atom WHERE atom.element = 'c' OR atom.element = 'h' "
        }
    },
    {
        "question_id": 1240,
        "db_id": "thrombosis_prediction",
        "question": "\"Delete all laboratory examination records from 1991 where the hematocrit level is below the normal range (HCT < 29) and the patient ID matches the laboratory record ID.\"",
        "evidence": "laboratory examinations in 1991 refers to Date like '1991%'; average hematoclit level = AVG(HCT); hematoclit level that is lower than the normal range refers to HCT < 29;",
        "SQL": "SELECT AVG(T2.HCT) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.HCT < 29 AND STRFTIME('%Y', T2.Date) = '1991'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM Laboratory USING Patient AS T1 WHERE T1.ID = Laboratory.ID AND Laboratory.HCT < 29 AND TO_CHAR(CAST(Laboratory.Date AS TIMESTAMP), 'YYYY') = '1991' ",
        "result_size": 1,
        "result": "(26.064179150026234,)",
        "base_pg_sql": "SELECT AVG(T2.HCT) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.HCT < 29 AND TO_CHAR(CAST(T2.Date AS TIMESTAMP), 'YYYY') = '1991'",
        "base_question": "From laboratory examinations in 1991, what is the average hematoclit level that is lower than the normal range.",
        "base_evidence": "laboratory examinations in 1991 refers to Date like '1991%'; average hematoclit level = AVG(HCT); hematoclit level that is lower than the normal range refers to HCT < 29;",
        "gt": {
            "type": "delete",
            "table": "Laboratory",
            "condition": "SELECT Laboratory.id FROM Laboratory, Patient AS T1 WHERE T1.ID = Laboratory.ID AND Laboratory.HCT < 29 AND TO_CHAR(CAST(Laboratory.Date AS TIMESTAMP), 'YYYY') = '1991' "
        }
    },
    {
        "question_id": 1259,
        "db_id": "thrombosis_prediction",
        "question": "\"Remove the records of patients born after January 1, 1985 who have a normal Rheumatoid Factor (where RA is either '-' or '+-').\"",
        "evidence": "diseases refers to Diagnosis; born after 1985/1/1 refers to YEAR(Birthday) > = 1985; normal Rhuematoid Factor refers to RA IN('-', '+-');",
        "SQL": "SELECT T1.Diagnosis FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE (T2.RA = '-' OR T2.RA = '+-') AND T1.Birthday > '1985-01-01'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM Patient USING Laboratory AS T2 WHERE Patient.ID = T2.ID AND (T2.RA = '-' OR T2.RA = '+-') AND Patient.Birthday > '1985-01-01' ",
        "result_size": 24,
        "result": "('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('SLE',)####('RA susp',)####('RA susp',)####('RA susp',)####('SJS',)####('SJS',)####('SJS',)####('SJS',)####('SJS',)",
        "base_pg_sql": "SELECT T1.Diagnosis FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE (T2.RA = '-' OR T2.RA = '+-') AND T1.Birthday > '1985-01-01'",
        "base_question": "Please list the diseases of the patients born after 1985-1-1 and have a normal Rhuematoid Factor.",
        "base_evidence": "diseases refers to Diagnosis; born after 1985/1/1 refers to YEAR(Birthday) > = 1985; normal Rhuematoid Factor refers to RA IN('-', '+-');",
        "gt": {
            "type": "delete",
            "table": "Patient",
            "condition": "SELECT Patient.id FROM Patient, Laboratory AS T2 WHERE Patient.ID = T2.ID AND (T2.RA = '-' OR T2.RA = '+-') AND Patient.Birthday > '1985-01-01' "
        }
    },
    {
        "question_id": 1267,
        "db_id": "thrombosis_prediction",
        "question": "Delete all examination records for patients who have normal anti-SM antibody test results (where SM is 'negative' or '0') and do not have thrombosis.",
        "evidence": "normal anti-SM refers to SM IN('-', '+-'); SM = 'negative' means '-'; SM = '0' means '+-'; SM = '1' means '+'; does not have thrombosis refers to Thrombosis = 0;",
        "SQL": "SELECT COUNT(T1.ID) FROM Examination AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.SM IN ('negative','0') AND T1.Thrombosis = 0",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM Examination USING Laboratory AS T2 WHERE Examination.ID = T2.ID AND T2.SM IN ('negative', '0') AND Examination.Thrombosis = 0 ",
        "result_size": 1,
        "result": "(7,)",
        "base_pg_sql": "SELECT COUNT(T1.ID) FROM Examination AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.SM IN ('negative', '0') AND T1.Thrombosis = 0",
        "base_question": "Among the patients with normal anti-SM, how many of them does not have thrombosis?",
        "base_evidence": "normal anti-SM refers to SM IN('-', '+-'); SM = 'negative' means '-'; SM = '0' means '+-'; SM = '1' means '+'; does not have thrombosis refers to Thrombosis = 0;",
        "gt": {
            "type": "delete",
            "table": "Examination",
            "condition": "SELECT Examination.* FROM Examination, Laboratory AS T2 WHERE Examination.ID = T2.ID AND T2.SM IN ('negative', '0') AND Examination.Thrombosis = 0 "
        }
    },
    {
        "question_id": 636,
        "db_id": "codebase_community",
        "question": "Delete all negative comments (those with a score below 60) from Neil McGuigan's posts.",
        "evidence": "Negative comment refers to score < 60; DisplayName = 'Neil McGuigan';",
        "SQL": "SELECT COUNT(T3.Id) FROM users AS T1 INNER JOIN posts AS T2 ON T1.Id = T2.OwnerUserId INNER JOIN comments AS T3 ON T2.Id = T3.PostId WHERE T1.DisplayName = 'Neil McGuigan' AND T3.Score < 60",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM comments USING users AS T1, posts AS T2 WHERE T1.Id = T2.OwnerUserId AND T2.Id = comments.PostId AND T1.DisplayName = 'Neil McGuigan' AND comments.Score < 60 ",
        "result_size": 1,
        "result": "(136,)",
        "base_pg_sql": "SELECT COUNT(T3.Id) FROM users AS T1 INNER JOIN posts AS T2 ON T1.Id = T2.OwnerUserId INNER JOIN comments AS T3 ON T2.Id = T3.PostId WHERE T1.DisplayName = 'Neil McGuigan' AND T3.Score < 60",
        "base_question": "How many negative comments did Neil McGuigan get in his posts?",
        "base_evidence": "Negative comment refers to score < 60; DisplayName = 'Neil McGuigan';",
        "gt": {
            "type": "delete",
            "table": "comments",
            "condition": "SELECT comments.id FROM comments, users AS T1, posts AS T2 WHERE T1.Id = T2.OwnerUserId AND T2.Id = comments.PostId AND T1.DisplayName = 'Neil McGuigan' AND comments.Score < 60 "
        }
    },
    {
        "question_id": 1362,
        "db_id": "student_club",
        "question": "Delete all zip code entries for Orange County in the state of Virginia.",
        "evidence": "Orange County is the county name, Virginia is the state name",
        "SQL": "SELECT COUNT(city) FROM zip_code WHERE county = 'Orange County' AND state = 'Virginia'",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM zip_code WHERE county = 'Orange County' AND state = 'Virginia' ",
        "result_size": 1,
        "result": "(7,)",
        "base_pg_sql": "SELECT COUNT(city) FROM zip_code WHERE county = 'Orange County' AND state = 'Virginia'",
        "base_question": "How many cities are there in Orange County, Virginia?",
        "base_evidence": "Orange County is the county name, Virginia is the state name",
        "gt": {
            "type": "delete",
            "table": "zip_code",
            "condition": "SELECT zip_code.zip_code FROM zip_code WHERE county = 'Orange County' AND state = 'Virginia' "
        }
    },
    {
        "question_id": 1254,
        "db_id": "thrombosis_prediction",
        "question": "Delete the records of patients who have a normal Ig A level (between 80 and 500) and came to the hospital after 1990.",
        "evidence": "normal Ig A level refers to IGA > 80 AND IGA < 500; came to the hospital after 1990/1/1 refers to YEAR(`First Date`) > = 1990;",
        "SQL": "SELECT COUNT(T1.ID) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.IGA BETWEEN 80 AND 500 AND  strftime('%Y',  T1.`First Date`) > '1990'",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM Patient USING Laboratory AS T2 WHERE Patient.ID = T2.ID AND T2.IGA BETWEEN 80 AND 500 AND TO_CHAR(CAST(Patient.\"First Date\" AS TIMESTAMP), 'YYYY') > '1990' ",
        "result_size": 1,
        "result": "(1590,)",
        "base_pg_sql": "SELECT COUNT(T1.ID) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.IGA BETWEEN 80 AND 500 AND TO_CHAR(CAST(T1.\"First Date\" AS TIMESTAMP), 'YYYY') > '1990'",
        "base_question": "How many patients with a normal Ig A level came to the hospital after 1990/1/1?",
        "base_evidence": "normal Ig A level refers to IGA > 80 AND IGA < 500; came to the hospital after 1990/1/1 refers to YEAR(`First Date`) > = 1990;",
        "gt": {
            "type": "delete",
            "table": "Patient",
            "condition": "SELECT Patient.id FROM Patient, Laboratory AS T2 WHERE Patient.ID = T2.ID AND T2.IGA BETWEEN 80 AND 500 AND TO_CHAR(CAST(Patient.\"First Date\" AS TIMESTAMP), 'YYYY') > '1990' "
        }
    },
    {
        "question_id": 434,
        "db_id": "card_games",
        "question": "\"Delete all sets that are available in Japanese but not available in Magic: The Gathering Online.\"",
        "evidence": "Japanese refers to language = 'Japanese'; not in Magic: The Gathering Online refers to mtgoCode is null or mtgoCode = ''",
        "SQL": "SELECT COUNT(T1.id) FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T2.language = 'Japanese'  AND (T1.mtgoCode IS NULL OR T1.mtgoCode = '')",
        "difficulty": "moderate",
        "pg_sql": "DELETE FROM sets USING set_translations AS T2 WHERE T2.setCode = sets.code AND T2.language = 'Japanese' AND (sets.mtgoCode IS NULL OR sets.mtgoCode = '') ",
        "result_size": 1,
        "result": "(22,)",
        "base_pg_sql": "SELECT COUNT(T1.id) FROM sets AS T1 INNER JOIN set_translations AS T2 ON T2.setCode = T1.code WHERE T2.language = 'Japanese' AND (T1.mtgoCode IS NULL OR T1.mtgoCode = '')",
        "base_question": "How many sets are available just in Japanese and not in Magic: The Gathering Online?",
        "base_evidence": "Japanese refers to language = 'Japanese'; not in Magic: The Gathering Online refers to mtgoCode is null or mtgoCode = ''",
        "gt": {
            "type": "delete",
            "table": "sets",
            "condition": "SELECT sets.id FROM sets, set_translations AS T2 WHERE T2.setCode = sets.code AND T2.language = 'Japanese' AND (sets.mtgoCode IS NULL OR sets.mtgoCode = '') "
        }
    },
    {
        "question_id": 686,
        "db_id": "codebase_community",
        "question": "Delete all posts that have a view count above the average view count of all posts.",
        "evidence": "views above average refer to ViewCount > AVG(ViewCount);",
        "SQL": "SELECT Id FROM posts WHERE ViewCount > ( SELECT AVG(ViewCount) FROM posts )",
        "difficulty": "simple",
        "pg_sql": "DELETE FROM posts WHERE ViewCount > (SELECT AVG(ViewCount) FROM posts) ",
        "result_size": 7689,
        "result": "(562,)####(563,)####(564,)####(570,)####(573,)####(575,)####(577,)####(581,)####(622,)####(23159,)####(608,)####(612,)####(614,)####(631,)####(712,)####(638,)####(641,)####(643,)####(645,)####(652,)####(665,)####(672,)####(715,)####(723,)####(725,)####(726,)####(24964,)####(764,)####(775,)####(779,)####(790,)####(795,)####(798,)####(806,)####(812,)####(825,)####(834,)####(837,)####(841,)####(856,)####(859,)####(866,)####(868,)####(870,)####(871,)####(877,)####(884,)####(886,)####(887,)####(890,)####(897,)####(898,)####(899,)####(913,)####(920,)####(924,)####(927,)####(928,)####(942,)####(946,)####(949,)####(951,)####(955,)####(961,)####(973,)####(980,)####(1028,)####(1001,)####(1015,)####(1016,)####(1023,)####(1040,)####(1047,)####(1052,)####(1053,)####(1060,)####(1062,)####(1063,)####(1082,)####(1084,)####(2125,)####(1099,)####(1112,)####(1115,)####(1133,)####(1142,)####(1149,)####(2917,)####(1164,)####(1169,)####(1184,)####(1173,)####(1174,)####(1194,)####(1205,)####(1207,)####(14088,)####(1223,)####(1241,)####(1252,)####(1256,)####(1266,)####(1268,)####(1274,)####(1278,)####(1286,)####(1289,)####(1292,)####(1293,)####(1296,)####(1308,)####(1315,)####(1337,)####(1350,)####(1352,)####(1357,)####(1380,)####(1385,)####(1386,)####(1389,)####(1399,)####(1405,)####(1412,)####(1424,)####(1430,)####(1432,)####(1444,)####(1447,)####(1455,)####(1458,)####(1459,)####(1460,)####(1462,)####(1471,)####(1475,)####(1520,)####(1517,)####(1519,)####(1521,)####(1525,)####(1531,)####(1536,)####(1538,)####(1540,)####(1542,)####(1576,)####(1555,)####(1556,)####(1562,)####(1564,)####(1571,)####(1580,)####(1590,)####(1595,)####(1601,)####(1610,)####(1611,)####(1637,)####(1645,)####(1646,)####(1651,)####(1660,)####(1667,)####(1668,)####(1676,)####(1699,)####(1708,)####(1709,)####(1713,)####(1719,)####(1729,)####(1735,)####(1736,)####(1737,)####(1915,)####(1753,)####(1761,)####(1764,)####(1773,)####(1829,)####(1781,)####(1787,)####(1838,)####(1797,)####(1799,)####(1805,)####(1807,)####(1812,)####(1815,)####(1818,)####(1822,)####(1826,)####(1844,)####(1848,)####(1850,)####(1853,)####(1856,)####(1862,)####(1863,)####(1866,)####(1875,)####(1881,)####(1883,)####(1927,)####(1935,)####(28393,)####(1961,)####(1963,)####(1964,)####(1972,)####(35185,)####(1980,)####(1995,)####(1998,)####(2002,)####(2007,)####(2032,)####(2035,)####(2037,)####(2038,)####(2061,)####(2066,)####(2067,)####(2091,)####(2077,)####(2092,)####(2085,)####(2086,)####(2099,)####(2104,)####(2108,)####(2111,)####(2116,)####(2119,)####(2121,)####(2131,)####(2134,)####(2140,)####(2142,)####(2179,)####(2328,)####(2149,)####(2151,)####(2181,)####(2167,)####(2171,)####(2182,)####(2198,)####(2213,)####(2219,)####(2220,)####(2230,)####(2234,)####(2244,)####(2245,)####(2248,)####(2256,)####(2259,)####(2262,)####(2272,)####(2290,)####(2291,)####(2296,)####(2306,)####(2335,)####(2337,)####(2344,)####(2348,)####(2350,)####(2352,)####(2377,)####(2356,)####(2358,)####(2374,)####(2379,)####(2390,)####(2391,)####(2397,)####(2918,)####(2401,)####(2430,)####(2432,)####(2419,)####(2423,)####(2427,)####(2467,)####(2592,)####(2455,)####(2457,)####(2469,)####(2476,)####(2481,)####(2492,)####(2493,)####(2499,)####(2504,)####(2513,)####(2516,)####(2537,)####(2541,)####(2547,)####(2572,)####(2573,)####(3484,)####(2563,)####(2576,)####(2579,)####(2591,)####(2597,)####(2602,)####(2611,)####(2613,)####(2617,)####(2619,)####(2623,)####(2628,)####(2635,)####(2639,)####(2641,)####(2650,)####(2661,)####(2746,)####(2670,)####(2675,)####(2679,)####(28395,)####(2684,)####(2688,)####(2689,)####(2691,)####(2748,)####(2715,)####(2717,)####(2728,)####(2730,)####(2742,)####(2743,)####(2764,)####(2768,)####(2770,)####(2772,)####(2777,)####(2788,)####(2794,)####(2806,)####(2824,)####(2819,)####(2828,)####(2860,)####(2844,)####(2846,)####(2849,)####(2852,)####(2854,)####(2863,)####(2886,)####(2914,)####(2892,)####(2893,)####(2904,)####(2909,)####(2910,)####(2934,)####(2938,)####(15981,)####(2948,)####(2950,)####(2957,)####(2962,)####(3165,)####(2971,)####(2972,)####(2975,)####(2976,)####(2981,)####(2982,)####(2988,)####(3006,)####(3024,)####(3031,)####(3038,)####(3048,)####(3051,)####(3052,)####(3069,)####(3082,)####(3086,)####(3091,)####(3104,)####(3105,)####(3112,)####(3113,)####(3115,)####(3136,)####(3140,)####(3143,)####(3158,)####(3173,)####(3175,)####(3176,)####(3179,)####(3181,)####(3194,)####(3199,)####(3200,)####(3212,)####(3215,)####(3235,)####(3238,)####(3242,)####(3244,)####(3249,)####(3252,)####(3259,)####(4799,)####(3262,)####(3270,)####(3271,)####(3275,)####(29325,)####(3286,)####(3287,)####(3294,)####(3316,)####(3321,)####(3324,)####(3328,)####(3331,)####(3333,)####(3342,)####(3359,)####(3362,)####(3364,)####(4603,)####(3369,)####(3372,)####(3377,)####(3381,)####(3383,)####(4040,)####(3386,)####(3390,)####(3392,)####(3400,)####(3407,)####(3412,)####(3413,)####(3425,)####(20298,)####(3438,)####(3445,)####(3446,)####(3458,)####(3460,)####(3474,)####(3463,)####(3466,)####(3476,)####(3479,)####(3489,)####(3496,)####(3504,)####(3516,)####(3511,)####(3514,)####(3519,)####(3520,)####(3526,)####(3537,)####(3539,)####(3542,)####(3549,)####(3556,)####(3559,)####(3561,)####(3564,)####(3575,)####(3584,)####(3586,)####(3589,)####(3595,)####(3609,)####(3611,)####(3614,)####(3616,)####(3628,)####(3634,)####(3640,)####(3652,)####(3653,)####(3719,)####(35076,)####(3661,)####(3685,)####(4575,)####(3695,)####(3698,)####(3704,)####(3707,)####(3708,)####(3713,)####(3724,)####(3730,)####(3734,)####(3746,)####(3749,)####(3754,)####(3757,)####(3758,)####(3772,)####(3779,)####(3782,)####(3787,)####(3788,)####(3818,)####(3795,)####(3804,)####(3820,)####(3810,)####(3814,)####(3826,)####(3842,)####(3845,)####(3849,)####(3853,)####(3857,)####(3872,)####(3874,)####(3879,)####(3888,)####(3890,)####(4818,)####(3892,)####(3893,)####(3898,)####(3902,)####(3907,)####(3911,)####(3915,)####(3921,)####(3930,)####(3931,)####(3944,)####(3943,)####(3947,)####(3965,)####(3967,)####(3982,)####(3984,)####(3989,)####(3997,)####(3999,)####(4004,)####(4009,)####(4013,)####(4017,)####(4019,)####(4023,)####(4025,)####(4030,)####(4034,)####(4044,)####(4052,)####(4058,)####(4062,)####(4068,)####(4072,)####(4075,)####(4081,)####(4084,)####(4086,)####(4089,)####(4093,)####(4099,)####(4101,)####(4104,)####(4111,)####(4121,)####(4239,)####(4125,)####(4131,)####(4150,)####(4165,)####(4175,)####(4185,)####(4187,)####(4191,)####(4200,)####(4211,)####(4219,)####(4220,)####(4245,)####(4252,)####(4258,)####(4259,)####(4267,)####(4272,)####(4276,)####(4279,)####(4284,)####(4292,)####(4294,)####(4296,)####(4298,)####(4303,)####(4305,)####(4312,)####(4316,)####(4320,)####(4328,)####(4331,)####(4334,)####(4335,)####(4360,)####(4337,)####(4341,)####(4353,)####(4354,)####(4356,)####(4364,)####(4367,)####(4368,)####(4383,)####(4392,)####(4394,)####(4396,)####(4401,)####(4407,)####(4422,)####(4489,)####(4429,)####(4437,)####(4441,)####(4445,)####(4451,)####(4453,)####(4462,)####(4465,)####(4466,)####(4471,)####(4473,)####(4485,)####(4496,)####(4498,)####(4510,)####(4517,)####(4528,)####(4530,)####(4539,)####(4543,)####(4544,)####(4551,)####(4556,)####(4561,)####(4568,)####(4569,)####(4579,)####(4580,)####(4585,)####(4600,)####(4608,)####(4609,)####(4610,)####(4612,)####(4620,)####(4639,)####(4640,)####(4663,)####(4642,)####(4649,)####(4652,)####(4655,)####(4658,)####(4659,)####(4708,)####(4671,)####(4685,)####(4687,)####(4689,)####(4691,)####(4700,)####(4713,)####(4717,)####(4734,)####(4735,)####(4737,)####(4753,)####(4756,)####(4759,)####(4762,)####(4766,)####(4768,)####(4775,)####(4783,)####(4805,)####(4810,)####(4812,)####(4814,)####(4816,)####(4823,)####(4834,)####(4830,)####(4831,)####(4832,)####(4839,)####(4844,)####(4854,)####(4856,)####(4858,)####(4868,)####(4878,)####(4892,)####(4898,)####(4901,)####(4912,)####(10075,)####(4920,)####(4930,)####(11296,)####(4934,)####(4936,)####(4939,)####(4942,)####(4945,)####(4949,)####(4959,)####(4961,)####(4964,)####(4970,)####(4978,)####(4980,)####(4983,)####(4987,)####(4989,)####(4991,)####(4997,)####(5004,)####(5007,)####(5013,)####(5015,)####(5025,)####(5026,)####(5038,)####(5042,)####(5559,)####(5056,)####(5058,)####(5062,)####(5065,)####(5070,)####(5073,)####(5077,)####(5078,)####(5079,)####(5081,)####(5092,)####(5093,)####(5097,)####(5109,)####(5115,)####(5119,)####(5135,)####(5149,)####(5150,)####(5158,)####(5159,)####(5160,)####(5170,)####(5171,)####(5172,)####(5187,)####(5184,)####(5189,)####(5195,)####(5196,)####(5206,)####(5207,)####(5220,)####(5235,)####(5228,)####(5238,)####(5247,)####(5250,)####(5253,)####(5270,)####(5271,)####(5275,)####(5278,)####(5288,)####(5290,)####(5292,)####(5293,)####(5327,)####(5299,)####(5304,)####(5305,)####(5306,)####(5308,)####(5321,)####(5333,)####(5340,)####(5343,)####(5344,)####(5346,)####(5347,)####(5351,)####(5354,)####(5364,)####(5360,)####(5366,)####(5382,)####(5387,)####(5392,)####(5399,)####(5414,)####(5418,)####(5431,)####(5434,)####(5448,)####(5450,)####(5452,)####(5457,)####(5462,)####(5465,)####(5479,)####(5487,)####(5502,)####(5504,)####(5517,)####(5525,)####(5543,)####(5563,)####(5571,)####(5591,)####(5601,)####(5604,)####(5680,)####(5614,)####(5617,)####(5619,)####(5682,)####(5634,)####(5647,)####(5654,)####(5656,)####(5675,)####(5686,)####(5690,)####(5691,)####(5700,)####(5713,)####(5727,)####(5728,)####(5733,)####(5734,)####(5747,)####(5750,)####(5757,)####(5768,)####(5774,)####(5782,)####(5786,)####(5788,)####(5791,)####(5792,)####(5807,)####(5808,)####(5819,)####(5826,)####(5827,)####(5836,)####(5842,)####(5844,)####(5847,)####(5853,)####(5854,)####(5859,)####(5880,)####(5885,)####(5893,)####(5903,)####(5912,)####(5913,)####(5918,)####(5926,)####(5934,)####(5935,)####(5937,)####(5945,)####(5952,)####(5954,)####(5960,)####(5962,)####(5967,)####(5969,)####(5995,)####(5997,)####(6005,)####(6008,)####(6013,)####(6020,)####(6021,)####(6022,)####(6026,)####(6033,)####(6448,)####(6042,)####(6044,)####(6046,)####(6050,)####(6056,)####(6063,)####(6067,)####(6071,)####(6074,)####(6076,)####(6077,)####(6079,)####(6081,)####(6093,)####(6493,)####(6104,)####(6119,)####(6122,)####(6127,)####(6139,)####(6141,)####(6142,)####(6155,)####(6146,)####(6151,)####(6152,)####(6163,)####(6195,)####(6169,)####(6176,)####(6214,)####(6181,)####(6187,)####(6189,)####(6206,)####(6208,)####(6219,)####(6224,)####(6225,)####(6226,)####(6232,)####(6234,)####(6239,)####(6247,)####(6252,)####(6265,)####(6304,)####(6275,)####(6279,)####(6281,)####(6294,)####(6298,)####(6302,)####(6306,)####(6308,)####(6309,)####(6312,)####(6314,)####(6318,)####(6329,)####(6330,)####(6498,)####(6342,)####(6350,)####(6353,)####(6355,)####(6358,)####(6364,)####(6368,)####(6380,)####(6400,)####(6410,)####(6412,)####(6421,)####(6442,)####(6453,)####(6454,)####(6455,)####(6468,)####(6469,)####(6473,)####(6475,)####(6477,)####(6478,)####(6484,)####(6487,)####(6492,)####(6502,)####(6505,)####(6524,)####(6513,)####(6534,)####(6538,)####(6540,)####(6544,)####(6549,)####(6557,)####(6759,)####(6579,)####(6581,)####(6582,)####(6599,)####(6601,)####(6604,)####(6609,)####(6613,)####(6624,)####(6630,)####(6637,)####(6638,)####(6652,)####(6653,)####(6655,)####(6658,)####(6660,)####(6664,)####(6670,)####(6684,)####(6706,)####(6688,)####(6702,)####(6705,)####(6720,)####(6723,)####(6731,)####(6728,)####(6740,)####(6734,)####(6753,)####(6755,)####(6758,)####(6770,)####(6779,)####(6780,)####(6772,)####(6773,)####(6776,)####(6791,)####(6794,)####(6795,)####(6860,)####(6806,)####(6809,)####(6814,)####(6824,)####(6827,)####(6835,)####(6840,)####(6841,)####(7045,)####(6852,)####(6853,)####(6855,)####(6856,)####(6865,)####(6867,)####(6870,)####(6874,)####(6876,)####(6883,)####(6890,)####(6896,)####(6907,)####(6912,)####(6913,)####(6920,)####(6927,)####(6939,)####(6940,)####(6943,)####(6946,)####(6954,)####(6956,)####(6958,)####(6966,)####(7015,)####(6989,)####(7004,)####(7400,)####(7007,)####(7009,)####(7020,)####(7022,)####(7029,)####(7036,)####(7040,)####(7047,)####(7048,)####(7049,)####(7057,)####(7059,)####(7070,)####(7074,)####(7079,)####(7084,)####(7088,)####(7089,)####(7100,)####(7101,)####(7102,)####(7110,)####(7111,)####(7112,)####(7402,)####(7129,)####(7132,)####(7202,)####(7146,)####(7200,)####(7152,)####(7155,)####(7156,)####(7173,)####(7175,)####(7180,)####(7429,)####(7185,)####(7197,)####(7207,)####(7208,)####(7211,)####(7430,)####(7218,)####(7223,)####(7224,)####(7225,)####(7233,)####(7238,)####(7239,)####(7240,)####(7244,)####(7271,)####(7249,)####(7250,)####(7259,)####(7261,)####(7263,)####(7268,)####(7270,)####(7278,)####(7279,)####(7286,)####(7292,)####(7295,)####(7307,)####(7308,)####(7314,)####(7316,)####(7318,)####(7319,)####(7394,)####(7322,)####(7344,)####(7348,)####(7351,)####(7357,)####(7358,)####(7362,)####(7366,)####(7376,)####(7385,)####(7389,)####(7415,)####(7419,)####(7422,)####(7426,)####(7432,)####(7438,)####(7439,)####(7440,)####(7487,)####(7447,)####(7450,)####(7457,)####(7466,)####(7519,)####(7467,)####(7471,)####(7475,)####(7476,)####(7481,)####(7497,)####(7508,)####(7513,)####(7515,)####(7527,)####(7531,)####(7532,)####(7535,)####(7542,)####(7551,)####(7554,)####(7555,)####(7559,)####(7562,)####(7579,)####(7581,)####(7595,)####(7610,)####(7611,)####(7613,)####(7625,)####(7630,)####(7631,)####(7637,)####(7638,)####(7648,)####(7675,)####(7679,)####(7683,)####(7698,)####(7699,)####(7701,)####(7714,)####(7730,)####(7718,)####(7720,)####(7723,)####(7727,)####(7732,)####(7734,)####(7754,)####(7757,)####(7766,)####(7771,)####(7774,)####(7775,)####(7780,)####(7782,)####(7790,)####(7795,)####(7799,)####(7813,)####(7815,)####(7825,)####(7826,)####(7829,)####(7836,)####(7839,)####(7845,)####(7850,)####(7853,)####(7860,)####(7876,)####(7882,)####(7884,)####(7899,)####(7900,)####(7902,)####(7903,)####(7912,)####(7915,)####(7919,)####(7923,)####(7925,)####(7929,)####(7935,)####(7939,)####(7941,)####(7946,)####(7948,)####(7952,)####(7956,)####(7959,)####(7975,)####(7977,)####(7999,)####(8000,)####(8006,)####(8013,)####(8015,)####(8019,)####(8021,)####(11950,)####(8025,)####(8029,)####(8033,)####(8063,)####(8040,)####(12549,)####(8052,)####(8055,)####(8058,)####(8071,)####(8072,)####(8082,)####(8083,)####(8088,)####(8090,)####(8096,)####(8104,)####(8106,)####(8107,)####(8114,)####(8117,)####(8119,)####(8126,)####(8137,)####(8135,)####(8143,)####(8148,)####(8151,)####(8152,)####(8167,)####(28292,)####(8180,)####(8182,)####(8184,)####(8185,)####(8192,)####(8196,)####(8261,)####(8206,)####(8213,)####(8222,)####(8225,)####(8232,)####(8237,)####(8242,)####(8243,)####(8251,)####(8254,)####(8266,)####(8267,)####(8271,)####(8273,)####(8285,)####(8291,)####(8297,)####(8300,)####(8303,)####(8306,)####(8344,)####(8309,)####(8318,)####(8321,)####(8328,)####(8330,)####(8334,)####(8335,)####(8340,)####(8342,)####(8347,)####(8351,)####(12763,)####(8370,)####(8371,)####(8373,)####(8375,)####(8377,)####(8396,)####(8399,)####(8401,)####(8405,)####(8407,)####(8428,)####(8419,)####(31970,)####(8447,)####(8455,)####(8456,)####(8466,)####(8472,)####(8485,)####(8487,)####(8501,)####(8504,)####(8505,)####(8511,)####(8513,)####(8514,)####(8515,)####(8528,)####(8545,)####(8557,)####(8561,)####(8567,)####(8572,)####(8573,)####(8583,)####(8590,)####(8598,)####(8604,)####(8605,)####(8614,)####(8617,)####(8625,)####(8662,)####(8634,)####(8642,)####(8649,)####(8661,)####(8663,)####(8664,)####(8689,)####(8692,)####(8695,)####(8696,)####(8714,)####(8718,)####(8729,)####(8732,)####(8733,)####(8734,)####(8742,)####(8744,)####(8749,)####(8750,)####(8754,)####(9016,)####(8755,)####(8774,)####(8777,)####(8779,)####(8784,)####(8788,)####(8797,)####(8798,)####(8799,)####(8807,)####(8812,)####(8817,)####(8818,)####(8823,)####(8844,)####(8846,)####(8854,)####(8859,)####(8868,)####(8903,)####(8877,)####(8883,)####(8891,)####(8904,)####(8907,)####(8911,)####(8918,)####(8924,)####(8930,)####(8933,)####(8955,)####(8956,)####(8959,)####(8960,)####(8974,)####(8980,)####(8997,)####(9001,)####(9013,)####(9023,)####(9027,)####(9029,)####(9036,)####(9040,)####(9050,)####(9053,)####(9062,)####(9067,)####(9068,)####(9071,)####(9074,)####(9085,)####(9099,)####(9104,)####(9105,)####(9109,)####(9111,)####(9116,)####(9127,)####(9131,)####(9137,)####(9155,)####(9156,)####(9159,)####(9202,)####(9171,)####(9182,)####(9192,)####(9198,)####(9201,)####(9220,)####(9225,)####(9233,)####(9240,)####(9242,)####(9253,)####(9267,)####(9275,)####(9276,)####(9281,)####(9283,)####(9311,)####(9312,)####(9315,)####(9318,)####(9322,)####(9327,)####(9329,)####(9331,)####(9334,)####(9342,)####(9354,)####(9357,)####(9358,)####(9365,)####(9378,)####(9385,)####(9390,)####(9396,)####(9398,)####(9405,)####(9407,)####(9415,)####(9425,)####(9427,)####(9429,)####(9446,)####(9449,)####(9454,)####(9456,)####(9457,)####(9475,)####(9477,)####(9478,)####(9494,)####(9483,)####(9500,)####(9501,)####(9506,)####(9507,)####(9508,)####(9510,)####(9512,)####(9542,)####(9535,)####(9541,)####(9549,)####(9557,)####(9561,)####(9566,)####(9573,)####(9581,)####(9586,)####(9587,)####(9590,)####(9592,)####(9604,)####(9599,)####(9607,)####(9610,)####(9614,)####(9617,)####(9623,)####(9625,)####(9626,)####(9627,)####(9629,)####(9637,)####(9767,)####(9653,)####(9656,)####(9664,)####(9674,)####(9685,)####(9691,)####(9692,)####(9693,)####(9695,)####(9699,)####(9712,)####(9715,)####(9718,)####(9729,)####(9735,)####(9738,)####(9739,)####(9745,)####(9749,)####(9751,)####(9759,)####(9763,)####(9775,)####(9779,)####(9785,)####(9801,)####(9807,)####(9809,)####(9810,)####(9825,)####(9833,)####(9836,)####(9839,)####(9842,)####(9845,)####(9850,)####(9852,)####(9854,)####(9859,)####(9867,)####(9868,)####(9871,)####(9872,)####(9879,)####(9885,)####(9886,)####(9892,)####(9893,)####(9895,)####(9898,)####(9904,)####(9913,)####(9918,)####(9920,)####(9930,)####(9931,)####(9937,)####(10078,)####(9951,)####(9959,)####(9961,)####(9962,)####(9971,)####(9972,)####(9981,)####(9987,)####(9988,)####(9990,)####(10001,)####(10005,)####(10008,)####(10011,)####(10017,)####(10020,)####(10030,)####(10036,)####(10037,)####(10038,)####(10049,)####(10053,)####(10059,)####(10062,)####(10066,)####(10074,)####(10079,)####(10088,)####(10092,)####(10095,)####(10111,)####(10137,)####(10146,)####(10159,)####(10160,)####(10162,)####(10167,)####(10171,)####(10177,)####(10182,)####(10185,)####(10204,)####(10211,)####(10213,)####(10220,)####(10225,)####(10228,)####(10234,)####(10236,)####(10238,)####(10241,)####(10250,)####(10251,)####(10267,)####(10271,)####(10285,)####(10289,)####(10302,)####(10305,)####(10308,)####(10309,)####(10316,)####(10328,)####(10337,)####(10338,)####(10342,)####(10343,)####(10347,)####(10353,)####(10356,)####(10359,)####(10363,)####(10366,)####(10459,)####(10370,)####(10375,)####(10378,)####(10388,)####(10396,)####(10411,)####(10419,)####(10420,)####(10423,)####(10425,)####(10427,)####(10429,)####(10433,)####(10439,)####(10441,)####(10444,)####(10454,)####(10464,)####(10478,)####(10480,)####(10497,)####(10501,)####(10510,)####(10517,)####(10519,)####(10532,)####(10534,)####(10539,)####(10540,)####(10543,)####(10544,)####(10546,)####(10551,)####(10553,)####(10557,)####(10562,)####(10564,)####(10567,)####(10569,)####(10639,)####(10574,)####(10578,)####(10591,)####(10594,)####(10597,)####(10603,)####(10604,)####(10607,)####(10608,)####(10613,)####(10621,)####(10640,)####(10649,)####(10655,)####(10672,)####(10676,)####(10680,)####(10687,)####(10697,)####(10700,)####(10702,)####(10712,)####(10723,)####(10726,)####(10744,)####(11109,)####(10750,)####(10766,)####(10773,)####(10774,)####(10784,)####(10787,)####(10788,)####(10795,)####(10800,)####(10807,)####(10821,)####(10827,)####(10832,)####(10838,)####(10856,)####(10858,)####(10867,)####(10884,)####(10890,)####(10900,)####(10904,)####(10905,)####(10907,)####(10910,)####(10926,)####(10943,)####(10947,)####(10951,)####(10964,)####(10965,)####(10975,)####(10985,)####(10986,)####(10987,)####(10996,)####(11009,)####(11000,)####(11008,)####(11018,)####(11021,)####(11033,)####(11043,)####(11050,)####(11054,)####(11060,)####(11064,)####(11066,)####(11079,)####(11084,)####(11085,)####(11087,)####(11088,)####(11093,)####(11096,)####(11102,)####(11105,)####(11107,)####(11108,)####(11112,)####(11113,)####(11115,)####(11120,)####(11127,)####(11131,)####(11135,)####(11141,)####(11175,)####(11142,)####(11149,)####(11165,)####(11168,)####(11172,)####(11178,)####(11182,)####(11189,)####(11191,)####(11193,)####(11200,)####(11202,)####(11209,)####(11210,)####(11219,)####(11231,)####(11233,)####(11249,)####(11252,)####(11256,)####(11257,)####(11263,)####(11266,)####(11273,)####(11280,)####(11290,)####(11310,)####(11315,)####(11420,)####(11336,)####(11337,)####(11347,)####(11351,)####(11359,)####(11368,)####(11372,)####(11375,)####(11381,)####(11384,)####(11385,)####(11421,)####(11402,)####(11405,)####(11406,)####(11412,)####(11413,)####(11414,)####(11418,)####(11419,)####(11437,)####(11440,)####(11448,)####(11450,)####(11457,)####(11459,)####(11462,)####(11473,)####(17609,)####(11490,)####(11494,)####(11505,)####(17842,)####(11527,)####(11531,)####(11541,)####(11544,)####(11546,)####(11551,)####(11553,)####(11578,)####(11580,)####(11583,)####(11590,)####(11595,)####(11602,)####(11622,)####(11607,)####(11609,)####(11628,)####(11632,)####(11634,)####(11636,)####(11645,)####(11646,)####(11650,)####(11653,)####(11657,)####(11659,)####(11661,)####(11662,)####(11676,)####(11677,)####(11680,)####(11682,)####(11691,)####(11699,)####(11703,)####(11707,)####(11713,)####(11714,)####(11724,)####(11746,)####(11764,)####(11769,)####(11795,)####(11800,)####(11807,)####(11808,)####(11812,)####(11821,)####(11829,)####(11832,)####(11835,)####(11850,)####(11856,)####(11859,)####(11869,)####(11872,)####(11876,)####(11887,)####(11899,)####(11919,)####(11923,)####(11924,)####(11927,)####(11935,)####(11945,)####(11947,)####(11959,)####(11960,)####(12648,)####(11971,)####(11985,)####(11987,)####(11989,)####(11991,)####(12002,)####(12005,)####(12007,)####(12023,)####(12046,)####(12029,)####(12030,)####(12039,)####(12041,)####(12053,)####(12054,)####(12059,)####(12067,)####(12068,)####(12069,)####(12072,)####(12081,)####(12087,)####(12090,)####(12094,)####(12107,)####(12112,)####(12118,)####(12119,)####(12124,)####(12164,)####(12128,)####(12140,)####(12163,)####(12151,)####(12152,)####(12155,)####(12156,)####(12174,)####(12177,)####(12186,)####(12187,)####(12197,)####(12200,)####(12205,)####(12209,)####(12223,)####(12225,)####(12231,)####(12232,)####(12260,)####(12261,)####(12262,)####(12273,)####(12275,)####(12282,)####(12285,)####(12290,)####(12294,)####(12305,)####(12313,)####(12320,)####(12330,)####(12341,)####(12345,)####(12355,)####(12362,)####(12378,)####(12382,)####(12385,)####(12386,)####(12389,)####(12392,)####(12395,)####(12397,)####(12398,)####(12404,)####(12405,)####(12412,)####(12410,)####(12411,)####(13462,)####(12415,)####(12421,)####(12425,)####(12461,)####(16364,)####(12432,)####(12435,)####(12438,)####(12490,)####(12446,)####(16365,)####(12465,)####(12466,)####(12469,)####(12484,)####(12492,)####(12494,)####(12495,)####(12498,)####(12517,)####(12525,)####(12546,)####(12554,)####(12558,)####(12562,)####(12573,)####(12580,)####(12605,)####(12587,)####(12588,)####(12592,)####(12597,)####(12602,)####(12612,)####(12623,)####(12643,)####(12647,)####(12651,)####(12661,)####(12665,)####(12670,)####(12673,)####(12681,)####(12687,)####(12706,)####(12709,)####(12715,)####(12753,)####(12725,)####(12726,)####(12739,)####(12742,)####(12756,)####(12759,)####(12762,)####(12765,)####(12768,)####(12769,)####(12772,)####(12781,)####(12790,)####(12796,)####(12797,)####(12810,)####(12819,)####(12823,)####(12842,)####(12843,)####(12853,)####(12854,)####(12857,)####(12867,)####(12869,)####(12873,)####(12887,)####(12900,)####(12906,)####(12907,)####(12919,)####(12922,)####(12936,)####(12945,)####(12953,)####(12955,)####(12969,)####(12980,)####(12984,)####(12993,)####(12997,)####(13000,)####(13004,)####(13018,)####(13019,)####(13026,)####(13047,)####(13052,)####(13053,)####(13054,)####(13056,)####(13065,)####(13069,)####(13084,)####(13086,)####(13091,)####(13095,)####(13103,)####(13106,)####(13112,)####(13166,)####(13126,)####(13132,)####(13142,)####(13149,)####(13151,)####(13152,)####(13159,)####(13952,)####(13169,)####(13172,)####(13182,)####(13186,)####(13197,)####(13213,)####(13221,)####(13243,)####(13252,)####(13259,)####(13262,)####(13264,)####(13266,)####(13267,)####(13272,)####(13275,)####(13300,)####(13306,)####(13314,)####(13318,)####(13322,)####(13326,)####(13329,)####(13331,)####(13334,)####(13340,)####(13342,)####(13346,)####(13353,)####(13371,)####(13376,)####(13382,)####(13389,)####(13399,)####(13412,)####(13419,)####(13430,)####(13433,)####(13438,)####(13442,)####(13446,)####(13526,)####(13454,)####(13456,)####(13458,)####(13465,)####(13468,)####(13470,)####(13471,)####(13487,)####(13494,)####(13497,)####(13499,)####(13500,)####(13504,)####(13509,)####(13510,)####(13532,)####(13533,)####(13535,)####(13536,)####(13550,)####(13555,)####(13560,)####(13564,)####(13568,)####(16816,)####(13582,)####(13584,)####(13586,)####(13597,)####(13605,)####(13607,)####(13636,)####(13614,)####(13615,)####(13617,)####(13623,)####(13655,)####(13637,)####(13638,)####(13643,)####(13658,)####(13659,)####(13666,)####(13675,)####(13676,)####(13685,)####(13686,)####(13690,)####(13726,)####(13702,)####(13705,)####(13707,)####(13709,)####(13736,)####(13747,)####(13757,)####(22021,)####(13784,)####(13778,)####(13781,)####(13783,)####(13786,)####(13788,)####(13810,)####(13797,)####(13805,)####(13817,)####(13828,)####(13833,)####(13834,)####(13845,)####(13850,)####(16951,)####(13853,)####(13855,)####(13857,)####(13859,)####(13862,)####(13869,)####(13875,)####(14002,)####(13880,)####(13887,)####(13895,)####(13932,)####(13902,)####(16953,)####(13927,)####(13936,)####(13941,)####(13944,)####(13950,)####(13954,)####(13961,)####(14634,)####(13965,)####(14033,)####(13983,)####(13984,)####(13989,)####(13999,)####(14004,)####(14005,)####(14007,)####(14009,)####(14034,)####(14089,)####(14045,)####(14048,)####(14059,)####(14061,)####(14064,)####(14066,)####(14076,)####(14072,)####(14078,)####(14079,)####(14082,)####(14095,)####(14099,)####(14101,)####(22216,)####(14108,)####(14110,)####(14113,)####(14118,)####(14127,)####(14140,)####(14143,)####(14146,)####(14147,)####(14153,)####(14158,)####(14175,)####(14183,)####(14204,)####(14206,)####(14226,)####(14210,)####(14219,)####(14220,)####(14231,)####(14237,)####(14246,)####(17285,)####(14301,)####(14336,)####(14322,)####(14323,)####(14324,)####(14332,)####(14355,)####(14360,)####(14368,)####(14374,)####(14377,)####(14381,)####(14383,)####(14393,)####(14399,)####(14408,)####(14410,)####(14426,)####(14434,)####(14437,)####(14444,)####(14448,)####(14449,)####(14457,)####(14458,)####(14469,)####(14471,)####(14473,)####(14474,)####(14480,)####(14481,)####(14482,)####(14483,)####(14497,)####(14500,)####(14561,)####(14504,)####(14515,)####(14516,)####(14522,)####(14523,)####(14524,)####(14531,)####(14539,)####(14543,)####(14546,)####(14547,)####(14548,)####(14558,)####(14567,)####(14572,)####(14578,)####(14585,)####(14595,)####(14616,)####(14620,)####(14643,)####(14661,)####(14673,)####(14679,)####(14697,)####(14682,)####(14689,)####(14711,)####(14721,)####(15872,)####(14742,)####(14754,)####(14757,)####(14765,)####(14785,)####(14791,)####(14792,)####(14793,)####(14797,)####(14798,)####(14803,)####(14810,)####(14822,)####(14827,)####(14832,)####(14838,)####(14842,)####(14850,)####(14853,)####(14856,)####(14872,)####(14876,)####(14877,)####(14887,)####(14893,)####(14914,)####(14922,)####(14928,)####(14942,)####(14947,)####(14948,)####(14951,)####(14955,)####(14957,)####(14966,)####(14976,)####(14978,)####(14986,)####(14987,)####(14999,)####(15021,)####(15007,)####(15008,)####(15011,)####(15025,)####(15028,)####(15031,)####(15036,)####(15039,)####(15043,)####(15047,)####(15052,)####(15058,)####(15062,)####(15064,)####(15068,)####(15070,)####(15075,)####(15076,)####(15084,)####(15090,)####(15102,)####(15104,)####(15114,)####(15127,)####(15130,)####(15141,)####(15145,)####(15154,)####(15157,)####(15158,)####(15160,)####(15163,)####(15164,)####(15167,)####(15168,)####(15171,)####(15224,)####(15181,)####(15187,)####(15225,)####(15198,)####(15207,)####(15223,)####(15227,)####(15228,)####(15261,)####(15287,)####(15289,)####(15307,)####(15296,)####(15299,)####(15303,)####(15305,)####(15315,)####(15319,)####(15322,)####(15324,)####(15349,)####(15329,)####(15371,)####(15336,)####(15338,)####(15350,)####(15413,)####(15387,)####(15422,)####(15423,)####(15424,)####(15426,)####(15427,)####(15429,)####(15447,)####(15448,)####(15456,)####(15463,)####(15469,)####(15475,)####(15486,)####(15496,)####(15497,)####(15501,)####(15505,)####(15511,)####(15514,)####(15522,)####(15526,)####(15540,)####(25685,)####(15548,)####(15552,)####(15556,)####(15565,)####(15567,)####(15569,)####(15570,)####(15574,)####(15577,)####(15579,)####(15580,)####(15581,)####(15584,)####(15585,)####(15592,)####(15593,)####(15596,)####(15598,)####(15603,)####(15608,)####(15618,)####(15631,)####(15635,)####(15641,)####(15649,)####(15651,)####(15664,)####(15670,)####(15686,)####(15692,)####(15696,)####(15707,)####(15713,)####(15717,)####(15722,)####(15724,)####(15728,)####(15735,)####(15738,)####(15741,)####(15744,)####(15749,)####(15757,)####(15761,)####(15768,)####(15770,)####(15776,)####(15777,)####(15791,)####(15798,)####(15800,)####(15815,)####(15824,)####(15839,)####(15840,)####(15841,)####(15842,)####(15867,)####(15889,)####(15891,)####(15897,)####(15899,)####(15900,)####(15940,)####(17107,)####(15950,)####(15951,)####(15957,)####(15958,)####(15967,)####(15970,)####(15971,)####(15974,)####(15976,)####(15978,)####(15979,)####(15982,)####(15983,)####(15984,)####(16001,)####(16003,)####(16008,)####(16018,)####(16013,)####(16032,)####(16037,)####(16046,)####(16054,)####(16057,)####(16069,)####(16084,)####(16095,)####(16098,)####(20129,)####(16117,)####(16121,)####(16130,)####(16146,)####(16164,)####(16178,)####(16170,)####(16185,)####(16207,)####(16193,)####(16198,)####(16214,)####(16218,)####(16219,)####(16229,)####(16230,)####(16239,)####(16247,)####(16262,)####(16263,)####(16270,)####(16276,)####(16278,)####(16282,)####(16302,)####(16308,)####(16312,)####(16319,)####(16320,)####(16321,)####(16327,)####(16331,)####(16333,)####(16334,)####(16348,)####(16349,)####(16358,)####(16362,)####(16381,)####(16390,)####(16407,)####(16408,)####(16413,)####(16416,)####(16428,)####(16432,)####(16445,)####(16456,)####(16458,)####(16464,)####(16467,)####(16480,)####(16485,)####(16488,)####(16493,)####(16501,)####(16507,)####(16508,)####(16513,)####(16514,)####(16516,)####(16565,)####(16528,)####(16533,)####(16539,)####(16540,)####(16566,)####(16546,)####(16552,)####(16557,)####(16564,)####(16574,)####(16576,)####(16583,)####(16597,)####(16601,)####(16602,)####(16605,)####(16608,)####(16611,)####(16630,)####(16631,)####(16646,)####(16632,)####(16636,)####(16692,)####(16653,)####(16665,)####(16758,)####(16686,)####(16688,)####(16689,)####(16709,)####(16710,)####(16837,)####(16721,)####(16727,)####(16747,)####(16750,)####(16755,)####(16756,)####(16770,)####(16778,)####(16779,)####(16796,)####(16802,)####(16814,)####(16822,)####(16827,)####(16834,)####(16847,)####(16857,)####(16865,)####(16883,)####(16889,)####(16894,)####(16912,)####(16915,)####(16916,)####(16939,)####(16921,)####(16928,)####(16930,)####(16946,)####(16947,)####(16950,)####(16958,)####(16974,)####(16969,)####(16979,)####(16982,)####(16988,)####(16998,)####(17006,)####(17019,)####(17015,)####(17028,)####(17039,)####(17042,)####(17044,)####(17047,)####(17050,)####(17052,)####(17065,)####(17066,)####(17068,)####(17077,)####(17090,)####(17091,)####(17094,)####(17436,)####(17109,)####(17110,)####(17111,)####(17117,)####(17119,)####(17121,)####(17126,)####(17149,)####(17164,)####(17167,)####(17173,)####(17174,)####(17196,)####(17195,)####(17343,)####(17215,)####(17219,)####(17225,)####(17226,)####(17227,)####(17345,)####(17251,)####(17281,)####(17256,)####(17258,)####(17263,)####(17264,)####(17272,)####(17294,)####(17298,)####(17300,)####(17302,)####(17303,)####(17304,)####(17349,)####(17331,)####(17334,)####(17336,)####(17342,)####(17354,)####(17368,)####(17371,)####(17378,)####(17408,)####(17391,)####(17402,)####(17416,)####(17424,)####(17431,)####(17451,)####(17480,)####(17455,)####(17463,)####(17468,)####(17472,)####(17474,)####(17488,)####(17495,)####(17501,)####(17506,)####(17511,)####(17533,)####(17523,)####(17537,)####(17549,)####(17565,)####(17876,)####(17571,)####(17581,)####(17587,)####(17595,)####(17602,)####(17604,)####(17618,)####(17623,)####(17624,)####(17628,)####(17633,)####(17656,)####(17662,)####(17672,)####(17686,)####(17673,)####(17696,)####(17706,)####(17710,)####(17711,)####(17730,)####(17737,)####(17760,)####(17773,)####(17781,)####(17788,)####(17800,)####(17811,)####(17815,)####(17816,)####(17827,)####(17843,)####(17853,)####(17868,)####(17881,)####(17890,)####(17897,)####(17904,)####(17918,)####(17920,)####(17927,)####(17932,)####(17938,)####(17942,)####(17955,)####(17969,)####(17992,)####(18001,)####(18004,)####(18006,)####(18016,)####(18030,)####(18042,)####(18045,)####(18054,)####(18058,)####(18073,)####(18072,)####(18082,)####(18084,)####(18088,)####(18111,)####(18112,)####(18116,)####(18119,)####(18133,)####(18135,)####(18137,)####(18152,)####(18155,)####(18162,)####(18167,)####(18212,)####(18178,)####(18233,)####(18185,)####(18197,)####(18234,)####(18208,)####(18214,)####(18215,)####(18333,)####(18336,)####(18232,)####(18238,)####(18247,)####(18248,)####(18271,)####(18275,)####(18284,)####(18286,)####(18293,)####(18314,)####(18320,)####(18339,)####(18348,)####(18362,)####(18375,)####(18379,)####(18391,)####(18396,)####(18404,)####(18406,)####(18407,)####(18408,)####(18424,)####(18428,)####(18431,)####(18433,)####(18434,)####(18438,)####(18450,)####(18456,)####(18459,)####(18718,)####(18468,)####(18470,)####(18480,)####(18486,)####(18487,)####(18490,)####(18496,)####(18510,)####(18511,)####(18518,)####(18526,)####(18533,)####(18781,)####(18538,)####(18542,)####(18550,)####(18558,)####(18563,)####(18576,)####(18579,)####(18590,)####(18595,)####(18599,)####(18603,)####(18606,)####(18615,)####(18617,)####(18621,)####(18626,)####(18633,)####(18638,)####(18651,)####(18655,)####(18661,)####(18669,)####(18680,)####(18693,)####(18694,)####(18706,)####(18707,)####(18709,)####(18727,)####(18745,)####(18746,)####(18750,)####(18757,)####(18761,)####(18764,)####(18771,)####(18774,)####(18775,)####(18821,)####(18796,)####(18799,)####(18809,)####(18815,)####(18816,)####(18833,)####(18844,)####(18848,)####(18856,)####(18880,)####(18882,)####(18885,)####(18887,)####(18890,)####(18891,)####(18896,)####(18897,)####(18901,)####(18905,)####(18913,)####(18915,)####(18916,)####(18945,)####(18935,)####(18938,)####(18954,)####(18969,)####(18971,)####(18973,)####(18987,)####(18988,)####(19005,)####(19019,)####(19044,)####(19048,)####(19060,)####(19067,)####(19069,)####(19073,)####(19096,)####(19102,)####(19103,)####(19110,)####(19126,)####(19134,)####(19139,)####(19143,)####(19147,)####(19344,)####(19165,)####(19181,)####(19195,)####(19210,)####(19216,)####(19224,)####(19227,)####(19243,)####(19271,)####(19280,)####(19281,)####(19291,)####(19311,)####(19314,)####(19319,)####(19321,)####(19328,)####(19340,)####(19350,)####(19351,)####(19355,)####(19384,)####(19391,)####(19403,)####(19426,)####(19431,)####(19459,)####(19518,)####(19469,)####(19470,)####(19471,)####(19490,)####(19506,)####(19523,)####(19524,)####(19537,)####(19549,)####(19555,)####(19571,)####(19576,)####(19591,)####(19593,)####(19594,)####(19599,)####(19601,)####(19605,)####(19607,)####(19613,)####(19620,)####(19636,)####(19639,)####(19644,)####(19650,)####(19654,)####(19675,)####(19681,)####(19715,)####(19692,)####(19698,)####(19701,)####(19703,)####(19706,)####(20166,)####(19724,)####(19729,)####(19736,)####(19739,)####(19763,)####(19764,)####(19772,)####(19783,)####(19787,)####(19788,)####(19792,)####(19794,)####(19818,)####(19823,)####(19856,)####(19857,)####(19866,)####(19884,)####(19889,)####(19890,)####(19891,)####(19895,)####(19968,)####(19941,)####(20227,)####(19945,)####(19948,)####(19952,)####(19966,)####(19958,)####(19975,)####(19996,)####(19999,)####(20001,)####(20002,)####(20010,)####(20011,)####(20032,)####(20013,)####(20022,)####(20034,)####(20029,)####(20035,)####(20040,)####(20059,)####(20068,)####(20076,)####(20079,)####(20092,)####(20101,)####(20110,)####(20117,)####(20118,)####(20124,)####(20125,)####(20150,)####(20158,)####(20176,)####(20177,)####(20196,)####(20200,)####(20209,)####(20217,)####(20225,)####(20235,)####(20245,)####(20240,)####(20251,)####(20271,)####(20291,)####(20295,)####(20297,)####(20305,)####(20330,)####(20341,)####(20349,)####(20366,)####(20386,)####(20397,)####(20421,)####(23042,)####(20424,)####(20416,)####(20429,)####(20457,)####(20435,)####(20437,)####(20438,)####(20441,)####(20445,)####(20447,)####(20452,)####(20463,)####(20476,)####(20503,)####(20506,)####(20514,)####(20519,)####(20520,)####(20523,)####(20532,)####(20533,)####(20537,)####(20543,)####(20553,)####(20555,)####(20558,)####(20563,)####(20579,)####(20583,)####(20586,)####(20590,)####(20593,)####(20608,)####(20612,)####(20613,)####(20616,)####(20622,)####(20628,)####(20632,)####(20643,)####(20648,)####(20653,)####(20680,)####(20681,)####(20682,)####(20701,)####(20712,)####(20714,)####(20917,)####(20749,)####(20726,)####(20729,)####(20733,)####(20734,)####(20735,)####(20918,)####(20741,)####(20746,)####(20756,)####(20786,)####(20772,)####(20773,)####(23308,)####(20789,)####(20793,)####(20802,)####(20806,)####(20813,)####(20825,)####(20826,)####(20835,)####(20836,)####(20854,)####(20863,)####(20864,)####(20881,)####(20890,)####(20891,)####(20904,)####(20912,)####(20936,)####(20939,)####(20944,)####(20947,)####(20948,)####(20949,)####(20973,)####(20977,)####(34477,)####(20988,)####(20996,)####(21002,)####(21003,)####(21020,)####(21022,)####(21023,)####(21024,)####(21032,)####(21036,)####(21042,)####(21050,)####(21075,)####(21067,)####(21076,)####(21081,)####(21086,)####(21103,)####(21104,)####(21106,)####(21112,)####(21119,)####(21121,)####(21137,)####(21139,)####(21140,)####(21141,)####(21145,)####(21152,)####(21153,)####(21162,)####(21164,)####(21167,)####(21169,)####(21180,)####(21173,)####(21193,)####(21203,)####(21207,)####(21214,)####(21222,)####(21224,)####(21229,)####(21233,)####(21237,)####(21265,)####(21282,)####(21291,)####(21297,)####(21298,)####(21311,)####(21315,)####(21317,)####(21319,)####(21343,)####(21332,)####(21340,)####(21346,)####(21350,)####(21370,)####(21380,)####(21393,)####(21395,)####(21398,)####(21403,)####(21419,)####(21429,)####(21439,)####(21447,)####(21486,)####(21463,)####(21465,)####(21499,)####(21473,)####(21510,)####(21515,)####(25824,)####(21530,)####(21539,)####(21549,)####(21551,)####(21557,)####(21565,)####(21567,)####(21572,)####(21581,)####(21596,)####(21591,)####(21592,)####(21613,)####(21619,)####(21735,)####(21649,)####(21659,)####(21681,)####(21685,)####(21691,)####(21692,)####(21700,)####(21717,)####(21720,)####(21738,)####(21768,)####(21760,)####(21770,)####(21771,)####(21790,)####(21791,)####(21807,)####(21811,)####(21815,)####(21822,)####(21825,)####(21844,)####(21847,)####(21850,)####(21854,)####(25857,)####(21868,)####(21872,)####(21875,)####(21896,)####(21907,)####(21914,)####(21919,)####(21927,)####(21935,)####(21949,)####(21955,)####(21960,)####(21964,)####(21970,)####(21997,)####(21999,)####(22009,)####(22017,)####(22019,)####(22029,)####(22035,)####(22036,)####(22064,)####(22066,)####(22068,)####(22071,)####(22073,)####(22084,)####(22085,)####(22098,)####(22116,)####(22143,)####(22149,)####(22156,)####(22161,)####(22177,)####(22184,)####(22196,)####(22188,)####(22198,)####(22200,)####(22209,)####(22210,)####(22212,)####(22233,)####(22239,)####(22248,)####(22252,)####(22258,)####(22344,)####(22283,)####(22285,)####(22293,)####(22304,)####(22309,)####(22346,)####(22328,)####(22329,)####(22347,)####(22363,)####(26372,)####(22381,)####(22386,)####(22387,)####(22388,)####(22392,)####(22393,)####(22406,)####(22414,)####(22422,)####(22425,)####(22426,)####(22428,)####(22434,)####(22441,)####(22528,)####(22468,)####(22472,)####(22488,)####(22489,)####(22491,)####(22494,)####(22501,)####(22502,)####(22504,)####(22508,)####(22539,)####(22546,)####(22554,)####(22557,)####(22620,)####(22568,)####(22569,)####(22572,)####(22600,)####(22607,)####(22625,)####(22627,)####(22628,)####(22648,)####(22653,)####(22662,)####(22665,)####(22680,)####(22684,)####(22694,)####(22697,)####(22699,)####(22711,)####(22718,)####(22719,)####(22731,)####(22737,)####(22742,)####(22757,)####(22764,)####(22768,)####(22774,)####(22786,)####(22787,)####(22796,)####(22800,)####(31197,)####(22804,)####(22805,)####(22815,)####(22816,)####(22846,)####(22840,)####(22843,)####(22850,)####(22856,)####(22869,)####(22890,)####(22879,)####(22880,)####(22884,)####(22902,)####(22910,)####(22916,)####(22920,)####(22925,)####(22932,)####(22938,)####(22945,)####(22952,)####(22955,)####(22961,)####(22974,)####(22979,)####(22988,)####(22989,)####(22999,)####(23007,)####(23010,)####(23059,)####(23027,)####(23060,)####(23036,)####(23037,)####(23054,)####(23079,)####(23081,)####(23090,)####(23098,)####(23099,)####(23106,)####(23110,)####(23117,)####(23128,)####(23158,)####(23134,)####(23136,)####(23142,)####(23144,)####(23152,)####(23163,)####(23173,)####(23177,)####(23197,)####(23224,)####(23225,)####(23234,)####(23235,)####(23238,)####(23267,)####(23275,)####(23276,)####(23297,)####(23302,)####(23305,)####(23317,)####(23319,)####(23323,)####(23331,)####(23353,)####(23348,)####(23382,)####(23388,)####(23391,)####(23420,)####(23439,)####(23445,)####(23451,)####(23459,)####(23465,)####(23472,)####(23479,)####(23481,)####(23488,)####(23490,)####(23497,)####(23509,)####(23519,)####(23520,)####(23531,)####(23536,)####(23538,)####(23539,)####(33413,)####(23547,)####(23548,)####(23566,)####(23611,)####(23590,)####(23591,)####(23593,)####(23617,)####(23629,)####(23634,)####(23640,)####(23656,)####(23668,)####(23664,)####(23666,)####(23965,)####(23680,)####(23696,)####(23704,)####(23708,)####(23713,)####(23743,)####(23763,)####(23789,)####(23778,)####(23779,)####(23781,)####(23785,)####(23793,)####(23800,)####(23803,)####(23827,)####(23833,)####(23852,)####(23860,)####(23863,)####(23864,)####(23874,)####(23881,)####(23887,)####(23898,)####(23908,)####(23914,)####(23921,)####(23935,)####(23928,)####(23931,)####(23932,)####(24033,)####(23938,)####(23944,)####(23951,)####(23963,)####(23967,)####(23983,)####(24074,)####(23992,)####(23993,)####(23994,)####(24006,)####(24009,)####(24012,)####(24018,)####(24022,)####(24023,)####(24034,)####(24047,)####(24116,)####(24054,)####(24069,)####(24072,)####(24077,)####(24079,)####(24115,)####(24118,)####(24125,)####(24131,)####(24139,)####(24146,)####(24157,)####(24176,)####(24179,)####(24186,)####(24187,)####(24193,)####(24194,)####(24203,)####(24209,)####(24211,)####(24214,)####(24221,)####(24222,)####(24227,)####(24253,)####(24242,)####(24245,)####(24257,)####(24261,)####(24288,)####(24290,)####(24298,)####(24300,)####(24304,)####(24314,)####(24325,)####(24330,)####(24331,)####(24362,)####(24365,)####(24375,)####(24380,)####(24389,)####(24392,)####(24397,)####(24398,)####(24405,)####(24416,)####(24418,)####(24424,)####(33906,)####(24437,)####(24439,)####(24441,)####(24442,)####(24445,)####(37681,)####(24450,)####(24452,)####(24464,)####(24467,)####(24472,)####(24477,)####(24488,)####(24492,)####(24493,)####(24508,)####(24514,)####(24516,)####(24521,)####(24527,)####(24536,)####(24540,)####(24607,)####(24562,)####(24568,)####(24572,)####(24588,)####(24595,)####(24614,)####(24618,)####(24623,)####(24627,)####(24645,)####(24662,)####(24668,)####(24676,)####(24680,)####(24685,)####(24689,)####(24691,)####(24693,)####(24705,)####(24708,)####(24714,)####(24720,)####(24731,)####(24736,)####(24751,)####(24753,)####(24755,)####(24772,)####(24777,)####(24780,)####(24781,)####(24795,)####(24799,)####(24804,)####(24806,)####(24819,)####(24821,)####(24827,)####(24837,)####(24840,)####(24852,)####(24853,)####(24857,)####(24934,)####(24875,)####(24962,)####(24878,)####(24887,)####(24897,)####(24899,)####(24900,)####(24904,)####(24905,)####(24975,)####(24921,)####(24935,)####(24937,)####(24938,)####(24944,)####(24950,)####(24959,)####(24961,)####(24980,)####(24992,)####(24993,)####(24994,)####(25007,)####(25024,)####(25025,)####(25049,)####(25055,)####(25094,)####(25068,)####(25069,)####(25080,)####(25082,)####(25092,)####(25105,)####(25148,)####(25109,)####(25113,)####(25144,)####(25162,)####(25169,)####(25171,)####(25187,)####(25191,)####(25203,)####(25209,)####(25211,)####(25214,)####(25220,)####(25228,)####(25229,)####(25232,)####(25235,)####(25238,)####(25253,)####(25269,)####(25272,)####(25276,)####(25286,)####(25290,)####(25299,)####(25309,)####(25317,)####(25318,)####(25322,)####(25423,)####(25343,)####(25359,)####(25371,)####(25364,)####(25377,)####(25383,)####(25385,)####(25386,)####(25387,)####(25389,)####(25391,)####(25392,)####(25400,)####(25403,)####(25409,)####(25415,)####(25416,)####(25440,)####(25464,)####(25468,)####(25470,)####(25483,)####(25486,)####(25506,)####(25511,)####(25520,)####(25524,)####(25539,)####(25542,)####(25558,)####(25568,)####(25575,)####(25577,)####(25591,)####(25594,)####(25605,)####(25611,)####(25615,)####(25623,)####(25632,)####(25641,)####(25645,)####(25660,)####(25661,)####(25667,)####(25668,)####(25672,)####(25679,)####(25690,)####(25696,)####(25706,)####(25713,)####(25715,)####(25734,)####(25738,)####(25740,)####(25748,)####(25750,)####(25764,)####(25773,)####(25775,)####(25780,)####(25820,)####(25789,)####(25791,)####(25804,)####(25811,)####(25815,)####(25817,)####(25825,)####(25827,)####(25839,)####(25848,)####(25854,)####(25855,)####(25875,)####(25895,)####(25889,)####(25890,)####(25894,)####(25903,)####(25908,)####(25912,)####(26049,)####(25926,)####(25934,)####(25940,)####(25941,)####(25942,)####(25945,)####(25946,)####(25949,)####(25956,)####(25975,)####(25981,)####(25988,)####(25992,)####(26005,)####(26014,)####(26016,)####(26018,)####(26020,)####(26024,)####(26031,)####(26034,)####(26038,)####(26041,)####(26044,)####(26045,)####(26067,)####(26084,)####(26088,)####(26094,)####(26110,)####(26133,)####(26140,)####(26144,)####(26145,)####(26147,)####(26197,)####(26167,)####(26176,)####(26178,)####(26180,)####(26183,)####(26261,)####(26199,)####(26219,)####(26230,)####(26237,)####(26247,)####(26271,)####(26276,)####(26286,)####(26293,)####(26300,)####(26352,)####(26326,)####(26344,)####(26346,)####(26363,)####(26364,)####(26386,)####(26395,)####(26400,)####(26407,)####(26413,)####(26417,)####(26437,)####(26432,)####(26440,)####(26449,)####(26461,)####(26450,)####(26451,)####(26485,)####(26500,)####(26502,)####(26505,)####(26510,)####(26512,)####(26516,)####(26518,)####(26520,)####(26521,)####(26523,)####(26528,)####(26531,)####(26537,)####(26539,)####(26558,)####(26562,)####(26568,)####(26571,)####(26578,)####(26584,)####(26585,)####(26591,)####(26858,)####(26606,)####(26608,)####(26611,)####(26623,)####(26656,)####(26637,)####(26649,)####(26650,)####(26652,)####(26660,)####(26661,)####(26674,)####(26676,)####(26678,)####(26688,)####(26689,)####(26696,)####(26700,)####(26702,)####(26712,)####(26722,)####(26728,)####(26916,)####(26757,)####(26762,)####(26764,)####(26780,)####(26781,)####(26917,)####(37489,)####(26797,)####(26810,)####(26815,)####(26818,)####(26828,)####(26839,)####(26842,)####(26855,)####(26863,)####(26876,)####(26886,)####(26890,)####(26903,)####(26914,)####(26924,)####(26926,)####(26927,)####(26930,)####(26947,)####(26949,)####(27721,)####(26958,)####(26974,)####(26978,)####(26987,)####(26988,)####(26994,)####(26999,)####(27005,)####(27007,)####(27027,)####(27030,)####(27033,)####(27038,)####(27049,)####(27050,)####(27067,)####(27097,)####(27080,)####(27085,)####(27090,)####(27103,)####(27266,)####(27112,)####(27120,)####(27127,)####(27140,)####(27146,)####(27154,)####(27165,)####(27169,)####(27171,)####(27185,)####(27199,)####(27205,)####(27206,)####(27211,)####(27215,)####(27297,)####(27237,)####(27256,)####(27257,)####(27259,)####(27288,)####(27299,)####(27300,)####(27303,)####(27313,)####(27318,)####(27332,)####(27345,)####(27346,)####(27351,)####(27354,)####(27361,)####(27372,)####(27426,)####(27399,)####(27546,)####(27400,)####(27417,)####(27429,)####(27436,)####(27442,)####(27443,)####(27446,)####(27452,)####(27454,)####(27474,)####(27482,)####(27495,)####(27511,)####(27553,)####(27558,)####(27560,)####(27572,)####(27589,)####(27610,)####(27624,)####(27627,)####(27651,)####(27655,)####(27656,)####(27662,)####(27667,)####(27672,)####(27677,)####(27682,)####(27691,)####(27724,)####(27730,)####(27736,)####(27741,)####(27748,)####(27750,)####(27764,)####(27767,)####(27772,)####(27773,)####(27777,)####(27785,)####(27786,)####(27815,)####(27817,)####(27827,)####(27829,)####(27830,)####(27832,)####(27861,)####(27869,)####(27898,)####(27882,)####(27900,)####(27908,)####(27913,)####(27916,)####(27927,)####(27929,)####(27946,)####(27948,)####(27950,)####(27951,)####(27958,)####(27961,)####(27976,)####(27982,)####(27988,)####(27991,)####(27995,)####(28014,)####(28029,)####(28035,)####(28042,)####(28067,)####(28069,)####(28070,)####(28077,)####(28102,)####(28094,)####(28099,)####(28113,)####(28117,)####(28123,)####(28139,)####(28170,)####(28178,)####(28199,)####(28229,)####(28232,)####(28233,)####(28257,)####(28316,)####(28318,)####(28343,)####(28348,)####(28362,)####(28375,)####(28382,)####(28385,)####(28406,)####(28419,)####(28425,)####(28426,)####(28431,)####(28437,)####(28472,)####(28474,)####(28486,)####(28498,)####(28509,)####(28512,)####(28528,)####(28542,)####(28576,)####(28564,)####(28566,)####(29095,)####(28581,)####(29096,)####(28593,)####(28599,)####(28601,)####(28602,)####(28609,)####(28612,)####(28712,)####(28616,)####(28617,)####(28620,)####(28623,)####(28630,)####(28642,)####(28649,)####(28656,)####(28658,)####(28681,)####(28688,)####(28690,)####(28715,)####(28730,)####(28732,)####(28738,)####(28744,)####(28745,)####(28756,)####(28769,)####(28772,)####(28775,)####(28789,)####(28800,)####(28807,)####(28813,)####(28819,)####(28828,)####(28832,)####(28845,)####(28851,)####(28869,)####(28873,)####(28876,)####(28877,)####(28888,)####(28904,)####(28909,)####(28925,)####(28936,)####(29044,)####(28938,)####(28941,)####(28942,)####(28957,)####(28968,)####(28987,)####(28992,)####(28996,)####(29008,)####(29006,)####(29017,)####(29038,)####(29039,)####(29059,)####(29091,)####(29104,)####(29114,)####(29117,)####(29121,)####(29126,)####(29130,)####(29155,)####(29185,)####(29158,)####(29200,)####(29167,)####(29168,)####(29170,)####(29176,)####(29182,)####(29190,)####(29203,)####(29211,)####(29222,)####(29239,)####(29241,)####(29253,)####(29271,)####(29280,)####(29292,)####(29385,)####(29354,)####(29356,)####(29357,)####(29367,)####(29380,)####(29396,)####(29400,)####(29408,)####(29417,)####(29424,)####(29446,)####(29469,)####(29477,)####(29479,)####(29489,)####(29500,)####(29510,)####(29513,)####(29515,)####(29519,)####(29525,)####(29538,)####(29617,)####(29546,)####(29553,)####(29560,)####(29563,)####(29573,)####(29578,)####(29580,)####(29585,)####(29594,)####(29611,)####(29612,)####(29614,)####(29615,)####(29627,)####(29636,)####(29641,)####(29653,)####(29673,)####(29682,)####(29683,)####(29690,)####(29703,)####(29713,)####(29719,)####(29731,)####(29781,)####(29814,)####(29815,)####(29816,)####(29824,)####(29826,)####(29828,)####(29851,)####(29906,)####(29859,)####(29860,)####(29861,)####(29903,)####(29905,)####(29918,)####(29932,)####(30131,)####(29949,)####(29957,)####(29958,)####(29962,)####(29981,)####(29986,)####(29987,)####(29990,)####(30002,)####(30033,)####(30041,)####(30042,)####(30054,)####(30059,)####(30064,)####(30069,)####(30072,)####(30075,)####(30083,)####(30087,)####(30129,)####(30118,)####(30119,)####(30156,)####(30159,)####(30174,)####(30198,)####(30212,)####(30220,)####(30236,)####(30255,)####(30261,)####(30271,)####(30274,)####(30279,)####(30281,)####(30290,)####(30301,)####(30303,)####(30309,)####(30317,)####(30325,)####(30348,)####(30358,)####(30361,)####(30365,)####(30369,)####(30377,)####(30394,)####(30382,)####(30388,)####(30397,)####(30402,)####(30405,)####(30406,)####(30414,)####(30415,)####(30418,)####(30435,)####(30455,)####(30456,)####(30465,)####(30468,)####(30476,)####(30481,)####(30487,)####(30491,)####(30495,)####(30521,)####(30525,)####(30545,)####(30723,)####(30554,)####(30562,)####(30731,)####(30569,)####(30578,)####(30588,)####(30592,)####(30599,)####(30602,)####(30604,)####(30615,)####(30640,)####(30645,)####(30652,)####(30654,)####(30665,)####(30670,)####(30679,)####(30686,)####(30687,)####(30689,)####(30691,)####(30699,)####(30708,)####(30717,)####(30728,)####(30729,)####(30750,)####(30762,)####(30764,)####(30765,)####(30773,)####(30781,)####(30788,)####(30790,)####(30798,)####(30816,)####(30818,)####(30825,)####(30834,)####(30842,)####(30858,)####(30863,)####(30870,)####(30876,)####(30877,)####(30919,)####(30903,)####(30907,)####(30916,)####(30931,)####(30934,)####(30941,)####(30945,)####(30946,)####(30948,)####(30959,)####(30965,)####(30970,)####(30975,)####(30976,)####(30982,)####(30986,)####(30995,)####(30999,)####(31002,)####(31014,)####(31015,)####(31018,)####(31027,)####(31036,)####(31037,)####(31039,)####(31053,)####(31083,)####(31066,)####(31078,)####(31213,)####(31096,)####(31113,)####(31116,)####(31118,)####(31119,)####(31123,)####(31126,)####(31138,)####(31139,)####(31142,)####(31143,)####(31152,)####(31159,)####(31164,)####(31166,)####(31177,)####(31190,)####(31191,)####(31201,)####(31204,)####(31284,)####(31231,)####(31238,)####(31253,)####(31260,)####(31262,)####(31270,)####(31275,)####(31279,)####(31280,)####(31292,)####(31299,)####(31374,)####(31326,)####(31355,)####(31400,)####(31364,)####(31366,)####(31379,)####(31387,)####(31397,)####(31406,)####(31417,)####(31430,)####(31431,)####(31440,)####(31442,)####(31443,)####(31454,)####(31459,)####(31473,)####(31488,)####(31492,)####(31494,)####(31502,)####(31513,)####(31516,)####(31530,)####(31532,)####(31547,)####(31548,)####(31557,)####(31565,)####(31569,)####(31579,)####(31591,)####(31597,)####(31598,)####(31602,)####(31605,)####(31621,)####(31622,)####(31624,)####(31633,)####(31636,)####(31641,)####(31657,)####(31658,)####(31666,)####(31667,)####(31679,)####(31680,)####(31690,)####(31697,)####(31714,)####(31841,)####(31724,)####(31726,)####(31739,)####(31746,)####(31747,)####(31751,)####(31755,)####(31768,)####(31771,)####(31913,)####(31800,)####(31803,)####(31807,)####(31817,)####(31818,)####(31828,)####(31849,)####(31851,)####(31867,)####(31891,)####(31904,)####(31906,)####(31908,)####(31919,)####(31925,)####(31929,)####(31933,)####(31934,)####(31936,)####(31947,)####(31967,)####(31948,)####(31952,)####(31955,)####(31981,)####(32001,)####(32035,)####(32040,)####(32061,)####(32063,)####(32080,)####(32087,)####(32088,)####(32093,)####(32095,)####(32097,)####(32105,)####(32125,)####(32130,)####(32133,)####(32139,)####(32158,)####(32169,)####(32179,)####(32183,)####(32188,)####(32206,)####(32208,)####(32220,)####(32221,)####(32223,)####(32225,)####(32234,)####(32246,)####(32248,)####(32262,)####(32284,)####(32285,)####(32290,)####(32295,)####(32297,)####(32303,)####(32310,)####(32313,)####(32314,)####(32318,)####(32319,)####(32357,)####(32363,)####(32377,)####(32389,)####(32393,)####(32397,)####(32405,)####(32419,)####(32415,)####(32426,)####(32439,)####(32441,)####(32445,)####(32447,)####(32462,)####(32464,)####(32528,)####(32471,)####(32477,)####(32484,)####(32488,)####(32504,)####(32522,)####(32524,)####(33566,)####(32541,)####(32542,)####(32552,)####(32560,)####(32596,)####(32565,)####(32566,)####(32581,)####(32584,)####(32585,)####(32600,)####(32609,)####(32634,)####(32653,)####(32659,)####(32662,)####(32672,)####(32694,)####(32697,)####(32701,)####(32703,)####(32706,)####(32721,)####(32730,)####(32735,)####(32741,)####(32742,)####(32748,)####(32751,)####(32847,)####(32770,)####(32781,)####(32789,)####(32794,)####(32795,)####(32810,)####(32813,)####(32821,)####(32824,)####(32825,)####(32826,)####(32830,)####(32840,)####(32841,)####(32848,)####(32866,)####(32876,)####(32880,)####(32893,)####(32901,)####(32905,)####(32925,)####(33712,)####(32957,)####(32958,)####(32982,)####(32970,)####(32971,)####(32994,)####(33013,)####(33021,)####(33028,)####(33050,)####(33036,)####(33053,)####(33057,)####(33059,)####(33064,)####(33067,)####(33069,)####(33078,)####(33083,)####(33091,)####(33096,)####(33098,)####(33103,)####(33136,)####(33124,)####(33129,)####(33156,)####(33130,)####(33131,)####(33132,)####(33141,)####(33142,)####(33148,)####(33155,)####(33174,)####(33175,)####(33177,)####(33179,)####(33180,)####(33185,)####(33189,)####(33193,)####(33197,)####(33204,)####(33234,)####(33236,)####(33240,)####(33259,)####(33260,)####(33281,)####(33283,)####(33284,)####(33286,)####(33297,)####(33300,)####(33303,)####(33304,)####(33311,)####(33315,)####(33327,)####(33336,)####(33348,)####(33352,)####(33354,)####(33357,)####(33359,)####(33364,)####(33367,)####(33377,)####(33382,)####(33392,)####(33393,)####(33418,)####(33432,)####(33433,)####(33451,)####(33453,)####(33457,)####(33458,)####(33465,)####(33467,)####(33475,)####(33477,)####(33499,)####(33502,)####(33507,)####(33516,)####(33518,)####(33520,)####(33523,)####(33538,)####(33539,)####(33545,)####(33547,)####(33553,)####(33556,)####(33560,)####(33562,)####(33592,)####(33596,)####(33615,)####(33620,)####(33621,)####(33628,)####(33659,)####(33674,)####(33683,)####(33684,)####(33692,)####(33694,)####(33709,)####(33737,)####(33738,)####(33755,)####(33759,)####(33765,)####(33768,)####(33774,)####(33776,)####(33780,)####(33796,)####(33808,)####(33809,)####(33820,)####(33823,)####(33829,)####(33842,)####(33848,)####(33860,)####(33862,)####(33887,)####(33871,)####(33874,)####(33888,)####(33917,)####(33918,)####(33923,)####(33931,)####(33970,)####(33998,)####(34008,)####(34017,)####(34023,)####(34030,)####(34033,)####(34052,)####(34057,)####(34070,)####(34072,)####(34076,)####(34089,)####(34106,)####(34120,)####(34125,)####(34134,)####(34151,)####(34169,)####(34177,)####(34193,)####(34209,)####(34229,)####(34237,)####(34238,)####(34242,)####(34299,)####(34325,)####(34312,)####(34318,)####(34319,)####(34347,)####(34356,)####(34360,)####(34363,)####(34390,)####(34394,)####(34396,)####(34428,)####(34430,)####(34439,)####(34442,)####(34446,)####(34463,)####(34464,)####(34465,)####(34483,)####(34494,)####(34488,)####(34493,)####(34510,)####(34535,)####(34547,)####(34559,)####(34570,)####(34574,)####(34582,)####(34587,)####(34611,)####(34613,)####(34616,)####(34624,)####(34636,)####(34642,)####(34649,)####(34652,)####(34680,)####(34682,)####(34696,)####(34698,)####(34703,)####(34706,)####(34724,)####(34726,)####(34741,)####(34743,)####(34751,)####(34753,)####(34767,)####(34761,)####(34769,)####(34781,)####(34789,)####(34791,)####(34797,)####(34809,)####(34823,)####(34827,)####(34832,)####(34849,)####(34851,)####(34859,)####(34870,)####(34939,)####(34903,)####(34940,)####(34919,)####(34920,)####(34926,)####(34927,)####(34932,)####(34956,)####(34969,)####(35032,)####(35000,)####(35009,)####(35010,)####(35012,)####(35013,)####(35021,)####(35038,)####(35041,)####(35042,)####(35071,)####(35072,)####(35089,)####(35091,)####(35104,)####(35107,)####(35118,)####(35123,)####(35129,)####(35132,)####(35137,)####(35142,)####(35165,)####(35152,)####(35175,)####(35194,)####(35198,)####(35220,)####(35221,)####(35242,)####(35231,)####(35233,)####(35236,)####(35249,)####(35255,)####(35276,)####(35313,)####(35319,)####(35322,)####(35324,)####(35348,)####(35353,)####(35382,)####(35386,)####(35438,)####(35489,)####(35429,)####(35461,)####(35501,)####(35466,)####(35470,)####(35472,)####(35486,)####(35488,)####(35495,)####(35510,)####(35511,)####(35530,)####(35609,)####(35537,)####(35540,)####(35550,)####(35561,)####(35566,)####(35576,)####(35590,)####(35591,)####(35598,)####(35601,)####(35606,)####(35616,)####(35634,)####(35651,)####(35657,)####(35658,)####(35683,)####(35694,)####(35711,)####(35732,)####(35771,)####(35776,)####(35816,)####(35824,)####(35825,)####(35839,)####(38509,)####(35841,)####(35852,)####(35865,)####(35857,)####(35877,)####(35883,)####(35893,)####(35905,)####(35936,)####(35940,)####(35948,)####(35949,)####(35955,)####(35956,)####(35982,)####(35970,)####(35971,)####(35984,)####(36005,)####(36027,)####(36015,)####(36047,)####(36064,)####(36076,)####(36093,)####(36099,)####(36113,)####(36165,)####(36137,)####(36145,)####(36148,)####(36167,)####(36152,)####(36171,)####(36184,)####(36207,)####(36212,)####(36213,)####(36217,)####(36240,)####(36247,)####(36253,)####(36254,)####(36267,)####(36279,)####(36294,)####(36298,)####(37372,)####(36305,)####(36309,)####(36314,)####(36324,)####(36327,)####(36331,)####(36332,)####(36343,)####(36351,)####(37370,)####(37386,)####(37387,)####(37437,)####(37399,)####(37405,)####(37406,)####(37411,)####(37424,)####(37461,)####(37502,)####(37466,)####(37469,)####(37492,)####(37497,)####(37519,)####(37557,)####(37911,)####(37566,)####(37586,)####(37587,)####(37589,)####(38721,)####(37590,)####(37615,)####(37619,)####(37620,)####(37628,)####(37638,)####(37647,)####(37669,)####(37706,)####(37729,)####(37732,)####(37733,)####(37743,)####(37753,)####(37758,)####(37761,)####(37768,)####(37771,)####(37775,)####(37776,)####(37785,)####(37795,)####(37797,)####(37805,)####(37814,)####(37826,)####(37833,)####(37840,)####(37865,)####(37894,)####(37912,)####(37918,)####(37930,)####(37935,)####(37944,)####(37977,)####(37993,)####(38001,)####(38016,)####(38087,)####(38091,)####(38093,)####(38102,)####(38105,)####(38115,)####(38117,)####(38118,)####(38160,)####(38174,)####(38177,)####(38180,)####(38187,)####(38188,)####(38195,)####(38201,)####(38207,)####(38215,)####(38216,)####(38218,)####(38227,)####(38232,)####(38245,)####(38248,)####(38382,)####(38273,)####(38296,)####(38299,)####(38350,)####(38322,)####(38337,)####(38370,)####(38379,)####(38403,)####(38408,)####(38419,)####(38420,)####(38439,)####(38487,)####(38471,)####(38853,)####(38493,)####(38495,)####(38501,)####(38504,)####(38524,)####(38530,)####(38541,)####(38548,)####(38580,)####(38587,)####(38629,)####(38631,)####(38635,)####(38668,)####(38669,)####(38672,)####(38724,)####(38676,)####(38709,)####(38710,)####(38730,)####(38750,)####(38755,)####(38765,)####(38777,)####(38787,)####(38799,)####(38820,)####(38831,)####(38838,)####(38841,)####(38844,)####(38856,)####(38889,)####(38922,)####(38967,)####(38962,)####(39182,)####(38975,)####(38986,)####(38995,)####(39000,)####(39002,)####(39082,)####(39023,)####(39024,)####(39031,)####(39037,)####(39042,)####(39080,)####(39081,)####(39083,)####(39087,)####(39110,)####(39115,)####(39138,)####(39169,)####(39144,)####(39164,)####(39187,)####(39194,)####(39211,)####(39227,)####(39231,)####(39232,)####(39243,)####(39260,)####(39279,)####(39314,)####(39323,)####(40342,)####(40354,)####(40363,)####(40364,)####(40365,)####(40368,)####(40920,)####(40382,)####(40383,)####(40384,)####(40385,)####(40407,)####(40410,)####(40427,)####(40441,)####(40443,)####(46452,)####(40453,)####(40454,)####(40482,)####(40495,)####(40514,)####(40504,)####(40512,)####(40532,)####(40542,)####(40546,)####(40549,)####(40560,)####(40576,)####(40588,)####(40598,)####(40602,)####(40607,)####(40613,)####(40616,)####(40639,)####(40647,)####(40670,)####(41896,)####(40737,)####(40739,)####(40745,)####(40769,)####(40775,)####(40779,)####(40795,)####(40808,)####(74832,)####(40818,)####(40852,)####(40876,)####(40838,)####(40885,)####(40861,)####(40868,)####(40870,)####(40875,)####(40884,)####(40905,)####(40906,)####(40907,)####(40910,)####(40928,)####(40958,)####(40988,)####(40965,)####(40972,)####(40976,)####(40982,)####(40989,)####(41006,)####(41029,)####(41041,)####(41051,)####(41065,)####(41070,)####(41074,)####(41093,)####(41104,)####(41123,)####(41129,)####(46482,)####(41145,)####(41168,)####(41176,)####(41183,)####(41187,)####(41190,)####(41199,)####(41208,)####(41222,)####(41242,)####(41259,)####(41270,)####(41282,)####(41306,)####(41286,)####(41289,)####(41296,)####(41297,)####(41320,)####(41326,)####(41349,)####(41354,)####(41361,)####(41367,)####(41380,)####(41386,)####(41390,)####(41394,)####(41404,)####(41443,)####(41448,)####(41450,)####(41557,)####(41467,)####(41486,)####(41492,)####(41493,)####(41509,)####(41529,)####(41536,)####(41537,)####(41544,)####(41551,)####(41561,)####(41563,)####(41572,)####(41573,)####(41576,)####(41578,)####(41586,)####(41591,)####(41620,)####(41626,)####(41654,)####(41655,)####(41669,)####(41678,)####(41695,)####(41702,)####(41704,)####(41716,)####(41723,)####(41726,)####(41730,)####(41770,)####(41771,)####(41773,)####(41789,)####(41791,)####(41798,)####(41873,)####(41852,)####(41860,)####(41861,)####(41881,)####(41892,)####(41895,)####(41898,)####(41925,)####(41934,)####(41937,)####(42956,)####(42997,)####(42999,)####(43009,)####(43019,)####(43020,)####(43036,)####(43053,)####(43061,)####(43071,)####(43076,)####(43086,)####(43099,)####(43102,)####(43114,)####(43115,)####(43119,)####(43128,)####(43131,)####(43144,)####(43157,)####(43159,)####(43185,)####(43206,)####(43214,)####(43243,)####(43264,)####(43270,)####(43302,)####(43304,)####(43309,)####(43310,)####(43333,)####(43339,)####(43341,)####(43347,)####(43361,)####(43365,)####(43368,)####(43370,)####(43398,)####(43421,)####(43470,)####(43471,)####(43482,)####(43504,)####(43521,)####(43536,)####(43538,)####(43544,)####(43554,)####(43566,)####(43577,)####(43586,)####(43588,)####(43594,)####(43635,)####(43664,)####(43674,)####(43675,)####(43688,)####(43697,)####(45232,)####(43720,)####(43721,)####(43723,)####(43726,)####(43729,)####(43733,)####(43747,)####(43748,)####(43768,)####(43779,)####(43799,)####(43804,)####(43825,)####(43851,)####(43867,)####(43876,)####(43877,)####(43882,)####(43898,)####(43930,)####(43943,)####(43958,)####(43969,)####(43996,)####(44016,)####(54888,)####(44032,)####(44033,)####(44046,)####(44060,)####(44078,)####(44080,)####(44103,)####(44095,)####(44117,)####(44134,)####(44144,)####(44166,)####(44201,)####(44204,)####(44220,)####(44223,)####(44227,)####(44246,)####(44261,)####(44267,)####(44268,)####(44281,)####(44331,)####(44332,)####(44336,)####(44342,)####(44343,)####(44347,)####(44349,)####(44357,)####(44359,)####(44382,)####(44384,)####(44394,)####(44403,)####(44440,)####(44445,)####(44465,)####(44475,)####(44484,)####(44494,)####(44505,)####(44517,)####(44526,)####(44527,)####(44536,)####(44546,)####(44559,)####(44564,)####(44569,)####(44581,)####(44596,)####(44602,)####(44640,)####(44609,)####(44615,)####(44622,)####(44628,)####(44643,)####(44645,)####(44647,)####(44650,)####(44655,)####(44661,)####(44676,)####(44702,)####(44720,)####(44727,)####(44733,)####(44745,)####(44748,)####(44769,)####(44776,)####(44816,)####(44834,)####(44838,)####(44866,)####(44840,)####(44867,)####(44856,)####(44860,)####(44888,)####(44892,)####(44896,)####(44913,)####(44928,)####(44961,)####(44962,)####(44992,)####(44977,)####(44984,)####(44998,)####(44999,)####(45001,)####(45026,)####(45032,)####(45033,)####(45050,)####(45054,)####(45075,)####(45118,)####(45119,)####(45124,)####(45153,)####(45165,)####(45184,)####(45191,)####(45214,)####(45236,)####(45243,)####(45262,)####(45264,)####(45297,)####(45307,)####(45327,)####(45374,)####(45401,)####(45408,)####(45419,)####(45444,)####(45479,)####(45482,)####(45503,)####(45506,)####(45528,)####(45539,)####(45555,)####(45559,)####(45569,)####(45580,)####(45584,)####(45588,)####(45643,)####(45652,)####(45666,)####(45671,)####(45681,)####(45693,)####(45705,)####(45721,)####(45723,)####(45731,)####(45737,)####(45747,)####(45763,)####(45784,)####(45791,)####(45803,)####(45851,)####(45820,)####(45852,)####(45854,)####(45858,)####(45862,)####(45867,)####(45875,)####(45889,)####(45916,)####(45920,)####(45924,)####(45946,)####(45988,)####(46015,)####(46019,)####(46022,)####(46075,)####(46096,)####(46102,)####(46115,)####(46134,)####(46151,)####(46143,)####(46160,)####(46173,)####(46175,)####(46197,)####(46185,)####(46201,)####(46205,)####(46213,)####(46216,)####(46226,)####(46228,)####(46229,)####(46289,)####(46252,)####(46259,)####(46284,)####(46301,)####(46302,)####(46305,)####(46312,)####(46321,)####(46322,)####(46333,)####(46334,)####(46345,)####(46368,)####(46380,)####(46385,)####(46389,)####(46418,)####(46425,)####(46429,)####(46832,)####(46434,)####(46483,)####(46486,)####(46523,)####(46519,)####(46532,)####(46550,)####(46562,)####(46833,)####(46569,)####(46588,)####(46594,)####(46626,)####(46639,)####(46685,)####(46692,)####(49942,)####(46719,)####(46735,)####(60921,)####(46784,)####(46887,)####(46821,)####(46826,)####(46843,)####(46889,)####(46856,)####(46872,)####(46874,)####(46897,)####(46908,)####(46921,)####(46937,)####(46941,)####(46978,)####(46963,)####(46964,)####(47008,)####(47020,)####(47058,)####(47115,)####(47148,)####(47153,)####(47168,)####(47185,)####(47227,)####(47245,)####(47247,)####(47253,)####(47258,)####(47265,)####(47266,)####(47279,)####(47294,)####(47296,)####(47319,)####(47325,)####(47339,)####(47366,)####(47367,)####(47405,)####(47419,)####(47420,)####(47444,)####(47450,)####(47453,)####(47457,)####(47458,)####(47470,)####(47483,)####(47486,)####(47490,)####(47498,)####(47502,)####(47517,)####(47567,)####(47576,)####(47585,)####(47590,)####(47594,)####(47597,)####(47624,)####(47662,)####(47671,)####(47685,)####(47698,)####(47728,)####(47719,)####(47729,)####(47749,)####(47752,)####(47764,)####(47771,)####(47802,)####(47821,)####(47809,)####(47828,)####(47846,)####(47840,)####(47841,)####(47842,)####(47855,)####(47857,)####(47900,)####(47913,)####(47923,)####(47926,)####(47929,)####(47969,)####(47987,)####(47989,)####(47990,)####(48028,)####(48034,)####(48036,)####(48066,)####(48072,)####(48100,)####(48107,)####(48109,)####(48133,)####(48149,)####(48162,)####(48178,)####(48187,)####(48188,)####(48206,)####(48212,)####(48214,)####(59557,)####(48215,)####(48247,)####(48253,)####(48254,)####(48267,)####(48298,)####(48304,)####(48317,)####(48326,)####(48347,)####(48366,)####(48357,)####(48360,)####(48374,)####(48378,)####(48381,)####(48390,)####(48396,)####(48415,)####(48450,)####(48455,)####(48469,)####(48485,)####(48496,)####(48500,)####(48506,)####(48520,)####(48542,)####(48543,)####(48547,)####(48553,)####(48603,)####(48594,)####(48609,)####(48615,)####(48621,)####(48627,)####(48636,)####(48639,)####(48656,)####(48671,)####(48688,)####(48678,)####(48696,)####(48703,)####(48714,)####(48755,)####(48757,)####(48758,)####(48766,)####(48786,)####(48796,)####(48822,)####(48837,)####(48838,)####(48847,)####(48854,)####(48864,)####(48879,)####(48921,)####(48897,)####(48922,)####(49692,)####(48956,)####(48973,)####(48974,)####(48983,)####(48993,)####(49002,)####(49012,)####(49010,)####(49014,)####(49027,)####(49050,)####(49052,)####(49082,)####(49093,)####(49103,)####(49108,)####(49113,)####(49120,)####(49125,)####(49141,)####(49149,)####(49152,)####(49161,)####(49162,)####(49184,)####(49189,)####(49198,)####(49205,)####(49212,)####(49226,)####(49243,)####(49265,)####(49270,)####(49272,)####(49291,)####(49298,)####(49309,)####(49313,)####(49333,)####(49370,)####(49374,)####(49821,)####(49399,)####(49402,)####(49416,)####(49443,)####(49453,)####(49465,)####(49480,)####(49527,)####(49506,)####(49521,)####(49528,)####(49549,)####(49555,)####(49570,)####(49579,)####(49593,)####(49607,)####(49614,)####(49617,)####(49639,)####(49666,)####(49646,)####(49649,)####(49670,)####(49719,)####(49750,)####(49775,)####(49782,)####(49826,)####(49835,)####(49857,)####(49869,)####(49872,)####(49898,)####(49901,)####(49905,)####(49916,)####(49924,)####(49925,)####(49939,)####(49953,)####(50007,)####(50015,)####(50016,)####(50055,)####(50110,)####(50079,)####(50121,)####(65442,)####(50151,)####(50156,)####(50170,)####(50171,)####(50172,)####(50186,)####(50206,)####(50210,)####(50226,)####(50277,)####(50318,)####(50321,)####(50376,)####(50424,)####(50425,)####(50427,)####(50433,)####(50446,)####(50447,)####(50474,)####(50501,)####(50507,)####(50536,)####(50537,)####(50551,)####(50573,)####(50587,)####(50595,)####(50598,)####(50609,)####(50623,)####(50630,)####(50682,)####(50727,)####(50738,)####(50745,)####(50770,)####(50778,)####(50932,)####(50807,)####(50817,)####(50822,)####(50830,)####(50844,)####(50871,)####(50883,)####(50901,)####(50904,)####(50917,)####(50929,)####(50933,)####(50941,)####(50947,)####(50948,)####(50949,)####(50977,)####(51006,)####(51046,)####(51062,)####(51075,)####(51083,)####(51095,)####(51139,)####(51152,)####(51161,)####(51194,)####(51171,)####(51200,)####(51217,)####(51219,)####(51242,)####(51248,)####(51273,)####(51275,)####(51279,)####(51296,)####(51321,)####(51347,)####(51370,)####(51416,)####(51418,)####(51432,)####(51442,)####(51444,)####(51456,)####(51470,)####(51473,)####(51490,)####(51556,)####(51558,)####(51559,)####(51576,)####(51583,)####(51584,)####(51622,)####(51597,)####(51604,)####(51605,)####(51606,)####(51629,)####(51676,)####(51699,)####(51707,)####(51718,)####(51743,)####(51759,)####(51780,)####(51801,)####(51804,)####(51805,)####(51820,)####(51840,)####(51826,)####(51848,)####(51873,)####(51875,)####(51907,)####(51918,)####(51946,)####(51987,)####(52005,)####(52035,)####(52047,)####(52067,)####(52104,)####(52089,)####(52091,)####(52109,)####(52132,)####(52146,)####(52155,)####(52177,)####(52182,)####(52192,)####(52206,)####(52209,)####(52212,)####(52239,)####(52252,)####(52256,)####(52259,)####(52274,)####(52276,)####(52293,)####(52301,)####(52305,)####(52379,)####(52421,)####(52458,)####(52462,)####(52475,)####(52499,)####(52509,)####(52517,)####(52522,)####(52527,)####(52534,)####(52541,)####(52583,)####(52585,)####(52629,)####(52636,)####(52644,)####(52646,)####(52653,)####(52719,)####(52695,)####(52702,)####(52704,)####(52713,)####(52716,)####(52751,)####(52754,)####(52794,)####(52825,)####(52829,)####(52834,)####(52838,)####(52840,)####(52856,)####(52906,)####(52909,)####(52913,)####(52915,)####(52927,)####(52943,)####(52966,)####(52976,)####(53009,)####(53042,)####(53053,)####(53068,)####(53102,)####(53078,)####(53083,)####(53111,)####(53151,)####(53154,)####(53155,)####(53239,)####(53240,)####(53254,)####(53334,)####(53269,)####(53280,)####(53299,)####(53312,)####(53345,)####(53390,)####(53393,)####(53407,)####(54450,)####(54455,)####(54472,)####(54514,)####(54522,)####(54533,)####(54547,)####(54556,)####(54566,)####(54577,)####(54578,)####(54623,)####(54648,)####(54699,)####(54750,)####(54766,)####(54769,)####(54818,)####(54821,)####(54849,)####(54854,)####(54856,)####(54885,)####(54900,)####(54937,)####(54952,)####(54954,)####(54960,)####(55658,)####(55018,)####(55034,)####(55072,)####(55083,)####(55100,)####(55105,)####(55112,)####(55118,)####(55132,)####(55134,)####(55147,)####(55148,)####(55168,)####(55177,)####(55215,)####(55236,)####(55268,)####(55269,)####(55288,)####(55302,)####(55310,)####(55316,)####(61080,)####(55377,)####(55389,)####(55393,)####(55465,)####(55473,)####(55501,)####(55508,)####(55525,)####(55550,)####(55563,)####(55571,)####(55591,)####(55610,)####(55612,)####(55615,)####(55624,)####(55691,)####(55715,)####(55716,)####(55718,)####(55742,)####(55752,)####(55772,)####(55777,)####(55795,)####(55805,)####(55813,)####(55817,)####(55856,)####(55874,)####(55887,)####(55888,)####(55899,)####(55901,)####(55925,)####(55962,)####(56036,)####(55999,)####(56040,)####(56052,)####(56066,)####(56092,)####(56103,)####(56120,)####(56134,)####(56139,)####(56150,)####(56160,)####(56201,)####(56236,)####(56251,)####(56271,)####(56300,)####(56302,)####(56304,)####(56322,)####(56341,)####(56374,)####(56380,)####(56385,)####(56392,)####(56398,)####(56399,)####(56402,)####(56409,)####(56427,)####(56440,)####(56465,)####(56479,)####(56500,)####(56506,)####(56521,)####(56530,)####(56534,)####(56538,)####(56543,)####(56575,)####(56589,)####(56596,)####(56600,)####(56649,)####(56658,)####(56695,)####(56699,)####(56737,)####(56757,)####(56771,)####(56784,)####(56793,)####(56824,)####(56832,)####(56837,)####(56852,)####(56870,)####(56881,)####(56895,)####(56900,)####(56940,)####(56949,)####(56971,)####(56972,)####(56980,)####(57010,)####(57031,)####(57027,)####(57068,)####(57074,)####(57104,)####(57107,)####(57115,)####(57117,)####(57133,)####(57145,)####(57150,)####(57152,)####(57168,)####(57180,)####(57191,)####(57213,)####(57240,)####(57251,)####(57259,)####(57273,)####(57351,)####(57395,)####(57407,)####(57438,)####(57467,)####(57492,)####(57510,)####(57536,)####(57573,)####(57577,)####(57578,)####(57581,)####(57602,)####(59175,)####(57685,)####(57705,)####(57715,)####(57725,)####(57746,)####(57765,)####(57776,)####(57778,)####(57807,)####(57842,)####(57872,)####(57859,)####(57885,)####(57888,)####(57912,)####(57985,)####(57987,)####(57999,)####(58031,)####(58046,)####(58058,)####(58062,)####(58094,)####(58097,)####(58141,)####(58214,)####(58220,)####(58221,)####(58227,)####(58272,)####(58349,)####(58359,)####(58354,)####(58368,)####(58404,)####(58408,)####(58425,)####(58435,)####(58438,)####(58449,)####(58486,)####(58497,)####(58531,)####(58540,)####(58564,)####(58576,)####(58585,)####(58657,)####(58669,)####(58679,)####(58757,)####(58717,)####(58739,)####(58745,)####(58765,)####(58772,)####(58791,)####(58800,)####(58818,)####(58827,)####(58828,)####(58830,)####(58855,)####(58861,)####(58874,)####(58900,)####(58910,)####(58930,)####(58941,)####(58959,)####(58966,)####(73032,)####(58986,)####(58999,)####(59013,)####(59018,)####(59047,)####(59058,)####(59062,)####(59074,)####(59085,)####(59096,)####(59107,)####(59124,)####(59133,)####(59145,)####(59152,)####(59192,)####(59202,)####(59213,)####(59235,)####(59250,)####(59262,)####(59297,)####(59301,)####(59305,)####(59325,)####(59327,)####(59330,)####(59342,)####(59343,)####(59360,)####(59384,)####(59434,)####(59478,)####(59489,)####(59493,)####(59502,)####(59546,)####(59568,)####(59588,)####(59609,)####(59625,)####(59630,)####(59636,)####(59643,)####(59668,)####(59755,)####(59757,)####(59774,)####(59777,)####(59784,)####(59838,)####(59796,)####(59814,)####(59815,)####(59829,)####(59831,)####(59850,)####(59873,)####(59879,)####(59887,)####(59891,)####(59910,)####(59916,)####(59947,)####(59988,)####(60035,)####(60073,)####(60074,)####(60149,)####(60153,)####(60154,)####(60166,)####(60221,)####(60235,)####(60238,)####(60256,)####(60274,)####(60283,)####(60328,)####(60334,)####(60352,)####(60355,)####(60362,)####(60383,)####(60410,)####(60414,)####(60418,)####(60456,)####(60438,)####(60448,)####(60460,)####(60856,)####(60476,)####(60500,)####(60511,)####(60601,)####(60619,)####(60622,)####(60643,)####(60666,)####(60670,)####(60680,)####(60718,)####(60743,)####(60767,)####(60777,)####(60804,)####(60817,)####(60825,)####(60828,)####(60872,)####(60877,)####(60893,)####(60938,)####(60939,)####(60952,)####(60958,)####(60994,)####(61001,)####(61002,)####(61026,)####(61008,)####(61034,)####(61036,)####(61037,)####(61055,)####(61078,)####(61090,)####(61108,)####(61110,)####(61131,)####(61144,)####(61150,)####(61189,)####(61217,)####(61218,)####(61225,)####(61230,)####(61232,)####(61235,)####(61264,)####(61309,)####(61349,)####(61328,)####(61335,)####(61344,)####(61393,)####(61453,)####(61510,)####(61521,)####(61535,)####(61547,)####(61569,)####(61573,)####(62347,)####(61626,)####(61638,)####(61649,)####(61697,)####(61678,)####(61681,)####(61705,)####(61711,)####(61768,)####(61729,)####(61733,)####(61740,)####(61798,)####(61805,)####(61815,)####(61824,)####(61829,)####(61863,)####(61869,)####(61906,)####(61924,)####(61935,)####(61956,)####(62004,)####(62006,)####(62015,)####(62032,)####(62075,)####(62092,)####(62106,)####(62225,)####(62208,)####(62211,)####(62237,)####(62247,)####(62312,)####(62353,)####(62370,)####(62409,)####(62416,)####(62420,)####(62439,)####(62445,)####(62470,)####(62524,)####(62531,)####(62545,)####(62621,)####(62631,)####(62646,)####(62658,)####(62677,)####(62705,)####(62714,)####(62741,)####(62747,)####(62756,)####(62758,)####(62770,)####(62795,)####(62824,)####(62852,)####(62853,)####(62862,)####(62873,)####(62876,)####(62880,)####(62899,)####(62920,)####(62949,)####(62968,)####(62995,)####(63026,)####(63028,)####(63067,)####(63074,)####(63079,)####(63099,)####(63152,)####(63155,)####(63203,)####(63222,)####(63250,)####(63313,)####(63321,)####(63350,)####(63357,)####(63417,)####(63441,)####(63447,)####(63464,)####(63487,)####(63499,)####(63621,)####(63546,)####(63558,)####(63566,)####(63584,)####(63604,)####(63647,)####(63652,)####(63669,)####(63717,)####(63725,)####(63730,)####(63760,)####(63809,)####(63856,)####(63881,)####(63922,)####(63930,)####(64006,)####(64069,)####(64147,)####(64152,)####(64172,)####(64195,)####(64208,)####(64226,)####(64224,)####(64246,)####(64249,)####(64272,)####(64293,)####(64304,)####(64311,)####(64343,)####(64370,)####(64403,)####(64425,)####(64433,)####(64500,)####(64513,)####(64526,)####(64535,)####(64538,)####(64545,)####(64551,)####(64555,)####(64558,)####(64573,)####(64581,)####(64601,)####(64612,)####(64617,)####(64621,)####(64634,)####(64652,)####(64674,)####(64676,)####(64680,)####(64683,)####(64686,)####(64697,)####(64705,)####(64711,)####(64725,)####(64738,)####(64739,)####(64788,)####(64789,)####(64790,)####(64829,)####(64830,)####(64852,)####(64863,)####(64880,)####(64949,)####(64927,)####(64936,)####(64952,)####(64991,)####(65001,)####(65005,)####(65127,)####(65128,)####(65186,)####(65202,)####(65211,)####(65212,)####(65244,)####(65276,)####(65287,)####(65290,)####(65292,)####(65344,)####(65353,)####(65409,)####(65379,)####(65390,)####(65405,)####(65421,)####(65446,)####(65479,)####(65487,)####(65489,)####(65566,)####(65585,)####(65637,)####(65640,)####(65643,)####(65650,)####(65671,)####(65681,)####(65682,)####(65690,)####(65692,)####(65699,)####(65705,)####(65713,)####(65762,)####(65764,)####(65775,)####(65828,)####(65844,)####(65866,)####(65898,)####(65900,)####(65912,)####(65919,)####(65942,)####(66088,)####(66048,)####(66086,)####(66105,)####(66113,)####(66161,)####(66192,)####(66199,)####(66280,)####(66300,)####(66314,)####(66315,)####(66365,)####(66369,)####(66373,)####(66448,)####(66509,)####(66543,)####(66622,)####(66586,)####(66591,)####(66600,)####(66610,)####(66616,)####(66665,)####(66710,)####(66749,)####(66757,)####(66760,)####(66791,)####(66825,)####(66946,)####(66925,)####(66956,)####(66963,)####(66992,)####(67087,)####(67028,)####(67049,)####(67185,)####(67204,)####(67309,)####(67311,)####(67318,)####(67320,)####(67362,)####(67385,)####(67400,)####(67437,)####(67443,)####(67468,)####(67543,)####(67547,)####(67551,)####(67560,)####(67623,)####(67658,)####(67677,)####(67704,)####(67721,)####(67762,)####(67827,)####(67771,)####(67897,)####(67911,)####(68011,)####(68394,)####(68069,)####(68087,)####(68077,)####(68080,)####(68086,)####(68106,)####(68140,)####(68151,)####(68205,)####(68236,)####(68261,)####(68274,)####(68290,)####(68349,)####(68354,)####(68431,)####(68454,)####(68542,)####(68562,)####(68588,)####(68644,)####(68662,)####(68692,)####(68696,)####(68710,)####(68717,)####(68737,)####(68740,)####(68759,)####(68793,)####(68830,)####(68834,)####(68893,)####(68938,)####(68940,)####(68966,)####(69001,)####(69114,)####(69115,)####(69130,)####(69136,)####(69145,)####(69157,)####(69208,)####(69237,)####(69263,)####(69353,)####(69383,)####(69371,)####(69407,)####(69424,)####(69438,)####(69452,)####(69501,)####(69575,)####(69515,)####(69549,)####(69570,)####(69638,)####(69664,)####(69699,)####(69825,)####(69886,)####(69898,)####(69983,)####(70034,)####(70054,)####(70089,)####(70100,)####(70101,)####(70107,)####(70150,)####(70168,)####(70179,)####(70248,)####(70249,)####(70266,)####(70274,)####(70322,)####(70325,)####(70425,)####(70469,)####(70490,)####(70553,)####(70498,)####(70509,)####(70541,)####(70545,)####(70556,)####(70558,)####(70574,)####(70593,)####(70619,)####(70629,)####(70643,)####(70645,)####(70658,)####(70699,)####(70741,)####(70764,)####(70783,)####(70801,)####(70824,)####(70848,)####(70855,)####(70868,)####(70889,)####(70899,)####(70937,)####(70940,)####(70959,)####(70990,)####(71026,)####(71065,)####(71136,)####(71172,)####(71176,)####(71184,)####(71187,)####(71194,)####(71197,)####(71201,)####(71208,)####(71260,)####(71314,)####(71335,)####(71414,)####(71417,)####(71452,)####(71466,)####(71485,)####(71489,)####(71517,)####(71519,)####(71535,)####(71622,)####(71684,)####(71708,)####(71727,)####(71764,)####(71777,)####(71788,)####(71914,)####(71935,)####(71946,)####(71962,)####(72047,)####(72090,)####(72196,)####(72224,)####(72242,)####(72230,)####(72243,)####(72251,)####(72258,)####(72266,)####(72288,)####(72294,)####(72370,)####(72381,)####(72376,)####(72400,)####(72421,)####(72418,)####(72439,)####(72453,)####(72515,)####(72553,)####(72557,)####(72613,)####(72627,)####(72652,)####(72654,)####(72685,)####(72709,)####(72774,)####(72788,)####(72839,)####(72854,)####(72911,)####(72940,)####(72941,)####(72953,)####(72968,)####(72992,)####(73050,)####(73065,)####(73165,)####(73231,)####(73238,)####(73293,)####(73320,)####(73376,)####(73416,)####(73463,)####(73540,)####(73576,)####(73592,)####(73613,)####(73646,)####(73708,)####(73765,)####(73824,)####(73869,)####(73921,)####(73966,)####(74082,)####(74098,)####(74140,)####(74174,)####(74261,)####(74351,)####(74372,)####(74377,)####(74394,)####(74418,)####(74448,)####(74468,)####(74542,)####(74544,)####(74622,)####(74661,)####(74797,)####(74850,)####(74843,)####(74865,)####(74878,)####(74924,)####(74965,)####(75022,)####(75050,)####(75045,)####(75054,)####(76058,)####(76059,)####(76226,)####(76240,)####(76358,)####(76365,)####(76371,)####(76444,)####(76461,)####(76488,)####(76587,)####(76642,)####(76738,)####(76761,)####(76815,)####(76856,)####(76865,)####(76875,)####(76896,)####(76935,)####(76959,)####(76994,)####(77018,)####(77125,)####(77213,)####(77290,)####(77313,)####(77374,)####(77359,)####(77394,)####(77411,)####(77528,)####(77546,)####(77573,)####(77580,)####(77586,)####(77609,)####(77634,)####(77692,)####(77752,)####(77791,)####(77834,)####(77836,)####(77850,)####(77891,)####(78104,)####(78042,)####(78063,)####(78139,)####(78155,)####(78168,)####(78255,)####(78287,)####(78321,)####(78324,)####(78365,)####(78517,)####(78547,)####(78644,)####(78711,)####(78754,)####(78797,)####(78828,)####(78839,)####(78885,)####(78940,)####(79234,)####(79281,)####(79289,)####(79357,)####(79399,)####(79406,)####(79420,)####(79454,)####(79470,)####(79531,)####(79687,)####(79688,)####(80050,)####(80189,)####(80377,)####(80380,)####(80528,)####(80730,)####(80823,)####(80943,)####(80874,)####(81000,)####(81057,)####(81138,)####(81151,)####(81261,)####(81262,)####(81284,)####(81368,)####(81396,)####(81427,)####(81457,)####(81481,)####(81482,)####(81483,)####(81523,)####(81546,)####(81609,)####(81622,)####(81727,)####(81778,)####(81864,)####(81871,)####(81986,)####(82050,)####(82102,)####(82105,)####(82162,)####(82242,)####(82326,)####(82475,)####(82497,)####(82503,)####(82579,)####(82603,)####(82682,)####(82754,)####(82776,)####(82963,)####(82984,)####(83012,)####(83022,)####(83030,)####(83136,)####(83269,)####(83310,)####(83347,)####(83392,)####(83554,)####(83577,)####(83663,)####(83731,)####(83778,)####(83899,)####(83913,)####(83982,)####(84012,)####(84076,)####(84268,)####(84314,)####(84330,)####(85363,)####(85372,)####(85445,)####(85605,)####(85687,)####(85767,)####(85903,)####(86015,)####(86024,)####(86221,)####(86285,)####(86318,)####(86348,)####(86399,)####(86429,)####(86588,)####(86719,)####(86720,)####(86767,)####(86834,)####(87132,)####(87182,)####(87403,)####(87486,)####(87524,)####(87538,)####(87695,)####(87730,)####(87744,)####(87856,)####(87871,)####(87921,)####(87956,)####(88065,)####(88267,)####(88188,)####(88218,)####(88315,)####(88348,)####(88485,)####(88603,)####(88756,)####(88905,)####(88960,)####(88980,)####(89032,)####(89104,)####(99690,)####(89179,)####(89303,)####(89376,)####(89382,)####(89456,)####(89513,)####(89635,)####(89941,)####(90004,)####(90149,)####(90157,)####(90231,)####(90490,)####(90711,)####(90754,)####(90793,)####(90902,)####(91034,)####(91078,)####(91107,)####(91143,)####(91253,)####(91512,)####(91536,)####(92065,)####(92081,)####(92097,)####(92150,)####(92213,)####(92303,)####(92344,)####(92443,)####(92724,)####(92829,)####(93303,)####(93392,)####(93540,)####(93592,)####(93705,)####(93743,)####(93757,)####(93830,)####(94136,)####(94974,)####(95016,)####(95694,)####(95993,)####(96042,)####(96371,)####(96483,)####(96553,)####(96739,)####(97013,)####(97085,)####(97381,)####(97604,)####(97834,)####(97929,)####(99171,)####(99717,)####(100041,)####(100137,)####(100151,)####(100267,)####(100785,)####(101003,)####(101255,)####(101274,)####(101354,)####(101590,)####(102709,)####(102778,)####(102806,)####(103345,)####(103417,)####(103592,)####(103843,)####(104146,)####(104500,)####(104713,)####(106367,)####(108911,)####(109076,)####(109210,)####(109390,)####(110113,)####(110473,)####(110564,)####(110801,)####(111035,)####(111010,)####(111422,)####(112147,)####(112451,)####(112769,)####(113526,)####(114152,)####(114610,)####(114665,)####(114763,)####(1,)####(2,)####(3,)####(4,)####(6,)####(7,)####(10,)####(30,)####(17,)####(22,)####(23,)####(25,)####(26,)####(31,)####(33,)####(35,)####(36,)####(50,)####(53,)####(58,)####(73,)####(75,)####(134,)####(97,)####(100,)####(103,)####(109,)####(114,)####(118,)####(124,)####(125,)####(128,)####(130,)####(138,)####(145,)####(146,)####(152,)####(155,)####(183,)####(161,)####(165,)####(166,)####(168,)####(170,)####(173,)####(175,)####(181,)####(192,)####(194,)####(196,)####(203,)####(206,)####(213,)####(220,)####(222,)####(224,)####(225,)####(363,)####(256,)####(257,)####(258,)####(267,)####(269,)####(270,)####(276,)####(277,)####(283,)####(287,)####(288,)####(290,)####(298,)####(305,)####(321,)####(322,)####(328,)####(346,)####(354,)####(362,)####(372,)####(373,)####(421,)####(396,)####(423,)####(411,)####(414,)####(418,)####(452,)####(480,)####(481,)####(485,)####(486,)####(490,)####(498,)####(507,)####(517,)####(527,)####(534,)####(539,)####(2093,)####(548,)####(555,)",
        "base_pg_sql": "SELECT Id FROM posts WHERE ViewCount > (SELECT AVG(ViewCount) FROM posts)",
        "base_question": "Identify the total number of posts with views above average.",
        "base_evidence": "views above average refer to ViewCount > AVG(ViewCount);",
        "gt": {
            "type": "delete",
            "table": "posts",
            "condition": "SELECT posts.id FROM posts WHERE ViewCount > (SELECT AVG(ViewCount) FROM posts) "
        }
    }
]